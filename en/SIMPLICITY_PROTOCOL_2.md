Aqui estÃ¡ a traduÃ§Ã£o do seu arquivo Markdown do portuguÃªs para o inglÃªs:

```markdown
# Simplicity Protocol 2

**Author**: JosuÃ© Amaral
**Creation Date**: December 02, 2025
**Based on**: Simplicity Protocol 1 v2.0
**Version**: 2.2
**Last Update**: December 10, 2025
**Objective**: ADVANCED professional methodology for incremental quality development with a focus on security, performance, and continuous improvement

**Differences from Simplicity 1**:
- âœ… **10 new optional steps** for complex and critical projects
- âœ… Objective **Decision Matrix** for task selection
- âœ… **Security Checklist** (OWASP Top 10)
- âœ… **CI/CD with automatic Quality Gates**
- âœ… **ADR** (Architecture Decision Records)
- âœ… **Profiling and Optimization** for critical features
- âœ… Documented **Rollback Plans**
- âœ… **Sprint Retrospectives** (continuous improvement)
- âœ… **Peer Code Review** (if a team exists)
- âœ… **Accessibility Checklist** (GUI)

---

## ðŸ“Š Comparison of the 3 Protocols

| Aspect | Simplicity 1 | Simplicity 2 | Simplicity 3 |
|---|---|---|---|
| **Steps** | 13 mandatory | 13 mand + 10 opt | 16 mand + 3 opt |
| **Scenario** | Prototypes/internal | **Enterprise teams** | Solo in production |
| **Security** | âŒ No | âœ… OWASP mandatory | âœ… OWASP mandatory |
| **CI/CD** | âŒ No | âœ… Mandatory | âœ… Mandatory |
| **Rollback** | âŒ No | âœ… Mandatory | âœ… Mandatory |
| **Code Review** | âŒ No | âœ… **Peer mandatory** | âŒ Solo |
| **Retrospectives** | âŒ No | âœ… **Formal team** | âŒ Solo |
| **Accessibility** | âŒ No | âœ… WCAG 2.1 | âŒ Optional |
| **API Docs** | âŒ No | âœ… Formal Sphinx | âŒ Docstrings |
| **Overhead** | Low | **High** | Medium |
| **Production** | âŒ Not recommended | âœ… **Companies** | âœ… Solo devs |
| **Time/Task** | ~2-3h | ~4-6h | ~3-4h |
| **Best For** | Learning, prototyping | Large teams | Solo in production |

---

## ðŸŽ¯ When to Use Each Protocol?

### **Simplicity Protocol 1** (13 mandatory steps)
**Use for**:
- âœ… Solo projects or small teams (1-3 devs)
- âœ… Simple to medium features
- âœ… Rapid prototyping
- âœ… First development of a functionality
- âœ… When speed is more important than perfection
- âœ… Non-critical internal projects

**Do not use for**:
- âŒ Critical production applications
- âŒ Systems with security requirements
- âŒ High-impact/high-risk features
- âŒ Projects with large teams (>5 devs)

### **Simplicity Protocol 2** (13 mandatory + 10 optional = 23 steps)
**Use for**:
- âœ… **Critical production applications** with a team
- âœ… Systems with **sensitive data** (LGPD, GDPR, PCI-DSS)
- âœ… **High-impact/high-risk features**
- âœ… Projects with **medium/large teams** (3+ devs)
- âœ… **Public Libraries/APIs** with multiple users
- âœ… Systems with critical **performance requirements**
- âœ… **Commercial/enterprise applications**
- âœ… Projects with **regulatory compliance** (ISO, SOC2)
- âœ… Code that requires **external audit**

**Do not use for**:
- âŒ **Rapid prototyping** (unnecessary overhead)
- âŒ **Disposable scripts** or single-use
- âŒ **Simple personal projects**
- âŒ **Solo developer** without a team â†’ Use **Simplicity 3** (less overhead)
- âŒ **Non-critical internal apps** â†’ Use **Simplicity 1**

**Rationale**: Simplicity 2 offers **maximum quality and security** through:
- **Peer Code Review**: Detects bugs that a solo developer might miss
- **Formal Retrospectives**: Continuous team improvement
- **Formal ADRs**: Documentation of architectural decisions for the long term
- **Accessibility WCAG**: Legal compliance for public apps
- **API Docs Sphinx**: Professional documentation for libraries

However, this rigor comes with a **cost**: ~4-6h per task vs ~2-3h in Simplicity 1. For a **solo developer**, this overhead doesn't pay off - use **Simplicity 3** which maintains production security without team bureaucracy.

---

**Changelog v2.2** (10/12/2025):
- âœ… **[COMPLEMENTATION]** Added comparative table of the 3 protocols (Simplicity 1/2/3)
- âœ… Expanded section "ðŸŽ¯ When to Use Simplicity 2?"
- âœ… Additional criteria: Regulatory compliance, external audit, solo dev (use S3)
- âœ… Detailed Rationale: Why code review/ADR/accessibility are worth the 4-6h overhead
- âœ… Comparison: Simplicity 2 vs 3 (team vs solo) with 12 aspects analyzed
- âœ… Inspiration: Concepts adapted from Simplicity 3 v3.1 (tables, criteria, rationale)

**Changelog v2.1** (09/12/2025):
- âœ… **[STEP 3]** Added recommendation for AI to provide suggestions and guesses for questions
- âœ… Recommended format: "â“ Question + ðŸ’¡ AI Suggestion + Options A/B/C"
- âœ… Rationale: Accelerates decisions, reduces cognitive load, maintains consistency with existing code
- âœ… Classification: **OPTIONAL but HIGHLY RECOMMENDED**

**Changelog v2.0** (02/12/2025):
- âœ… **[NEW PROTOCOL]** Created Simplicity Protocol 2 based on Simplicity 1 v1.8
- âœ… **Step 2.5**: Decision Matrix for objective task selection (HIGH PRIORITY)
- âœ… **Step 6.5**: Security Checklist - OWASP Top 10 (HIGH PRIORITY)
- âœ… **Step 6.7**: Generate API Documentation (Sphinx/pdoc)
- âœ… **Step 8.5**: Accessibility Checklist - WCAG 2.1
- âœ… **Step 9.5**: Peer Code Review (Pull Request)
- âœ… **Step 10.5**: Profiling and Optimization (critical features)
- âœ… **Step 10.6**: Validate Quality Metrics - CI/CD (HIGH PRIORITY)
- âœ… **Step 11.5**: Create ADR (Architecture Decision Record)
- âœ… **Step 12.5**: Document Rollback Plan
- âœ… **Step 13.5**: Sprint Retrospective (continuous improvement)
- âœ… **Total**: 13 mandatory steps + 10 optional steps = 23 steps
- âœ… **Focus**: Security, Performance, Quality, Continuous Improvement

---

**Changelogs Inherited from Simplicity 1**:

**Changelog v1.8** (02/12/2025):
- âœ… **[REORGANIZATION]** Code Review integrated into CLI and GUI steps
- âœ… Step 7: Verify CLI Implementation (includes 9 quality criteria)
- âœ… Step 8: Verify GUI Implementation (includes 9 quality criteria)
- âœ… Step 9: Verify Integration with Main Program (maintained as a separate step)
- âœ… 9 Criteria: Omission, Ambiguity, Incorrect Fact, Redundancy, Inconsistency, Lack of Integration, Lower Cohesion, Higher Coupling, Strange Information
- âœ… Review integrated into the CLI/GUI verification process
- âœ… Total steps: 12 â†’ 13 (added integration verification after GUI)

**Changelog v1.7** (02/12/2025):
- âœ… **[CRITICAL]** Added Step 8.5: Code Review (BEFORE testing)
- âœ… 9 Quality Criteria: Omission, Ambiguity, Incorrect Fact, Redundancy, Inconsistency, Lack of Integration, Lower Cohesion, Higher Coupling, Strange Information
- âœ… Complete review checklist (36 verification items)
- âœ… Recommended tools (pylint, vulture, radon, black, isort)
- âœ… Detailed CLI and GUI review process
- âœ… Practical examples of problems and corrections
- âœ… Integration with Step 9 (test after review)
- âœ… Total steps: 12 â†’ 13 (8.5 added between 8 and 9)

**Changelog v1.6**:
- âœ… **[ADVANCED]** Added Step 9.2: Tests in Threads/Processes with Monitoring
- âœ… Execution of tests in a separate process (`multiprocessing.Process`)
- âœ… Real-time logging via `Queue` (progress of each test)
- âœ… Manual cancellation at any time (graceful Ctrl+C)
- âœ… Global + individual timeout (double protection)
- âœ… Real-time statistics (passed/failed/elapsed)
- âœ… Complete implementation of `test_runner_monitored.py` (~150 lines)
- âœ… Optional additional checklist (6 items)

**Changelog v1.5**:
- âœ… **[CRITICAL]** Added Step 9.1: Security in Tests
- âœ… 7 mandatory solutions to avoid infinite loops and timeouts
- âœ… Mandatory maximum timeout (30s per test)
- âœ… Mandatory headless environment for GUI tests (QT_QPA_PLATFORM=offscreen)
- âœ… Mandatory dry-run before executing tests (syntax + import + collect)
- âœ… Security checklist with 6 mandatory items
- âœ… Golden rules and safe commands documented
- âœ… Lessons learned from Task Example (infinite loop >1h)

**Changelog v1.4**:
- âœ… Reorganized final order: Implement â†’ Integrate GUI â†’ CLI â†’ Test â†’ Organize â†’ Document â†’ Commit
- âœ… Tests moved to AFTER integration checks (test integrated system)
- âœ… Organize root folder moved to BEFORE documentation (document clean state)
- âœ… Logic: Integrate â†’ Test integration â†’ Clean repository â†’ Document final state

**Changelog v1.3**:
- âœ… Reorganized step order: GUI and CLI Integration Verification now come BEFORE Documentation
- âœ… New order: Tests â†’ GUI Integration â†’ CLI â†’ Documentation â†’ Organize â†’ Commit
- âœ… Logic: Verifying integration before documenting ensures documentation reflects the actual state

**Changelog v1.2**:
- âœ… Added Step 8: Verify integration with main program
- âœ… Added Step 9: Verify CLI implementation with parameter passing
- âœ… Total steps: 10 â†’ 12

---

## ðŸŽ¯ Core Philosophy

> "There will always be complex tasks to do, but also those that are more difficult and those that are easier. **I want you to always start with the easier ones**."

**Principle**: From simple to complex, incremental, professional, and complete.

---

## âš ï¸ Golden Rule: Absolute Priority for Workspace Errors

> **CRITICAL FOR AIs**: Before implementing new features or continuing with tasks, **all workspace errors must be fixed BY YOU (AI)**.

### ðŸ¤– This Rule is For AI Assistants

**If you are an AI (Cursor, GitHub Copilot, etc.):**
- âœ… **YOU MUST** fix all existing errors BEFORE implementing new features
- âœ… **YOU MUST** resolve issues proactively, not wait for humans to fix them
- âœ… **YOU MUST** treat error correction as the highest priority
- âœ… **YOU MUST** clean the workspace before adding new code

**This rule does NOT mean:**
- âŒ That human developers must stop implementing when errors exist
- âŒ That the project cannot advance while errors are present
- âŒ That humans need to manually fix the errors

### ðŸš¨ Types of Errors That Block Development

Consider the existence of errors in the workspace (visible in the IDE's "Problems" tab) as **undesirable and blocking**. If any of the following types of errors occur, **fixing them is an absolute priority** before continuing:

1. **âŒ Syntax Issues**
   - Code parsing errors
   - Unclosed parentheses, braces, or brackets
   - Incorrect indentation (Python)
   - Missing semicolons (JavaScript, C, Java)

2. **âŒ Code Inconsistencies**
   - Variables declared but not used
   - Unused or missing imports
   - Dead code (unreachable code)
   - Type mismatches (TypeScript, Python with type hints)

3. **âŒ Unexpected Omissions**
   - Functions declared but not implemented
   - Missing required parameters
   - Missing return statements when expected
   - Missing mandatory documentation

4. **âŒ Incorrect Facts**
   - References to non-existent variables
   - Function calls with wrong number of arguments
   - Access to non-existent properties
   - Imports of non-existent modules

5. **âŒ Ambiguities**
   - Type checking warnings
   - Possible null/undefined references
   - Variable shadowing
   - Dangerous implicit type conversions

6. **âŒ Missing Files**
   - Dependencies not installed
   - Imported modules not found
   - Missing configuration files
   - Referenced but non-existent assets

7. **âŒ Execution Failures**
   - Build failures
   - Compilation errors
   - Failing tests
   - Linter errors (when configured)

### âœ… When You Can Continue

**ONLY** continue with development of new features when:

- âœ… **Zero errors** in the workspace "Problems" tab
- âœ… **All builds** complete successfully
- âœ… **All tests** pass (if they already exist)
- âœ… **Linter/formatter** doesn't report critical errors
- âœ… **Type checker** doesn't report errors (if applicable)

### ðŸ“‹ Checklist Before Each Task

```markdown
Before starting any new task:

[ ] Check IDE "Problems" tab (0 errors)
[ ] Run project build (success)
[ ] Run existing tests (all passing)
[ ] Run linter/formatter (no critical errors)
[ ] Verify imports and dependencies (all resolved)
[ ] Confirm code is in clean state (committable)
```

### â±ï¸ Estimated Time for Fixing

- **Syntax Errors**: ~2-5 minutes per error
- **Imports/Dependencies**: ~5-10 minutes
- **Type Errors**: ~5-15 minutes per error
- **Failing Tests**: ~10-30 minutes (depends on complexity)

**Rule of Thumb**: If you have >10 errors in the workspace, **dedicate 1-2 hours** to clean everything before proceeding.

### ðŸŽ¯ Rationale

**Why is this rule critical?**

1. **Cascade Prevention**: One uncorrected error can generate 10 new errors
2. **Code Quality**: Code with errors = immediate technical debt
3. **Reliability**: New features on top of broken code = guaranteed bugs
4. **Productivity**: Fixing old + new errors is more time-consuming than fixing only old ones
5. **Professionalism**: Clean, error-free code is a minimum requirement

**Message for AIs**: 
> "Until the errors are resolved BY YOU (AI), tasks and features cannot continue being implemented BY YOU (AI). Fix the errors first, then continue with implementation."

---

## ðŸŒ Code Language: Variable Naming and Comments

> **IMPORTANT FOR AIs**: The choice of language for variable names and comments should be defined at the beginning of the project, preferably during the first session of interaction with the programmer.

### ðŸ“‹ Default Rule

**By default**, when programming with artificial intelligence:
- âœ… **Variable names**: Should be in **English** (recommended for international projects)
- âœ… **Comments**: Should be in **English** (recommended for international projects)
- âœ… **Docstrings**: Should be in **English** (recommended for international projects)

**Note**: For Portuguese-speaking developers working on national projects, **Portuguese is the recommended default**. The AI should adapt based on the programmer's language preference.

**Justification**: Facilitates understanding and maintenance of code for developers, maintaining consistency with project documentation and communication. English is recommended for international projects, while native language (e.g., Portuguese) is recommended for national projects.

### ðŸ¤” Mandatory Question in First Session

**The AI MUST ask the programmer at the first moment (or during the first session)**:

```
â“ Code Language Preferences

To maintain consistency in the project, I need to define the default 
language for variable names and comments in the code:

ðŸ’¡ Suggestion: English (recommended for international projects)
   or Native Language (recommended for national projects)

Options:
A) ðŸ‡ºðŸ‡¸ English - Variables and comments in English (RECOMMENDED for international)
B) ðŸ‡§ðŸ‡· Native Language - Variables and comments in native language (RECOMMENDED for national)
C) ðŸŒ Mixed - Variables in English, comments in native language
D) âš™ï¸ Custom - Specify custom preference

What is your preference?
```

### âœ… Available Options

#### Option A: ðŸ‡ºðŸ‡¸ English (RECOMMENDED for International Projects)
```python
# âœ… Example in English
def calculate_total_price(items: List[Item]) -> float:
    """
    Calculates the total price of a list of items.
    
    Args:
        items: List of items to be summed
        
    Returns:
        Total price with taxes included
    """
    subtotal_price = sum(item.price for item in items)
    tax_rate = 0.15
    final_price = subtotal_price * (1 + tax_rate)
    return final_price
```

#### Option B: ðŸ‡§ðŸ‡· Native Language (e.g., Portuguese)
```python
# âœ… Exemplo em PortuguÃªs
def calcular_preco_total(itens: List[Item]) -> float:
    """
    Calcula o preÃ§o total de uma lista de itens.
    
    Args:
        itens: Lista de itens a serem somados
        
    Returns:
        PreÃ§o total com impostos incluÃ­dos
    """
    preco_subtotal = sum(item.preco for item in itens)
    taxa_imposto = 0.15
    preco_final = preco_subtotal * (1 + taxa_imposto)
    return preco_final
```

#### Option C: ðŸŒ Mixed (Variables in English, Comments in Native Language)
```python
# âœ… Mixed Example
def calculate_total_price(items: List[Item]) -> float:
    """
    Calcula o preÃ§o total de uma lista de itens.
    
    Args:
        items: Lista de itens a serem somados
        
    Returns:
        PreÃ§o total com impostos incluÃ­dos
    """
    subtotal_price = sum(item.price for item in items)
    tax_rate = 0.15  # Taxa de imposto de 15%
    final_price = subtotal_price * (1 + tax_rate)
    return final_price
```

### ðŸ“ Register the Preference

After the programmer's response, the AI should:

1. **Register the preference** in a visible location (e.g., README.md, CONTRIBUTING.md)
2. **Apply consistently** throughout all generated code
3. **Remember the preference** in future sessions of the same project

**Example Registration in README.md**:
```markdown
## ðŸŒ Code Conventions

- **Code Language**: English
- **Variables**: Names in English (e.g., `active_user`, `calculate_total`)
- **Comments**: In English
- **Documentation**: In English
```

### ðŸ”„ Preference Change

The programmer can request a language change at any time:
- âœ… "Switch to English from now on"
- âœ… "I prefer comments in Portuguese, but variables in English"
- âœ… "Use English only for public APIs"

**The AI should confirm the change** and update the conventions documentation.

### âš ï¸ Common Exceptions

Regardless of the language choice, **keep in English**:
- âœ… Library and framework names (e.g., `import pandas`, `from flask import`)
- âœ… Language keywords (e.g., `def`, `class`, `if`, `for`)
- âœ… Public API names (if code is distributed internationally)
- âœ… Technical terms without adequate translation (e.g., `callback`, `payload`, `refactoring`)

### ðŸŽ¯ Rationale

**Why ask the programmer?**

1. **Project Context**: National vs. international projects have different needs
2. **Team**: Brazilian team may prefer Portuguese; international team needs English
3. **Readability**: Code is read more times than written - should be clear for maintainers
4. **Consistency**: Defining standard at the start avoids confusing language mixing
5. **Professionalism**: Demonstrates attention to detail and respect for developer preferences

**Why English as recommended for international?**

For international/open-source projects:
- âœ… Universal programming language
- âœ… Easier collaboration with developers worldwide
- âœ… Better integration with English documentation and resources
- âœ… Industry standard for libraries and frameworks

**Why Native Language for national projects?**

For national/regional projects (e.g., Portuguese for Brazil/Portugal):
- âœ… Developers read and understand faster
- âœ… Facilitates onboarding of new team members
- âœ… Documentation and code in same language = less mental translation
- âœ… Variables represent business concepts in native language

**When to prefer English?**

- ðŸŒ International open-source project
- ðŸŒ Multicultural team
- ðŸŒ Product aimed at global market
- ðŸŒ Library/framework for public distribution

---

## ðŸ“§ Contact Methods for User Feedback

> **IMPORTANT FOR AIs**: During the first session of interaction with the programmer, the artificial intelligence must ask if the developer would like to include contact methods in the project so that users can provide feedback to those responsible.

### ðŸ“‹ Context and Purpose

Software projects greatly benefit from direct user feedback. Comments, suggestions, criticisms, complaints, compliments, and opinions are fundamental for the evolution and continuous improvement of the project.

### ðŸ¤” Mandatory Question in the First Session

**The AI MUST ask the programmer at the very first moment (or during the first session)**:

```
â“ Contact Methods for User Feedback

Would you like to include contact methods in the project so users
can send feedback (comments, suggestions, criticisms, complaints,
compliments, and opinions)?

ðŸ’¡ Suggestion: Yes (recommended for projects with end users)

Options:
A) âœ… Yes, include email for feedback (DEFAULT RECOMMENDED)
B) âœ… Yes, include GitHub Issues (for open-source projects)
C) âœ… Yes, include contact form in the application
D) âœ… Yes, include multiple channels (email + issues + form)
E) âŒ No, do not include contact methods

What is your preference?
```

### âœ… Available Options

#### Option A: âœ… Email for Feedback (DEFAULT RECOMMENDED)

**What to include**:
- Dedicated email for feedback
- All types of feedback are welcome:
  - ðŸ’¬ General comments
  - ðŸ’¡ Improvement suggestions
  - ðŸ› Constructive criticisms
  - ðŸ˜ž Complaints about problems
  - ðŸŽ‰ Compliments and recognition
  - ðŸ“ Opinions about features

**Where to document**:
```markdown
## ðŸ“§ Feedback and Contact

Your opinion is very important to us! Send your comments, 
suggestions, criticisms, complaints, compliments, and opinions to:

**Email**: feedback@yourproject.com

All feedback is read and considered for future improvements.
```

**Implementation example (README.md)**:
```markdown
## ðŸ“® Feedback

We'd love to hear from you! Send your comments, suggestions, 
criticisms, complaints, compliments, and opinions to:

- **Email**: contact@myproject.com
- **Response**: We typically respond within 48 hours

Your feedback helps us improve continuously!
```

#### Option B: âœ… GitHub Issues

**For open-source projects**:
```markdown
## ðŸ› Report Problems or Give Feedback

Use [GitHub Issues](https://github.com/your-user/your-project/issues) to:

- ðŸ› Report bugs
- ðŸ’¡ Suggest new features
- ðŸ’¬ Share general feedback
- â“ Ask questions

**Available templates**:
- Bug Report
- Feature Request  
- General Feedback
```

#### Option C: âœ… Contact Form in the Application

**For web/desktop applications**:
- Add "Feedback" or "Contact" section in the interface
- Form with fields:
  - Name (optional)
  - Email (for response)
  - Type: Comment | Suggestion | Criticism | Complaint | Compliment | Opinion
  - Message
- Send via email or save to database

**Implementation example (GUI)**:
```python
# Menu: Help â†’ Send Feedback
class FeedbackDialog(QDialog):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Send Feedback")
        
        # Feedback type
        self.type_combo = QComboBox()
        self.type_combo.addItems([
            "ðŸ’¬ Comment",
            "ðŸ’¡ Suggestion",
            "ðŸ› Criticism/Bug",
            "ðŸ˜ž Complaint",
            "ðŸŽ‰ Compliment",
            "ðŸ“ Opinion"
        ])
        
        # Email (optional)
        self.email_input = QLineEdit()
        self.email_input.setPlaceholderText("your@email.com (optional)")
        
        # Message
        self.message_text = QTextEdit()
        self.message_text.setPlaceholderText(
            "Share your comments, suggestions, criticisms, "
            "complaints, compliments, or opinions..."
        )
        
        # Send button
        self.send_button = QPushButton("Send Feedback")
        self.send_button.clicked.connect(self.send_feedback)
```

#### Option D: âœ… Multiple Channels

**Combine several options**:
```markdown
## ðŸ“ž Get in Touch

We value your feedback! You can contact us through:

### ðŸ“§ Email
- **General Feedback**: feedback@project.com
- **Technical Support**: support@project.com
- We respond within 48 hours

### ðŸ’¬ GitHub Issues
- Report bugs: [Issues](https://github.com/user/project/issues)
- Suggest features: [Discussions](https://github.com/user/project/discussions)

### ðŸŒ Contact Form
- Access: Menu â†’ Help â†’ Send Feedback
- Or: https://project.com/contact

### ðŸ“± Social Media
- Twitter: [@yourproject](https://twitter.com/yourproject)
- Discord: [Community](https://discord.gg/yourproject)
```

#### Option E: âŒ Do Not Include

**When to choose this option**:
- âš ï¸ Personal/internal projects without external users
- âš ï¸ Disposable prototypes
- âš ï¸ Single-use scripts

**Consequence**: Users will not have a direct channel for feedback, which may limit the project's evolution.

### ðŸ“ Register the Preference

After the programmer's response, the AI should:

1. **Add contact/feedback section** in README.md
2. **Create CONTACT.md file** (if needed) with details
3. **Implement form** (if application with interface)
4. **Document** in CONTRIBUTING.md (for open-source projects)

**Registration example (README.md)**:
```markdown
## ðŸ“¬ Feedback and Contact

This project values user feedback! 

- **Email**: feedback@project.com
- **Feedback types welcome**: Comments, suggestions, criticisms, 
  complaints, compliments, and opinions
- **Response time**: Within 48 business hours

Your feedback is essential for continuous improvement!
```

### ðŸŽ¯ Rationale

**Why ask about contact methods?**

1. **Continuous Improvement**: Direct feedback helps identify problems and opportunities
2. **Engagement**: Users who can give feedback feel more connected to the project
3. **Quality**: Criticisms and suggestions improve software quality
4. **Prioritization**: Feedback helps understand what is most important to users
5. **Recognition**: Compliments motivate the development team
6. **Transparency**: Open channel demonstrates commitment to users

**Why Email as default?**

For projects with users:
- âœ… **Universal**: Everyone has email
- âœ… **Simple**: Doesn't require account or additional registration
- âœ… **Direct**: Private and personal communication
- âœ… **Consolidated**: All types of feedback in a single channel
- âœ… **Traceable**: Complete history of communications
- âœ… **Professional**: Formal channel suitable for any type of feedback

**When to prefer other options?**

- ðŸŒ **GitHub Issues**: Open-source projects (public transparency)
- ðŸŒ **Form**: Apps with many users (organization and categorization)
- ðŸŒ **Multiple channels**: Large projects (different audiences, different needs)
- ðŸŒ **None**: Internal/personal projects without external users

### âš ï¸ Important Considerations

**Feedback Management**:
- âœ… Define who will respond to feedback (responsible person)
- âœ… Establish expected response time (SLA)
- âœ… Create process for triage and prioritization
- âœ… Document relevant feedback (issues, backlog)
- âœ… Always thank, even for criticisms

**Privacy**:
- âœ… Inform how contact data will be used
- âœ… Do not share emails without permission
- âœ… GDPR/LGPD compliance if applicable

**Best practices example**:
```markdown
## ðŸ“§ Feedback Policy

**We commit to**:
- âœ… Respond to all feedback within 48 business hours
- âœ… Treat all opinions with respect
- âœ… Seriously consider criticisms and suggestions
- âœ… Maintain contact data privacy (GDPR/LGPD)
- âœ… Thank constructive contributions

**You can expect**:
- Personalized response (not automated)
- Updates on implemented suggestions
- Recognition in changelogs (if desired)
```

---

## ðŸ“Š Recursive Division of Complex Tasks

> **IMPORTANT**: If the task is very long or complex, and there are time limits or response length limits, the artificial intelligence should divide the task into smaller parts, recursively, until achieving a task that can provide a satisfactory response according to the determined response limit.

### ðŸ”„ Division Strategy (Enterprise)

**When to Apply** (Simplicity Protocol 2):
- âœ… Task estimated at >6 hours (divide into 3+ sprints)
- âœ… Enterprise feature with multiple stakeholders
- âœ… Very long response (>1500 lines of code)
- âœ… Multiple interdependent functionalities
- âœ… Requires peer code review at each phase
- âœ… Risk of timeout or response limit

**How to Divide** (Recursively with ADRs):

1. **Level 1 - Epics (2-4 weeks)**:
   ```
   Epic: "Enterprise Payment System"
   â†“ Divide into:
   â”œâ”€â”€ Sprint 1: Stripe Integration (6h)
   â”œâ”€â”€ Sprint 2: Webhooks and notifications (6h)
   â”œâ”€â”€ Sprint 3: Transaction dashboard (8h)
   â”œâ”€â”€ Sprint 4: Audit and compliance (6h)
   â””â”€â”€ Sprint 5: Rollback and recovery (4h)
   
   Each sprint â†’ Documented ADR
   Each sprint â†’ Peer code review
   Each sprint â†’ Incremental deployment
   ```

2. **Level 2 - Sprints (4-8 hours)**:
   ```
   Sprint 1: Stripe Integration
   â†“ Divide into:
   â”œâ”€â”€ Task 1.1: Setup API keys + secrets (1h)
   â”œâ”€â”€ Task 1.2: Checkout session endpoint (2h)
   â”œâ”€â”€ Task 1.3: Webhook receiver (2h)
   â””â”€â”€ Task 1.4: Tests + Security checklist (1h)
   
   Each task â†’ Quality gates (CI/CD)
   ```

3. **Level 3 - Tasks (<4 hours)** (if still too large):
   ```
   Task 1.2: Checkout session endpoint
   â†“ Divide into:
   â”œâ”€â”€ Subtask 1.2.1: Order schema (30min)
   â”œâ”€â”€ Subtask 1.2.2: Input validation (30min)
   â”œâ”€â”€ Subtask 1.2.3: Stripe session creation (1h)
   â”œâ”€â”€ Subtask 1.2.4: Logging and monitoring (30min)
   â””â”€â”€ Subtask 1.2.5: Unit tests (1h)
   ```

**Stopping Criteria**:
- â±ï¸ Task can be completed in <4 hours (vs <3h in Simplicity 1)
- ðŸ“ Response fits within reasonable limit (<1000 lines)
- âœ… Clear scope with defined acceptance criteria
- ðŸ§ª Can be tested in isolation
- ðŸ‘¥ Can be peer-reviewed in <1h
- ðŸ”’ Security checklist can be applied in isolation
- ðŸ¤– CI/CD can validate in isolation

**Enterprise Division Principles**:
1. **Independence**: Each subtask must be independently deployable
2. **Cohesion**: Related subtasks should be close in sequence
3. **Incremental Value**: Each subtask should add measurable value
4. **Testability**: Each subtask must have 100% test coverage
5. **Reversibility**: Each subtask must have rollback plan (if critical)
6. **Documentation**: Each sprint must have ADR if architectural decision
7. **Reviewability**: Each subtask must have small diff for code review

**Practical Enterprise Example**:
```markdown
âŒ BAD - Epic too large (60h):
[ ] Implement complete e-commerce platform

âœ… GOOD - Divided into epics and sprints:

Epic 1 - Product Catalog (2 weeks):
â”œâ”€â”€ Sprint 1.1 (6h): Product CRUD + categories
â”‚   â”œâ”€â”€ ADR-001: PostgreSQL choice
â”‚   â””â”€â”€ Rollback plan: N/A (non-critical)
â”œâ”€â”€ Sprint 1.2 (6h): Search and filters
â”‚   â””â”€â”€ ADR-002: ElasticSearch vs PostgreSQL full-text
â””â”€â”€ Sprint 1.3 (4h): Image upload (S3)
    â””â”€â”€ Rollback plan: Revert to local storage

Epic 2 - Shopping Cart (1 week):
â”œâ”€â”€ Sprint 2.1 (6h): Session-based cart
â”‚   â”œâ”€â”€ ADR-003: Redis for sessions
â”‚   â””â”€â”€ Security checklist: Session fixation, CSRF
â””â”€â”€ Sprint 2.2 (4h): Persistence and checkout
    â””â”€â”€ Rollback plan: Fallback to in-memory

Epic 3 - Payments (2 weeks):
â”œâ”€â”€ Sprint 3.1 (6h): Stripe Integration
â”‚   â”œâ”€â”€ ADR-004: Stripe vs PayPal
â”‚   â”œâ”€â”€ Security checklist: PCI-DSS compliance
â”‚   â””â”€â”€ Rollback plan: CRITICAL (feature flag)
â”œâ”€â”€ Sprint 3.2 (6h): Webhooks
â”‚   â””â”€â”€ Security checklist: Webhook validation
â””â”€â”€ Sprint 3.3 (4h): Transaction dashboard
    â””â”€â”€ Rollback plan: N/A (visualization only)

Each Sprint:
- Code review by 2 peers
- CI/CD quality gates (80% coverage)
- Security scan (bandit + pip-audit)
- Deploy staging â†’ production
```

**Decision Matrix for Division**:
Use Decision Matrix (Step 2.5) when there are multiple ways to divide:

| Division | Complexity | Risk | Value | Independence | **Score** |
|---------|------------|------|-------|--------------|-----------|
| **By functionality** | 3 | 2 | 5 | 5 | **23** ðŸŸ¢ |
| By layer (backend/frontend) | 2 | 4 | 3 | 2 | **17** ðŸŸ¡ |
| By team | 4 | 3 | 2 | 3 | **18** ðŸŸ¡ |

**Why?**: Dividing enterprise tasks ensures incremental deliveries with value, facilitates code review, allows granular rollback, and maintains stable velocity in large teams.

---

## ðŸ“‹ Protocol Backbone (24 Steps: 14 Mandatory + 10 Optional)

### **Mandatory Steps** (Simplicity Protocol 1):
1. ðŸ“š Read the documentation
2. âœ… Choose simpler tasks
3. â“ Ask questions until 100% clarity
4. ðŸ” Analyze and study the project
5. ðŸŽ¯ Sprint the simpler tasks
6. ðŸ’» Implement with professional architecture (GoF + GRASP)
   - 6.6 ðŸŽ¨ **Project Icons** (MANDATORY)
7. âŒ¨ï¸ Verify CLI Implementation + Code Review (9 criteria)
8. ðŸ–¥ï¸ Verify GUI Implementation + Code Review (9 criteria)
9. ðŸ”— Verify Integration with Main Program
10. ðŸ§ª Run tests (100% coverage)
11. ðŸ§¹ Organize project root folder
12. ðŸ“ Fill in documentation
13. ðŸš€ Commit and push

### **Advanced Optional Steps** (Simplicity 2):
**2.5** ðŸŽ¯ Decision Matrix (objective choice) - **HIGH PRIORITY**
**6.5** ðŸ”’ Security Checklist (OWASP) - **HIGH PRIORITY**
**6.7** ðŸ“š Generate API Documentation
**8.5** â™¿ Accessibility Checklist (WCAG)
**9.5** ðŸ‘¥ Peer Code Review
**10.5** âš¡ Profiling and Optimization
**10.6** âœ… Quality Metrics (CI/CD) - **HIGH PRIORITY**
**11.5** ðŸ“‹ Create ADR (Architectural Decision Records)
**12.5** ðŸ”™ Rollback Plan
**13.5** ðŸ”„ Sprint Retrospective

### 1ï¸âƒ£ **Read the Documentation**
- Consult `TASKS.md` (or equivalent file defined by the user) to see pending tasks
- Consult `docs/REQUIREMENTS.md` to understand the project context
- Review previous specifications (`v2.9.X-SPECIFICATIONS.md`)
- Understand dependencies and existing architecture
- Check examples in `tests/files/` when applicable

**ðŸ“‹ About the Task File**:

The `TASKS.md` file is the **default file** for managing project tasks, but you can use any ASCII format file (`.txt`, `.md`, etc.) according to your preference.

**Task File Requirements**:
- âœ… **ASCII format mandatory**: `.md`, `.txt` or similar (readable as plain text)
- âŒ **NOT accepted**: `.docx`, `.pdf`, or binary formats
- ðŸ“ **Location**: Project root or in `docs/` (e.g., `TASKS.md`, `TODO.md`, `requirements.md`)
- ðŸ”„ **Alternative**: If you prefer another name/location, specify at project start

**If no task file exists**:
1. AI should ask the user: "Which file do you use to manage tasks?"
2. If none exists, suggest creating the default `TASKS.md`
3. Confirm file location and name with the user

**Why?**: Avoid rework and ensure coherence with existing code. The task file centralizes project planning and progress.

**ðŸ“‹ About Action Plans (ACTION_PLANS.md)**:

In addition to `TASKS.md`, you may find (or create) an `ACTION_PLANS.md` file for **tasks requiring detailed step-by-step guidance**.

**What are Action Plans?**
- ðŸŽ¯ **Practical roadmaps** with numbered intermediate steps for complex tasks
- âš¡ **More urgent and detailed** than TASKS.md items
- ðŸ”§ **Applicable to**: Maintenance, Correction, Evolution, Adaptation

**Difference between TASKS.md and ACTION_PLANS.md:**
- **TASKS.md**: List of general tasks ("WHAT to do") - e.g., `[ ] Implement OAuth2 authentication`
- **ACTION_PLANS.md**: Detailed execution guide ("HOW to do it") - e.g.:
  ```
  PLAN #01: Implement OAuth2
  â”œâ”€ Step 1: Install passport.js library
  â”œâ”€ Step 2: Configure Google OAuth strategy
  â”œâ”€ Step 3: Create /auth/google routes
  â””â”€ Step 4: Add tests
  ```

**When to use Action Plans:**
- âœ… Complex task with multiple interdependent steps
- âœ… Critical bug requiring step-by-step diagnosis
- âœ… Refactoring affecting multiple modules
- âœ… Technology migration or framework update

**Specifics for Simplicity 2 (Enterprise):**
- ðŸ¤ **Team validation**: Action plans must be reviewed by peers before execution
- ðŸ“‹ **Create ADR**: For action plans involving significant architectural decisions (see Step 11.5)
- ðŸ‘¥ **Clear assignment**: Each plan step must have assigned responsible person
- ðŸ“Š **Metrics**: Include measurable success metrics for each step

**Default location**: `docs/ACTION_PLANS.md`

**Template for Simplicity 2 (Enterprise):**
```markdown
## ðŸŽ¯ ACTION PLAN #[ID]: [Title]
**ðŸ“… Created on**: YYYY-MM-DD
**âš¡ Priority**: ðŸ”´ Critical | ðŸŸ¡ High | ðŸŸ¢ Normal
**ðŸ·ï¸ Type**: Maintenance | Correction | Evolution | Adaptation
**ðŸ‘¤ Lead**: [Lead name]
**ðŸ‘¥ Reviewers**: [Reviewer names]

### ðŸ“ Context
[Why was this plan created?]

### ðŸŽ¯ Final Objective
[What will be achieved?]

### ðŸ“‹ Intermediate Steps
- [ ] **Step 1**: [Description]
  - **Responsible**: [Name]
  - **Completion criteria**: [...]
  - **Success metrics**: [...]
  
- [ ] **Step 2**: [Description]
  - **Responsible**: [Name]
  - **Completion criteria**: [...]
  - **Dependencies**: Step 1
[...]

### âœ… Completion Criteria
- [ ] All steps completed
- [ ] Code Review approved (see Step 9.5)
- [ ] Tests passing
- [ ] Documentation updated
- [ ] ADR created (if architectural decision)
```

**Workflow with Action Plans (Enterprise):**
1. Consult TASKS.md to see pending tasks
2. If complex task â†’ create detailed Action Plan
3. **Team review**: Validate plan before starting execution
4. **Assign responsible parties** for each step
5. Execute step by step, with code review of each stage
6. Upon completion â†’ mark task in TASKS.md as complete
7. **Retrospective**: Discuss lessons learned (Step 13.5)
8. Move plan to history or separate file

**Benefits for teams:**
- âœ… **Coordination**: Everyone knows who does what and when
- âœ… **Quality**: Multiple reviews reduce errors
- âœ… **Shared knowledge**: Plan documents process for entire team
- âœ… **Onboarding**: New members learn from previous plans

ðŸ“– **Complete details on Action Plans**: See README.md in repository, section "Action Plans (ACTION_PLANS.md)"

---

### 2ï¸âƒ£ **Choose the Simplest Tasks**
- **Golden Rule**: Always start with the tasks **easiest to implement**
- Even in a list of complex tasks, **there are always some simpler than others**
- Proportionality: balance simplicity vs. impact

**Simplicity Criteria**:
- âœ… Fewer dependencies
- âœ… Well-defined and clear scope
- âœ… Fewer files to modify
- âœ… Lower risk of breaking existing functionalities
- âœ… Can be tested in isolation

**Real Example**:
```
List of remaining complex tasks:
[ ] Complex Feature Example (VERY COMPLEX - 50h)
[ ] Semantic AI Search (COMPLEX - 20h)
[ ] Tooltip preview on hover (SIMPLE - 30min) âœ… START HERE!
```

---

### 2ï¸âƒ£.5ï¸âƒ£ **Decision Matrix for Objective Task Selection** â­ NEW - HIGH PRIORITY

**Problem**: "Simplest" is subjective and can lead to incorrect choices
**Solution**: Objective scoring matrix with 5 quantifiable criteria

**Scoring Criteria** (0-5 points each):

1. **Technical Simplicity** (5=very simple, 0=very complex)
   - Amount of code needed
   - Algorithmic complexity
   - Amount of new concepts

2. **Dependencies** (5=zero dependencies, 0=many)
   - Files to modify
   - Modules that depend on this feature
   - External libraries required

3. **Impact** (5=high impact, 0=low)
   - Value for the end-user
   - Expected frequency of use
   - Benefit vs. effort

4. **Clarity of Requirements** (5=100% clear, 0=ambiguous)
   - Complete specification
   - Usage examples provided
   - Acceptance criteria defined

5. **Risk** (5=zero risk, 0=high risk)
   - Probability of breaking existing code
   - Reversibility of the change
   - Impact on critical features

**Prioritization Formula**:
```
Priority = (Simplicity Ã— 2) + Dependencies + (Impact Ã— 1.5) + Clarity + Risk

Maximum Score: 35 points
Minimum Score: 0 points
```

**Interpretation**:
- **30-35 points**: ðŸŸ¢ IDEAL - Start immediately
- **20-29 points**: ðŸŸ¡ GOOD - Strongly consider
- **10-19 points**: ðŸŸ  MEDIUM - Evaluate context
- **0-9 points**: ðŸ”´ COMPLEX - Leave for last

**Practical Application Example**:

| Task | Simpl<br>(0-5) | Dep<br>(0-5) | Imp<br>(0-5) | Clar<br>(0-5) | Risc<br>(0-5) | **Score** | **Decision** |
|---|---|---|---|---|---|---|---|
| **Tooltip Preview** | 5 | 5 | 3 | 5 | 5 | **33.5** ðŸŸ¢ | **1st - START HERE** |
| **Feature** | 3 | 4 | 4 | 5 | 4 | **26.0** ðŸŸ¡ | 2nd |
| **Integrated Editor** | 1 | 2 | 5 | 4 | 2 | **20.5** ðŸŸ¡ | 3rd |
| **Semantic AI** | 0 | 1 | 4 | 2 | 1 | **10.0** ðŸŸ  | 4th - Leave for last |

**"Tooltip Preview" Example Details**:
- **Simplicity: 5** - Just add QToolTip to existing widgets
- **Dependencies: 5** - Modify only 1 GUI file
- **Impact: 3** - Improves UX but not critical
- **Clarity: 5** - Requirement 100% clear (show preview on hover)
- **Risk: 5** - Zero risk of breaking anything (only adds tooltip)
- **Total: (5Ã—2) + 5 + (3Ã—1.5) + 5 + 5 = 33.5 points** ðŸŸ¢

**Template for Filling**:
```markdown
## Decision Matrix - Sprint vX.X.X

| Task ID | Simplicity | Dependencies | Impact | Clarity | Risk | **Score** | Order |
|---|---|---|---|---|---|---|---|
| #XX | ? | ? | ? | ? | ? | **?** | ? |
| #YY | ? | ? | ? | ? | ? | **?** | ? |

**Justification for Choice**:
Task #XX chosen because:
- Highest score (XX points)
- [specific reason]
- [specific reason]
```

**When Not to Use the Matrix**:
- âŒ Only 1 task available (no choice)
- âŒ Urgent/blocking task (ignore score)
- âŒ Critical production bugfix (absolute priority)

**Why use it**:
- âœ… **Objectivity**: Eliminates personal bias
- âœ… **Traceability**: Justifies decisions
- âœ… **Learning**: Improves future estimates
- âœ… **Communication**: Easy to explain choice to the team

---

### 3ï¸âƒ£ **Ask the Programmer Questions and More Questions**
- **CRITICAL**: Never assume or guess requirements
- Ask **all necessary questions** until **100% clarity**
- Validate understanding before starting implementation
- ðŸ¤– **[NEW v2.1]** AI **CAN and IS HIGHLY RECOMMENDED** to provide **suggestions and guesses** for the answer to each question (optional, but encouraged)

**Recommended Question Format with Suggestions**:
```
â“ Question: "How should it behave when [scenario X]?"
ðŸ’¡ AI Suggestion: "Based on existing code, I suggest [option A] because [reason Y]."
Options: A) [option A] | B) [option B] | C) [option C]
```

**Why AI Suggestions Are Important**:
- âœ… Accelerates decisions when the programmer is undecided
- âœ… AI has context of existing code and can suggest consistent patterns
- âœ… Reduces programmer's cognitive load (they just validate, not create from scratch)
- âœ… Maintains quality: AI suggests based on already implemented best practices

**Question Categories**:
1. **Functional Requirements**:
   - "How should it behave when [scenario X]?"
   - "What happens if the user [action Y]?"
   - "What is the priority between [option A] and [option B]?"

2. **Technical Requirements**:
   - "Should I use [library X] or build from scratch?"
   - "What is the expected output format?"
   - "Are there any performance restrictions?"

3. **Edge Cases**:
   - "What if the file is empty?"
   - "What if there are special characters?"
   - "How to handle None/null values?"

4. **Integration**:
   - "Does it need to integrate with [existing module]?"
   - "Should I maintain compatibility with [previous version]?"
   - "Where should the results be saved?"

5. **Understanding Validation**:
   - "I understand you want [X]. Is that correct?"
   - "My proposed solution is [Y]. Does it make sense?"
   - "Can I start, or did I miss anything?"

**Real Example (Task Example)**:
```
â“ "Get first N words (how many? 3-5?)?"
âœ… Answer: "Default can be 30 characters"

â“ "Convert to camelCase removing accents?"
âœ… Answer: "Yes, accents should be removed"

â“ "Name conflicts: how to resolve?"
âœ… Answer: "If they have the same parent key, don't touch. Smaller line wins."
```

**Why?**: Saves time, avoids rework, ensures the solution meets exactly what was requested.

#### ðŸŒ **Online Research: When AI Has Many Doubts or Difficulties**

> **IMPORTANT FOR AIs**: If you have **many doubts** about how to solve a particular problem, or have **significant difficulties** solving that same problem, **remember that you can perform online research**.

**When to Perform Online Research**:
- âœ… When there are complex technical doubts that you cannot resolve alone
- âœ… When you encounter a critical problem that has no obvious solution in the existing code
- âœ… When you need to understand how to implement something completely new
- âœ… When there is an error or bug that is difficult to diagnose
- âœ… When you need practical implementation examples

**Where to Search** (in order of priority):
1. **ðŸ“š Official GitHub Documentation of Related Projects**:
   - Similar repositories or those that solve similar problems
   - Issues and Pull Requests discussing similar problems
   - Wiki and technical documentation of related open source projects

2. **ðŸ“– Online Documentation Platforms**:
   - Official documentation of libraries and frameworks used in the project
   - Specialized tutorials and technical guides
   - Technical blogs and articles from experienced developers

3. **ðŸ’¬ Question and Answer Platforms**:
   - **StackOverflow**: Main platform for programming questions
   - **GitHub Discussions**: For project-specific questions
   - Other technical communities relevant to the project's technology

**Why Online Research Is Important**:
- âœ… **Saves time**: Complex problems may already have documented solutions
- âœ… **Best practices**: Learn from implementations already validated by the community
- âœ… **Avoid reinventing the wheel**: Many problems have already been solved by other developers
- âœ… **Reduces errors**: Solutions tested and approved by the community have fewer bugs
- âœ… **Updates**: Discover the most modern and efficient approaches

**Example Flow with Online Research**:
```
1. â“ I tried to implement [feature X] but encountered [problem Y]
2. ðŸ” I searched on GitHub: "similar implementation [feature X]"
3. ðŸ“š I found 3 similar projects that solve this in different ways
4. ðŸ’¡ I analyzed the examples and identified the most appropriate approach for our context
5. âœ… I implemented based on the best practices found
6. ðŸ“ I documented the solution source for future reference
```

**âš ï¸ Important**: Always cite the consulted sources in the project documentation for future reference and traceability.

---

### 4ï¸âƒ£ **Analyze and Study the Project**
- **CRITICAL**: After understanding all doubts, **study the code before implementing**
- Read relevant documentation (README, docs/, comments in code)
- Understand existing architecture and patterns used
- Check dependencies and necessary imports
- Identify reusable functions/classes

**Analysis Checklist**:
1. **Documentation Review**:
   - `docs/REQUIREMENTS.md` - General project context
   - `docs/SPECIFICATIONS.md` - Specifications of previous versions
   - `README.md` - Overview and usage instructions
   - Docstrings of related modules

2. **Existing Code Analysis**:
   - Find modules similar to what will be implemented
   - Identify design patterns already used (GoF, GRASP)
   - Check naming conventions and structure
   - Locate reusable helper functions

3. **Dependency Mapping**:
   - Which modules need to be imported?
   - Are there name or version conflicts?
   - Which base classes or mixins should be inherited?
   - Where should new files be created?

4. **Compatibility Validation**:
   - Will the solution break existing code?
   - Is it necessary to refactor something before implementing?
   - Are there tests that need to be updated?
   - Will the public API be maintained?

**Real Example (Task Example - Tutorials)**:
```
âœ… Analyzed: Other docks (ComponentA, ComponentB)
âœ… Identified: BaseDock pattern with FileInputMixin
âœ… Verified: QTreeWidget + QTextBrowser for navigation
âœ… Studied: How other modules convert markdown â†’ HTML
âœ… Located: Where to add imports in app.py
âœ… Confirmed: Menu structure in _build_menu()
â†’ Result: Implementation in 2h instead of 5h (60% savings)
```

**Why?**: Avoids refactoring, saves time, ensures consistent code with the existing codebase.

---

### 5ï¸âƒ£ **Sprint the Simplest Tasks**
- Group 2-4 related tasks into a sprint
- Estimate total time: **maximum 3-4 hours** per sprint
- Maintain focus: **one sprint = one version (e.g., vX.Y.Z)**

**âš ï¸ Important - Task Division into Subtasks**:
> Tasks should be divided into smaller parts **only if really necessary**, that is:
> - âœ… When there is **higher probability of exceeding the maximum time** (>4h)
> - âœ… When there is **higher possibility the response will be too long** (complex implementation)
> - âŒ **DO NOT divide** if the task is reasonably simple and fits within the time limit
> 
> This decision should be made by the **artificial intelligence responsible for programming** the project, based on the real complexity of the task.

**Sprint Structure**:
```
Sprint vX.Y.Z (Task Example):
â”œâ”€â”€ Task Example: Feature Update (estimated 3h)
â”‚   â”œâ”€â”€ Subtask 1: Ask programmer questions (15min)
â”‚   â”œâ”€â”€ Subtask 2: extract_all_keys_from_obj() (45min)
â”‚   â”œâ”€â”€ Subtask 3: build_substitution_map_by_value() (45min)
â”‚   â”œâ”€â”€ Subtask 4: Integration into cli_dedupe() (30min)
â”‚   â”œâ”€â”€ Subtask 5: Unit tests (60min)
â”‚   â””â”€â”€ Subtask 6: Documentation (30min)
â””â”€â”€ Total: 3h45min âœ…
```

---

### 6ï¸âƒ£ **Implement from Simple to Complex with Professional Architecture**
- **Within each task**, start with the easiest part
- Build incrementally: helper function â†’ main function â†’ integration
- Test each part before moving on

**Implementation Order**:
1. **Helper functions** (e.g., `extract_all_keys_from_obj()`)
2. **Main functions** (e.g., `build_substitution_map_by_value()`)
3. **Integration** (e.g., update `cli_dedupe()`)
4. **GUI/UX** (if applicable)
5. **Optimizations** (last step)

**Architectural Principles (Mandatory)**:

#### ðŸ”„ **Code Reusability with Modules**
- Create separate modules for each responsibility
- Avoid duplication (DRY - Don't Repeat Yourself)
- Generic functions reusable in multiple contexts

**Example**:
```python
# âœ… GOOD: Reusable module
# src/utils/file_utils.py
def read_file_safe(path: str) -> Optional[str]:
    """Function reused in 10+ places"""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        logger.error(f"Error reading {path}: {e}")
        return None

# âŒ BAD: Duplicate code in each module
# (repeats try/except 20 times)
```

#### ðŸŒ³ **Import Tree Analogy**

**Concept**: A program's import structure can be visualized as a tree, where each module imports other modules, forming a dependency hierarchy.

**Unlimited Depth**: This tree can reach **any level or height** depending on program complexity:
- **Simple Programs**: Shallow tree (2-3 levels)
  ```
  main.py
  â””â”€â”€ utils.py
      â””â”€â”€ helpers.py
  ```

- **Medium Programs**: Moderate tree (4-6 levels)
  ```
  app.py
  â”œâ”€â”€ controllers/
  â”‚   â””â”€â”€ user_controller.py
  â”‚       â””â”€â”€ services/
  â”‚           â””â”€â”€ user_service.py
  â”‚               â””â”€â”€ models/
  â”‚                   â””â”€â”€ user.py
  â””â”€â”€ config.py
  ```

- **Complex Programs**: Deep tree (7+ levels)
  ```
  enterprise_app.py
  â”œâ”€â”€ api/
  â”‚   â”œâ”€â”€ routes/
  â”‚   â”‚   â””â”€â”€ v1/
  â”‚   â”‚       â””â”€â”€ users.py
  â”‚   â”‚           â””â”€â”€ handlers/
  â”‚   â”‚               â””â”€â”€ authentication.py
  â”‚   â”‚                   â””â”€â”€ providers/
  â”‚   â”‚                       â””â”€â”€ oauth/
  â”‚   â”‚                           â””â”€â”€ google.py
  â”‚   â”‚                               â””â”€â”€ scopes.py
  ```

**Application in Refactoring**:

1. **Identify Excessive Depth**:
   - âœ… If tree > 8 levels â†’ Consider simplification
   - âœ… Very deep modules = difficult maintenance

2. **Detect Circular Dependencies**:
   ```python
   # âŒ BAD: Circular dependency
   # module_a.py
   from module_b import B
   
   # module_b.py
   from module_a import A  # Circular!
   ```

3. **Reorganize by Cohesion**:
   ```python
   # âœ… GOOD: Group related imports
   # before (dispersed):
   from utils.string import normalize
   from helpers.text import clean
   from tools.format import sanitize
   
   # after (cohesive):
   from text_processing import normalize, clean, sanitize
   ```

4. **Reduce Coupling**:
   - âœ… Direct imports only of what's necessary
   - âœ… Avoid `from module import *` (increases coupling)
   - âœ… Use interfaces/abstractions to decouple

5. **Visualize to Understand**:
   - Use tools like `pydeps`, `import-graph` (Python)
   - Identify "hubs" (heavily imported modules)
   - Refactor central modules to reduce impact

**Why it's important**:
- âœ… **Comprehension**: Clear tree = easier to understand code
- âœ… **Maintenance**: Organized dependencies = localized changes
- âœ… **Performance**: Fewer unnecessary imports = faster startup
- âœ… **Testing**: Independent modules = isolated tests
- âœ… **Refactoring**: Visualizing tree helps identify improvement opportunities

#### ðŸ“¦ **Hierarchies and Encapsulation**
- Use classes when there is shared state
- Encapsulate private attributes (`_attribute`)
- Expose only necessary public interface

**Example**:
```python
# âœ… GOOD: Proper encapsulation
class ReferenceUpdater:
    def __init__(self, project_dir: str):
        self._project_dir = project_dir
        self._substitutions = {}
    
    def update_references(self) -> Dict[str, int]:
        """Clear public interface"""
        self._scan_files()  # Private method
        self._build_map()   # Private method
        return self._apply_changes()

# âŒ BAD: Everything exposed, no structure
def do_everything(dir, old, new, backup, ext):
    # 200 lines without organization
```

#### ðŸŽ¯ **High Cohesion and Low Coupling**
- **High Cohesion**: Each module/class has a single, clear responsibility
- **Low Coupling**: Independent modules, communication via interfaces

**Example**:
```python
# âœ… HIGH COHESION: Each class does ONE thing
class KeyExtractor:
    """Only extracts keys from structures"""
    def extract(self, data) -> Dict[str, str]: ...

class SubstitutionMapBuilder:
    """Only builds substitution maps"""
    def build(self, old, new) -> Dict[str, str]: ...

class FileUpdater:
    """Only updates files"""
    def update(self, files, map) -> int: ...

# âœ… LOW COUPLING: Communication via interfaces
class ReferenceUpdater:
    def __init__(self, extractor: KeyExtractor, builder: SubstitutionMapBuilder):
        self._extractor = extractor  # Dependency injection
        self._builder = builder

# âŒ BAD: Low cohesion, high coupling
class EverythingManager:
    def do_all(self):
        # Does extraction + building + updating + logging + GUI
        # Imports 20 different modules
        # Impossible to test in isolation
```

#### ðŸ—ï¸ **GoF (Gang of Four) Patterns**
Apply design patterns when appropriate:

1. **Strategy Pattern** (algorithm choice at runtime):
```python
class CaseConverter:
    def __init__(self, strategy: CaseStrategy):
        self._strategy = strategy
    
    def convert(self, text: str) -> str:
        return self._strategy.apply(text)

class CamelCaseStrategy(CaseStrategy):
    def apply(self, text: str) -> str: ...

class SnakeCaseStrategy(CaseStrategy):
    def apply(self, text: str) -> str: ...
```

2. **Factory Pattern** (creation of complex objects):
```python
class ProcessorFactory:
    @staticmethod
    def create(type: str) -> Processor:
        if type == "data":
            return DATAProcessor()
        elif type == "ts":
            return TypeScriptProcessor()
```

3. **Observer Pattern** (event notification):
```python
class ProcessingModal(QDialog):
    cancel_requested = Signal()  # Observer pattern
    
    def _on_cancel_clicked(self):
        self.cancel_requested.emit()  # Notifies observers
```

4. **Command Pattern** (undo/redo):
```python
class ReplaceCommand:
    def __init__(self, file: str, old: str, new: str):
        self._file = file
        self._old = old
        self._new = new
    
    def execute(self): ...
    def undo(self): ...
```

#### ðŸŽ¨ **GRASP (General Responsibility Assignment Software Patterns) Patterns**

1. **Information Expert**: Assign responsibility to the one who has the information
```python
# âœ… GOOD: Dictionary has the info, so it has the method
class DataStore:
    def __init__(self, data: dict):
        self._data = data
    
    def get_value(self, key_path: str) -> Optional[str]:
        """Dictionary knows its structure"""
        return self._navigate_path(key_path)

# âŒ BAD: External class manipulates internal structure
def get_value_from_dict(dict_data, key_path):
    # Direct access to the internal structure of the dict
```

2. **Creator**: Class A creates B if A contains/aggregates B
```python
# âœ… GOOD: RewriterDock creates its own widgets
class ComponentB(BaseDock):
    def __init__(self):
        self._create_widgets()  # Creator pattern
        self._setup_layout()
    
    def _create_widgets(self):
        self.ed_input = QLineEdit()  # Creates its children
        self.btn_process = QPushButton()
```

3. **Controller**: Delegate system operations to controller
```python
# âœ… GOOD: Controller coordinates operations
class RewriterController:
    def process_file(self, path: str):
        data = self._reader.read(path)
        processed = self._processor.process(data)
        self._writer.write(path, processed)

# âŒ BAD: GUI does everything directly
class RewriterDock:
    def on_button_click(self):
        # 50 lines of business logic in the GUI
```

4. **Low Coupling**: Minimize dependencies
```python
# âœ… GOOD: Generic interface
def update_references(updater: ReferenceUpdater):
    """Accepts any updater that implements the interface"""
    updater.update()

# âŒ BAD: Concrete dependency
def update_references(file_path: str, backup: bool, ext: list):
    """Many parameters, high coupling"""
```

5. **High Cohesion**: One class, one responsibility
```python
# âœ… GOOD: High cohesion
class FileReader:
    """Only reads files"""
    def read(self, path: str) -> str: ...

class DataValidator:
    """Only validates data"""
    def validate(self, data: dict) -> bool: ...

# âŒ BAD: Low cohesion
class FileManager:
    def read(self): ...
    def write(self): ...
    def validate(self): ...
    def send_email(self): ...  # ?!
```

**Anti-pattern** âŒ:
```python
# DO NOT do everything at once:
def complex_function_with_everything():
    # 500 lines of code
    # Multiple responsibilities
    # Difficult to test
    # High coupling
    # No reusability
```

**Correct Pattern** âœ…:
```python
# Module: src/rewriter/key_extractor.py
class KeyExtractor:
    """High cohesion: only extracts keys"""
    def extract_from_obj(self, data) -> Dict[str, str]:
        return self._recurse(data, prefix='t')

# Module: src/rewriter/substitution_builder.py
class SubstitutionMapBuilder:
    """High cohesion: only builds maps"""
    def build_by_value(self, old, new) -> Dict[str, str]:
        return self._match_values(old, new)

# Module: src/rewriter/reference_updater.py
class ReferenceUpdater:
    """Low coupling: uses interfaces"""
    def __init__(self, extractor: KeyExtractor, builder: SubstitutionMapBuilder):
        self._extractor = extractor  # Dependency injection
        self._builder = builder
    
    def update_project(self, dir: str) -> Dict[str, int]:
        """Coordinates but doesn't implement everything"""
        old = self._extractor.extract(self._read_old())
        new = self._extractor.extract(self._read_new())
        map = self._builder.build_by_value(old, new)
        return self._apply_to_files(dir, map)
```

---

### 6ï¸âƒ£.5ï¸âƒ£ **Security Checklist (OWASP Top 10)** â­ NEW - HIGH PRIORITY

**When to Apply**: During Step 6 (Implementation) if the feature involves:
- âœ… User input (forms, CLI arguments, file uploads)
- âœ… File/operating system access
- âœ… Network connections (APIs, databases, external services)
- âœ… Authentication/authorization
- âœ… Sensitive data (passwords, tokens, PII)

**OWASP Top 10 Applied**:

**1. ðŸ›¡ï¸ Injection (SQL, Command, Path Traversal)**
```python
# âŒ INSECURE - SQL Injection
query = f"SELECT * FROM users WHERE id = {user_id}"  # NEVER DO THIS!

# âœ… SECURE - Parameterized Query
query = "SELECT * FROM users WHERE id = ?"
cursor.execute(query, (user_id,))

# âŒ INSECURE - Command Injection
os.system(f"convert {filename} output.png")  # NEVER DO THIS!

# âœ… SECURE - List of arguments
subprocess.run(["convert", filename, "output.png"], check=True)

# âŒ INSECURE - Path Traversal
with open(user_path, 'r') as f:  # ../../../etc/passwd
    data = f.read()

# âœ… SECURE - Validate and restrict path
from pathlib import Path
safe_path = Path(user_path).resolve()
if not safe_path.is_relative_to(ALLOWED_DIR):
    raise SecurityError("Path traversal detected!")
data = safe_path.read_text()
```

**2. ðŸ” Broken Authentication**
```python
# âŒ INSECURE - Plain text password
password = "admin123"  # NEVER DO THIS!

# âœ… SECURE - Hashing with salt
import bcrypt
hashed = bcrypt.hashpw(password.encode(), bcrypt.gensalt())

# âŒ INSECURE - Session without timeout
session['user_id'] = user_id  # Never expires

# âœ… SECURE - Session with timeout
session['user_id'] = user_id
session.permanent = True
app.permanent_session_lifetime = timedelta(hours=1)
```

**3. ðŸ”“ Sensitive Data Exposure**
```python
# âŒ INSECURE - API key in code
API_KEY = "sk-1234567890abcdef"  # NEVER DO THIS!

# âœ… SECURE - Environment variables
import os
API_KEY = os.getenv('API_KEY')
if not API_KEY:
    raise ValueError("API_KEY not set!")

# âŒ INSECURE - Sensitive data log
logger.info(f"User logged in: {email}, password: {password}")

# âœ… SECURE - Log without sensitive data
logger.info(f"User logged in: {email}")
```

**4. ðŸŒ XML External Entities (XXE)**
```python
# âŒ INSECURE - XML parsing without protection
import xml.etree.ElementTree as ET
tree = ET.parse(user_file)  # Vulnerable to XXE

# âœ… SECURE - Disable external entities
import defusedxml.ElementTree as ET
tree = ET.parse(user_file)
```

**5. ðŸšª Broken Access Control**
```python
# âŒ INSECURE - No permission check
def delete_file(file_id):
    file = File.query.get(file_id)
    file.delete()  # Any user can delete any file!

# âœ… SECURE - Check ownership
def delete_file(file_id, current_user):
    file = File.query.get(file_id)
    if file.owner_id != current_user.id:
        raise PermissionError("You don't own this file!")
    file.delete()
```

**6. âš™ï¸ Security Misconfiguration**
```python
# âŒ INSECURE - Debug mode in production
app = Flask(__name__)
app.debug = True  # NEVER in production!

# âœ… SECURE - Debug only in development
app.debug = os.getenv('FLASK_ENV') == 'development'

# âŒ INSECURE - Too open permissions
os.chmod(secret_file, 0o777)  # Everyone can read/write

# âœ… SECURE - Restrictive permissions
os.chmod(secret_file, 0o600)  # Only owner can read/write
```

**7. ðŸŽ¨ Cross-Site Scripting (XSS)**
```python
# âŒ INSECURE - Unescaped HTML
html = f"<div>Hello {user_name}</div>"  # XSS if user_name = "<script>..."

# âœ… SECURE - HTML escaping
from html import escape
html = f"<div>Hello {escape(user_name)}</div>"

# âœ… BETTER - Template engine with auto-escape
return render_template('hello.html', name=user_name)  # Jinja2 automatically escapes
```

**8. ðŸ”„ Insecure Deserialization**
```python
# âŒ INSECURE - pickle from untrusted source
import pickle
data = pickle.loads(user_data)  # Code execution!

# âœ… SECURE - DATA (does not execute code)
import data
data = data.loads(user_data)
```

**9. ðŸ“¦ Using Components with Known Vulnerabilities**
```bash
# âŒ INSECURE - Outdated dependencies
# requirements.txt
flask==0.12.0  # Old version with vulnerabilities

# âœ… SECURE - Updated dependencies
pip install --upgrade flask
pip-audit  # Checks for vulnerabilities

# Automatic - GitHub Dependabot
# .github/dependabot.yml
version: 2
updates:
  - package-ecosystem: "pip"
    directory: "/"
    schedule:
      interval: "weekly"
```

**10. ðŸ“‹ Insufficient Logging & Monitoring**
```python
# âŒ INSECURE - No security logs
def login(username, password):
    user = authenticate(username, password)
    return user  # Silent failure

# âœ… SECURE - Log security events
def login(username, password):
    try:
        user = authenticate(username, password)
        logger.info(f"Login success: {username} from {request.remote_addr}")
        return user
    except AuthenticationError:
        logger.warning(f"Login failed: {username} from {request.remote_addr}")
        raise
```

**Security Checklist**:
```markdown
### Security Checklist - [Feature Name]

#### Injection
- [ ] All inputs are sanitized/validated?
- [ ] Queries use parameterization?
- [ ] System commands use argument lists (not strings)?
- [ ] Paths are validated against path traversal?

#### Authentication & Sessions
- [ ] Passwords are hash + salt (bcrypt/argon2)?
- [ ] Sessions have timeouts?
- [ ] Tokens are generated with crypto.secrets (not random)?
- [ ] Login failures are logged?

#### Sensitive Data
- [ ] API keys/secrets in environment variables?
- [ ] Sensitive data is NOT logged?
- [ ] Connections use HTTPS/TLS?
- [ ] Data at rest is encrypted (if necessary)?

#### Access Control
- [ ] Permissions checked before each operation?
- [ ] User only accesses their own resources?
- [ ] Principle of least privilege applied?

#### Configuration
- [ ] Debug mode DISABLED in production?
- [ ] Error messages DO NOT expose stack traces to the user?
- [ ] Correct file permissions (0o600 for secrets)?

#### Dependencies
- [ ] All dependencies updated?
- [ ] pip-audit executed without vulnerabilities?
- [ ] Dependabot configured (if GitHub)?
```

**Security Tools**:
```bash
# Static security analysis
pip install bandit
bandit -r src/ -f data -o security-report.data

# Check for vulnerabilities in dependencies
pip install pip-audit
pip-audit

# Scan for secrets in code
pip install detect-secrets
detect-secrets scan > .secrets.baseline

# Pre-commit hook for security
# .pre-commit-config.yaml
repos:
  - repo: local
    hooks:
      - id: bandit
        name: Security check (bandit)
        entry: bandit -r src/
        language: system
      
      - id: secrets
        name: Detect secrets
        entry: detect-secrets-hook
        language: system
```

**Why this step is critical**:
- âœ… **LGPD/GDPR Compliance**: Avoids fines and lawsuits
- âœ… **Reputation**: Data breaches destroy trust
- âœ… **Cost**: Security bugs are 100x more expensive to fix in production
- âœ… **Legal**: Civil and criminal liability

---

### 6ï¸âƒ£.6ï¸âƒ£ **Generate API Documentation** (Optional - If creating a library/reusable module)

**When to Apply**:
- âœ… Module will be used by other developers
- âœ… Public/open-source library
- âœ… REST/GraphQL API
- âœ… SDK or plugin
- âœ… Complex functions that need examples

**Do Not Apply If**:
- âŒ Disposable internal code
- âŒ One-off scripts
- âŒ Rapid prototype

**Recommended Tools**:

**1. Sphinx** (Complete professional documentation)
```bash
# Install
pip install sphinx sphinx-rtd-theme

# Initialize
cd docs/
sphinx-quickstart

# Automatically generate from docstrings
sphinx-apidoc -o source/ ../src/

# Compile
make html

# Result: docs/build/html/index.html
```

**Configuration** (`docs/source/conf.py`):
```python
extensions = [
    'sphinx.ext.autodoc',       # Automatic docstrings
    'sphinx.ext.napoleon',      # Google/NumPy style docstrings
    'sphinx.ext.viewcode',      # Link to source code
    'sphinx.ext.intersphinx',   # Links to other docs
]

html_theme = 'sphinx_rtd_theme'  # Read the Docs theme
```

**2. pdoc** (Simple and fast documentation)
```bash
# Install
pip install pdoc

# Generate (serves with hot-reload)
pdoc --http : src/

# Generate static HTML
pdoc --html --output-dir docs/ src/

# Result: docs/src/index.html
```

**3. MkDocs** (Documentation in Markdown)
```bash
# Install
pip install mkdocs mkdocs-material

# Initialize
mkdocs new .

# Serve with hot-reload
mkdocs serve

# Build for production
mkdocs build

# Deploy to GitHub Pages
mkdocs gh-deploy
```

**Example of Complete Docstring**:
```python
def build_substitution_map_by_value(
    old_keys: Dict[str, str],
    new_keys: Dict[str, str]
) -> Dict[str, str]:
    """
    Build substitution map matching keys by their VALUES (not names).
    
    This function compares translation values between old and new DATA files
    to detect feature that need updating. It ignores key names and
    focuses solely on value equality.
    
    Args:
        old_keys: Dictionary mapping old key paths to their values.
            Example: {"t.welcome": "Welcome", "t.hello": "Hello"}
        new_keys: Dictionary mapping new key paths to their values.
            Example: {"t.greeting": "Welcome", "t.hi": "Hi"}
    
    Returns:
        Dictionary mapping old key paths to new key paths where values match.
        Example: {"t.welcome": "t.greeting"}  # Both have value "Welcome"
    
    Raises:
        ValueError: If old_keys or new_keys are empty.
        TypeError: If inputs are not dictionaries.
    
    Examples:
        >>> old = {"t.btn1": "Save", "t.btn2": "Cancel"}
        >>> new = {"t.save_btn": "Save", "t.cancel_btn": "Cancel"}
        >>> build_substitution_map_by_value(old, new)
        {'t.btn1': 't.save_btn', 't.btn2': 't.cancel_btn'}
        
        >>> old = {"t.msg": "Hello"}
        >>> new = {"t.greeting": "Hi"}  # Different value
        >>> build_substitution_map_by_value(old, new)
        {}  # No matches
    
    Notes:
        - Comparison is case-sensitive and exact
        - First match wins if multiple new keys have same value
        - Parent keys are not matched (only leaf values)
    
    See Also:
        - extract_all_keys_from_obj: Extract keys from DATA/Obj
        - apply_substitutions_to_file: Apply map to TSX files
    
    References:
        - Task Example: Feature Update System
        - FEATURE_SPEC.md
    
    Version:
        Added in vX.Y.Z
    """
    # Implementation...
```

**API Documentation Checklist**:
```markdown
### API Documentation - [Module Name]

#### Coverage
- [ ] All public functions have docstrings?
- [ ] All parameters documented?
- [ ] All returns documented?
- [ ] Possible exceptions listed?

#### Quality
- [ ] Usage examples included?
- [ ] Edge cases documented?
- [ ] Type hints present (PEP 484)?
- [ ] Docstrings follow standard (Google/NumPy)?

#### Accessibility
- [ ] API docs published (Read the Docs, GitHub Pages)?
- [ ] Tutorial/Getting Started included?
- [ ] Changelog maintained?
- [ ] Link to docs in README?
```

**Why**:
- âœ… **Adoption**: Good docs = more users
- âœ… **Support**: Fewer repetitive questions
- âœ… **Onboarding**: New devs understand faster
- âœ… **Professionalism**: Shows project quality

---

### 6ï¸âƒ£.6ï¸âƒ£ **Project Icons** [MANDATORY]

> **CRITICAL FOR AIs**: Every project must include appropriate icons to ensure professionalism and visual identity.

**When to Apply**: During implementation (Step 6), after defining the basic project structure.

#### ðŸ“‹ Mandatory Requirement

Artificial intelligence **MUST** produce or download an icon for the project, whether:
- ðŸŒ Website/Web Application
- ðŸ’» Desktop Program
- ðŸ“± Mobile Application
- ðŸ”§ Tool/Utility

#### ðŸŽ¨ Icon Formats by Technology

**Web Applications**:
- âœ… **favicon.ico** (16x16, 32x32, 48x48 px) - Universal compatibility
- âœ… **icon.svg** - Vector, scalable, modern
- âœ… **icon-192.png** and **icon-512.png** - PWA/Android
- âœ… **apple-touch-icon.png** (180x180 px) - iOS

**Desktop Applications**:
- âœ… **icon.png** (256x256, 512x512 px) - Linux
- âœ… **icon.ico** (multiple sizes) - Windows
- âœ… **icon.icns** - macOS

**Mobile Applications**:
- âœ… **icon.png** (1024x1024 px) - iOS App Store
- âœ… **ic_launcher.png** (multiple densities) - Android
- âœ… **adaptive-icon.xml** - Android adaptive

#### ðŸ“ Folder Structure (MANDATORY)

Icons **MUST** be organized in a dedicated folder:

```
project/
â”œâ”€â”€ assets/              # âœ… PREFERRED (default for all)
â”‚   â”œâ”€â”€ icons/
â”‚   â”‚   â”œâ”€â”€ favicon.ico
â”‚   â”‚   â”œâ”€â”€ icon.svg
â”‚   â”‚   â”œâ”€â”€ icon-192.png
â”‚   â”‚   â”œâ”€â”€ icon-512.png
â”‚   â”‚   â””â”€â”€ apple-touch-icon.png
â”‚   â””â”€â”€ ...
â”‚
# OR alternatives according to technology:
â”œâ”€â”€ public/              # âœ… React, Vue, Next.js
â”‚   â”œâ”€â”€ favicon.ico
â”‚   â””â”€â”€ icons/
â”œâ”€â”€ static/              # âœ… Flask, Django, Svelte
â”‚   â””â”€â”€ icons/
â”œâ”€â”€ src/assets/          # âœ… Angular, Ionic
â”‚   â””â”€â”€ icons/
â”œâ”€â”€ resources/           # âœ… Electron, Tauri
â”‚   â””â”€â”€ icons/
â””â”€â”€ res/                 # âœ… Native Android
    â””â”€â”€ drawable/
```

**Golden Rule**: Always use a specific folder for icons, never loose files at the project root.

#### ðŸ”§ How to Obtain/Create Icons

AI must follow this priority order:

1. **Ask the Programmer** (ALWAYS first):
   ```
   â“ Do you already have an icon for the project?
   
   Options:
   A) âœ… Yes, I have (provide the path/file)
   B) ðŸŽ¨ No, create a simple icon for me
   C) ðŸ” No, download a suitable free icon
   D) â­ï¸ Skip for now (not recommended)
   ```

2. **If A (User provides)**:
   - Validate format and size
   - Convert to necessary formats (use tools like `convert`, `sharp`, `imagemagick`)
   - Organize in the correct folder

3. **If B (AI creates simple icon)**:
   - Create vector SVG icon with project initials
   - Export to necessary formats (PNG, ICO)
   - Use project identity colors (if defined)

4. **If C (AI downloads icon)**:
   - Use free and copyright-free sources:
     - âœ… [Heroicons](https://heroicons.com/) (MIT License)
     - âœ… [Lucide Icons](https://lucide.dev/) (ISC License)
     - âœ… [Tabler Icons](https://tabler-icons.io/) (MIT License)
     - âœ… [Iconoir](https://iconoir.com/) (MIT License)
   - Verify license before using
   - Document source in README

5. **If D (Skip)**:
   - âš ï¸ Warn that project will lack visual identity
   - Add task in TASKS.md for future: `[ ] Create project icon`

#### ðŸŽ¨ Simple SVG Icon Example (Generated by AI)

```svg
<!-- assets/icons/icon.svg -->
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100">
  <rect width="100" height="100" rx="20" fill="#4F46E5"/>
  <text x="50" y="65" font-family="Arial, sans-serif" font-size="48" 
        font-weight="bold" fill="white" text-anchor="middle">MP</text>
</svg>
```

#### ðŸ”¨ Icon Conversion Tools

**Python** (recommended for automation):
```bash
# Install Pillow
pip install Pillow

# Convert SVG to PNG (via cairosvg)
pip install cairosvg
python -c "import cairosvg; cairosvg.svg2png(url='icon.svg', write_to='icon.png', output_width=512)"

# Create ICO with multiple sizes
from PIL import Image
img = Image.open('icon.png')
img.save('favicon.ico', format='ICO', sizes=[(16,16), (32,32), (48,48)])
```

**Node.js** (web projects):
```bash
# Install sharp
npm install sharp

# Conversion script
node -e "
const sharp = require('sharp');
sharp('icon.svg').resize(192, 192).toFile('icon-192.png');
sharp('icon.svg').resize(512, 512).toFile('icon-512.png');
"
```

**ImageMagick** (universal):
```bash
# Convert SVG to PNG
convert icon.svg -resize 192x192 icon-192.png

# Create favicon.ico
convert icon.png -define icon:auto-resize=16,32,48 favicon.ico
```

#### ðŸ—‚ï¸ Project Integration

**HTML (Web)**:
```html
<!-- index.html -->
<head>
  <!-- Basic favicon -->
  <link rel="icon" type="image/x-icon" href="/assets/icons/favicon.ico">
  
  <!-- Modern SVG (preferred) -->
  <link rel="icon" type="image/svg+xml" href="/assets/icons/icon.svg">
  
  <!-- PNG for different sizes -->
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/icon-32.png">
  <link rel="icon" type="image/png" sizes="192x192" href="/assets/icons/icon-192.png">
  
  <!-- Apple Touch Icon -->
  <link rel="apple-touch-icon" href="/assets/icons/apple-touch-icon.png">
  
  <!-- Android Chrome -->
  <link rel="manifest" href="/manifest.json">
</head>
```

**manifest.json (PWA)**:
```json
{
  "name": "My Project",
  "short_name": "MP",
  "icons": [
    {
      "src": "/assets/icons/icon-192.png",
      "sizes": "192x192",
      "type": "image/png"
    },
    {
      "src": "/assets/icons/icon-512.png",
      "sizes": "512x512",
      "type": "image/png"
    }
  ]
}
```

**Python (Desktop - PyQt/Tkinter)**:
```python
# PyQt6
from PyQt6.QtGui import QIcon
from PyQt6.QtWidgets import QApplication

app = QApplication([])
app.setWindowIcon(QIcon('assets/icons/icon.png'))

# Tkinter
import tkinter as tk
root = tk.Tk()
root.iconbitmap('assets/icons/icon.ico')  # Windows
# or
root.iconphoto(True, tk.PhotoImage(file='assets/icons/icon.png'))  # Linux/Mac
```

**Electron (Desktop)**:
```javascript
// main.js
const { app, BrowserWindow } = require('electron');
const path = require('path');

const win = new BrowserWindow({
  icon: path.join(__dirname, 'resources/icons/icon.png')
});
```

**React Native (Mobile)**:
```
// android/app/src/main/res/
mipmap-hdpi/ic_launcher.png      (72x72)
mipmap-mdpi/ic_launcher.png      (48x48)
mipmap-xhdpi/ic_launcher.png     (96x96)
mipmap-xxhdpi/ic_launcher.png    (144x144)
mipmap-xxxhdpi/ic_launcher.png   (192x192)

// ios/ProjectName/Images.xcassets/AppIcon.appiconset/
// Configured via Xcode or Contents.json
```

#### â° Best Timing to Add Icons

**Recommendation**: **During Step 6 (Implementation)**, preferably:

1. **Project Start** (âœ… IDEAL):
   - When creating initial folder structure
   - Before first commit
   - Facilitates visual identity from the beginning

2. **MVP/Prototype** (âœ… GOOD):
   - After basic functionalities work
   - Before showing to users/clients
   - Ensures minimum professionalism

3. **Before Production** (âš ï¸ ACCEPTABLE):
   - During deployment preparation
   - Before publishing (App Store, Play Store, web)
   - Minimum necessary, but delayed

4. **âŒ NEVER**: Leave for "later" without defined date

#### ðŸ“‹ Icon Checklist (Validation)

```markdown
## Icon Checklist - Project [Name]

### Icons Created
- [ ] Main icon created/obtained (source: [specify])
- [ ] License verified (if downloaded from external source)
- [ ] Vector format available (SVG) or high-quality PNG source

### Necessary Formats
- [ ] **favicon.ico** (16x16, 32x32, 48x48 px)
- [ ] **icon.svg** (vector)
- [ ] **icon-192.png** (192x192 px) - PWA
- [ ] **icon-512.png** (512x512 px) - PWA
- [ ] **apple-touch-icon.png** (180x180 px) - iOS
- [ ] Other technology-specific formats

### Organization
- [ ] `assets/icons/` folder created
- [ ] All icons organized in correct folder
- [ ] No loose icons at project root

### Integration
- [ ] Icon referenced in HTML/main code
- [ ] manifest.json updated (if PWA)
- [ ] Tested in browser/application (icon appears)
- [ ] Documented in README (if third-party icon)

### Quality
- [ ] Icon has good resolution (not pixelated)
- [ ] Colors appropriate to project
- [ ] Visible on light AND dark backgrounds (if applicable)
- [ ] Recognizable at small sizes (16x16)
```

#### ðŸŽ¯ Rationale: Why Icons Are Mandatory

1. **Professionalism**: Projects without icons appear incomplete/amateur
2. **Visual Identity**: Users recognize the app by its icon (branding)
3. **User Experience**: Icon helps locate the app among multiple tabs/windows
4. **Platform Requirements**: App stores (iOS/Android) REQUIRE icons
5. **PWA**: Browsers request icons for installation
6. **Organization**: Facilitates finding and managing visual assets
7. **Traceability**: Documenting source ensures license compliance

#### ðŸš¨ Common Mistakes to Avoid

âŒ **Don't**:
- Leave icon at project root (e.g., loose `favicon.ico`)
- Use low-resolution icon (pixelated when enlarged)
- Forget to reference in HTML/code
- Use copyrighted icon without permission
- Create only one size (browsers need multiple)

âœ… **Do**:
- Organize in dedicated folder (`assets/icons/`)
- Generate multiple sizes (16, 32, 192, 512 px)
- Validate that icon appears correctly
- Document source if third-party icon
- Use vector format (SVG) when possible

#### ðŸ“š Useful Resources

**Free Icon Generators** (online):
- [Favicon.io](https://favicon.io/) - Generates favicon from text/image/emoji
- [RealFaviconGenerator](https://realfavicongenerator.net/) - Generates all formats
- [Favicon Generator](https://www.favicon-generator.org/) - Simple and fast

**Free Icon Libraries**:
- [Heroicons](https://heroicons.com/) - MIT License
- [Lucide Icons](https://lucide.dev/) - ISC License
- [Tabler Icons](https://tabler-icons.io/) - MIT License
- [Iconoir](https://iconoir.com/) - MIT License
- [Bootstrap Icons](https://icons.getbootstrap.com/) - MIT License

**Conversion Tools**:
- [ImageMagick](https://imagemagick.org/) - Universal CLI
- [Pillow (Python)](https://pillow.readthedocs.io/) - Image library
- [Sharp (Node.js)](https://sharp.pixelplumbing.com/) - High performance

#### ðŸ“ Example README Documentation

```markdown
## ðŸŽ¨ Project Icon

**Source**: Created by AI using project initials  
**License**: Free to use (generated for this project)  
**Location**: `assets/icons/`

### Available Formats
- `icon.svg` - Vector (preferred)
- `favicon.ico` - Browsers (16, 32, 48 px)
- `icon-192.png` - PWA/Android
- `icon-512.png` - PWA/Android
- `apple-touch-icon.png` - iOS

### Colors
- Primary: `#4F46E5` (Indigo Blue)
- Text: `#FFFFFF` (White)
```

#### â±ï¸ Estimated Time

- **Create simple icon (AI)**: 5-10 minutes
- **Download and adapt icon**: 10-15 minutes
- **Convert to necessary formats**: 5-10 minutes
- **Integrate into project**: 5-10 minutes
- **TOTAL**: 15-30 minutes

**Small investment, big impact on project quality perception.**

---

### 6ï¸âƒ£.7ï¸âƒ£ **Execution Scripts to Facilitate Usage** [RECOMMENDED]

> **RECOMMENDATION FOR AIs**: When dealing with an application or program written in a programming language that is executable, it's recommended to create batch scripts for Windows, Linux, and Mac accessible in the main folder or root directory, in order to facilitate application execution.

**When to Apply**: During implementation (Step 6), especially after setting up the basic structure of the executable project.

#### ðŸ“‹ Context and Purpose

Execution scripts in the project root significantly facilitate application usage, especially:
- âœ… **Development**: Accelerates development cycle (run without manually configuring environment)
- âœ… **Onboarding**: New developers can run the project immediately
- âœ… **Testing**: Facilitates test execution and validation
- âœ… **Production**: In some cases, can simplify deployment (if there are no better alternatives like Docker, systemd, etc.)

#### ðŸŽ¯ When to Create Execution Scripts

**âœ… CREATE scripts IF:**
- âœ… Application is executable (not a library)
- âœ… Requires environment configuration (variables, paths, dependencies)
- âœ… Has multiple initialization commands
- âœ… Needs setup before execution (migrations, build, etc.)
- âœ… Team/users need to execute frequently

**âŒ DO NOT create scripts IF:**
- âŒ Application already has well-documented native CLI
- âŒ Uses standard language tools (npm start, cargo run, etc.)
- âŒ Deployment uses orchestration (Docker, Kubernetes) - scripts stay in Dockerfile
- âŒ Project is a library/framework (not executable)

#### ðŸ“ Recommended Folder Structure

```
project/
â”œâ”€â”€ run.bat                 # âœ… Windows (main execution)
â”œâ”€â”€ run.sh                  # âœ… Linux/Mac (main execution)
â”œâ”€â”€ dev.bat                 # ðŸ”„ Development Windows (optional)
â”œâ”€â”€ dev.sh                  # ðŸ”„ Development Linux/Mac (optional)
â”œâ”€â”€ test.bat                # ðŸ§ª Tests Windows (optional)
â”œâ”€â”€ test.sh                 # ðŸ§ª Tests Linux/Mac (optional)
â”œâ”€â”€ build.bat               # ðŸ—ï¸ Build Windows (optional)
â”œâ”€â”€ build.sh                # ðŸ—ï¸ Build Linux/Mac (optional)
â””â”€â”€ README.md               # Script usage documentation
```

**Golden Rule**: Scripts in project root = easy access. Complex scripts can stay in `scripts/` with simple wrappers in root.

#### ðŸ’» Script Examples by Language

##### **Python**

**run.sh (Linux/Mac)**:
```bash
#!/bin/bash
# Execution script for Linux/Mac

# Colors for output
GREEN='\033[0;32m'
RED='\033[0;31m'
NC='\033[0m' # No Color

echo -e "${GREEN}ðŸš€ Starting Python application...${NC}"

# Check if virtual environment exists
if [ ! -d "venv" ]; then
    echo -e "${RED}âŒ Virtual environment not found. Creating...${NC}"
    python3 -m venv venv
fi

# Activate virtual environment
source venv/bin/activate

# Install/update dependencies
if [ -f "requirements.txt" ]; then
    echo -e "${GREEN}ðŸ“¦ Installing dependencies...${NC}"
    pip install -q -r requirements.txt
fi

# Run application
echo -e "${GREEN}âœ… Running application...${NC}"
python src/main.py "$@"
```

**run.bat (Windows)**:
```batch
@echo off
REM Execution script for Windows

echo ðŸš€ Starting Python application...

REM Check if virtual environment exists
if not exist "venv\" (
    echo âŒ Virtual environment not found. Creating...
    python -m venv venv
)

REM Activate virtual environment
call venv\Scripts\activate.bat

REM Install/update dependencies
if exist "requirements.txt" (
    echo ðŸ“¦ Installing dependencies...
    pip install -q -r requirements.txt
)

REM Run application
echo âœ… Running application...
python src\main.py %*
```

##### **Node.js**

**run.sh (Linux/Mac)**:
```bash
#!/bin/bash
# Execution script for Linux/Mac

GREEN='\033[0;32m'
NC='\033[0m'

echo -e "${GREEN}ðŸš€ Starting Node.js application...${NC}"

# Check if node_modules exists
if [ ! -d "node_modules" ]; then
    echo -e "${GREEN}ðŸ“¦ Installing dependencies...${NC}"
    npm install
fi

# Run application
echo -e "${GREEN}âœ… Running application...${NC}"
npm start "$@"
```

**run.bat (Windows)**:
```batch
@echo off
REM Execution script for Windows

echo ðŸš€ Starting Node.js application...

REM Check if node_modules exists
if not exist "node_modules\" (
    echo ðŸ“¦ Installing dependencies...
    call npm install
)

REM Run application
echo âœ… Running application...
npm start %*
```

##### **Java**

**run.sh (Linux/Mac)**:
```bash
#!/bin/bash
# Execution script for Linux/Mac

GREEN='\033[0;32m'
NC='\033[0m'

echo -e "${GREEN}ðŸš€ Starting Java application...${NC}"

# Compile if necessary
if [ ! -d "target" ]; then
    echo -e "${GREEN}ðŸ—ï¸ Compiling project...${NC}"
    mvn clean package -DskipTests
fi

# Run JAR
echo -e "${GREEN}âœ… Running application...${NC}"
java -jar target/myapp.jar "$@"
```

**run.bat (Windows)**:
```batch
@echo off
REM Execution script for Windows

echo ðŸš€ Starting Java application...

REM Compile if necessary
if not exist "target\" (
    echo ðŸ—ï¸ Compiling project...
    call mvn clean package -DskipTests
)

REM Run JAR
echo âœ… Running application...
java -jar target\myapp.jar %*
```

##### **Go**

**run.sh (Linux/Mac)**:
```bash
#!/bin/bash
# Execution script for Linux/Mac

GREEN='\033[0;32m'
NC='\033[0m'

echo -e "${GREEN}ðŸš€ Starting Go application...${NC}"

# Download dependencies if necessary
if [ ! -f "go.sum" ]; then
    echo -e "${GREEN}ðŸ“¦ Downloading dependencies...${NC}"
    go mod download
fi

# Run application
echo -e "${GREEN}âœ… Running application...${NC}"
go run cmd/main.go "$@"
```

**run.bat (Windows)**:
```batch
@echo off
REM Execution script for Windows

echo ðŸš€ Starting Go application...

REM Download dependencies if necessary
if not exist "go.sum" (
    echo ðŸ“¦ Downloading dependencies...
    go mod download
)

REM Run application
echo âœ… Running application...
go run cmd\main.go %*
```

##### **Rust**

**run.sh (Linux/Mac)**:
```bash
#!/bin/bash
# Execution script for Linux/Mac

GREEN='\033[0;32m'
NC='\033[0m'

echo -e "${GREEN}ðŸš€ Starting Rust application...${NC}"

# Compile and run
echo -e "${GREEN}âœ… Running application (cargo run)...${NC}"
cargo run --release "$@"
```

**run.bat (Windows)**:
```batch
@echo off
REM Execution script for Windows

echo ðŸš€ Starting Rust application...

REM Compile and run
echo âœ… Running application (cargo run)...
cargo run --release %*
```

#### ðŸ”§ Additional Useful Scripts

##### **Development Script** (watch/reload mode)

**dev.sh**:
```bash
#!/bin/bash
# Development mode with auto-reload

echo "ðŸ”„ Starting in development mode..."

# Python
# pip install watchdog
# watchmedo auto-restart --directory=./src --pattern=*.py python src/main.py

# Node.js
# npm run dev  # nodemon or similar

# Go
# go install github.com/cosmtrek/air@latest
# air

# Rust
# cargo install cargo-watch
# cargo watch -x run
```

##### **Test Script**

**test.sh**:
```bash
#!/bin/bash
# Run tests

echo "ðŸ§ª Running tests..."

# Python
# pytest tests/ -v

# Node.js
# npm test

# Java
# mvn test

# Go
# go test ./...

# Rust
# cargo test
```

#### ðŸ“‹ Execution Scripts Checklist

```markdown
## Scripts Checklist - Project [Name]

### Scripts Created
- [ ] **run.sh** (Linux/Mac) - Main execution script
- [ ] **run.bat** (Windows) - Main execution script
- [ ] Execution permissions configured (`chmod +x *.sh`)
- [ ] Scripts tested on each platform

### Optional Scripts (as needed)
- [ ] **dev.sh/dev.bat** - Development mode with auto-reload
- [ ] **test.sh/test.bat** - Run automated tests
- [ ] **build.sh/build.bat** - Compile/build project
- [ ] **install.sh/install.bat** - Install dependencies
- [ ] **clean.sh/clean.bat** - Clean build artifacts

### Documentation
- [ ] README.md updated with script usage instructions
- [ ] Usage examples documented
- [ ] System requirements documented (Python 3.9+, Node 18+, etc.)
- [ ] Basic troubleshooting included

### Script Features
- [ ] Check if dependencies are installed
- [ ] Create virtual environment/directories if needed
- [ ] Clear and informative output messages
- [ ] Support argument passing (`./run.sh --help`)
- [ ] Handle errors gracefully
- [ ] Include colors in output (optional, improves UX)
```

#### ðŸ“ Example README Documentation

```markdown
## ðŸš€ How to Run

### Requirements
- Python 3.9+ (or Node.js 18+, Java 17+, etc.)
- Git

### Quick Start

**Linux/Mac**:
```bash
./run.sh
```

**Windows**:
```batch
run.bat
```

### Available Scripts

| Script | Description | Platform |
|--------|-------------|----------|
| `run.sh` / `run.bat` | Runs the main application | Linux/Mac / Windows |
| `dev.sh` / `dev.bat` | Development mode (auto-reload) | Linux/Mac / Windows |
| `test.sh` / `test.bat` | Runs automated tests | Linux/Mac / Windows |
| `build.sh` / `build.bat` | Compiles/builds the project | Linux/Mac / Windows |

### Arguments

Pass arguments to application:
```bash
./run.sh --port 8080 --debug
```

### Troubleshooting

**Error: Permission denied (Linux/Mac)**
```bash
chmod +x run.sh dev.sh test.sh build.sh
```

**Error: Dependencies not found**
- Scripts automatically install dependencies on first run
- If it fails, run manually: `pip install -r requirements.txt` (Python) or `npm install` (Node.js)
```

#### â±ï¸ Estimated Time

- **Create basic scripts (run.sh/run.bat)**: 10-15 minutes
- **Add optional scripts (dev, test, build)**: 5-10 minutes each
- **Document in README**: 10-15 minutes
- **Test on multiple platforms**: 10-20 minutes
- **TOTAL**: 30-60 minutes

**Investment: ~30-60 minutes. Benefit: Saves hours of setup for each developer and user.**

#### ðŸŽ¯ Rationale: Why Execution Scripts Are Important

1. **Developer Experience (DX)**: New developer clones repo, runs `./run.sh` and application works
2. **Friction Reduction**: No need to read complex documentation to run project
3. **Consistency**: Everyone runs the same way, reduces "works on my machine"
4. **Automation**: Scripts can automatically configure environment (create venv, install deps)
5. **Living Documentation**: Scripts serve as executable documentation of initialization process
6. **Onboarding**: Accelerates entry of new team members
7. **CI/CD**: Scripts can be reused in pipelines
8. **Cross-Platform**: Explicit support for Windows, Linux, and Mac

#### âš ï¸ When NOT to Use Root Scripts

**Use better alternatives when available:**
- ðŸ³ **Docker/Docker Compose**: For apps with multiple dependencies (databases, queues, etc.)
- ðŸ“¦ **Native Package Managers**: `npm start`, `cargo run`, `go run` are already sufficient
- ðŸŽ¯ **Task Runners**: Makefile, Just, Task for complex projects
- â˜¸ï¸ **Orchestration**: Kubernetes, systemd for enterprise production

**Recommended Combination**:
```
project/
â”œâ”€â”€ docker-compose.yml      # ðŸ³ For complete environment
â”œâ”€â”€ Makefile                # ðŸŽ¯ For complex commands
â”œâ”€â”€ run.sh                  # âœ… Simple wrapper that calls Make/Docker
â””â”€â”€ README.md               # ðŸ“š Documents when to use each one
```

**Wrapper example**:
```bash
#!/bin/bash
# run.sh - Simple wrapper

if command -v docker &> /dev/null; then
    echo "ðŸ³ Docker detected, using docker-compose..."
    docker-compose up
else
    echo "âš ï¸ Docker not found, running locally..."
    make run
fi
```

---

### 7ï¸âƒ£ **Verify CLI Implementation + Code Review**
- **CRITICAL**: Verify that the new functionality is available via **CLI (Command Line Interface)**
- **IMPORTANT**: During verification, apply the **9 Quality Criteria** to the CLI code
- It's not enough to implement a GUI, important functionalities must have a **CLI interface** for automation
- Verify subcommands, arguments, help text, integration, and code quality

**CLI Implementation Checklist**:

1. **Correct Import in app.py**:
   ```python
   # âœ… Verify if module was imported
   from .gui import (
       ComponentJ, ComponentK, ComponentI,
       ComponentC, ComponentD, ComponentA,
       ComponentB, ComponentF, ComponentG, ComponentH,
       ComponentE, NewComponent  # â† NEW module should be here
   )
   ```

2. **Export in Module's __init__.py**:
   ```python
   # src/gui/__init__.py
   from .text_to_data_dock import NewComponent
   
   __all__ = [
       'ComponentJ', 'ComponentK', 'ComponentI',
       'ComponentC', 'ComponentD', 'ComponentA',
       'ComponentB', 'ComponentF', 'ComponentG', 'ComponentH',
       'ComponentE', 'NewComponent'  # â† NEW module exported
   ]
   ```

3. **Menu Item Created and Connected**:
   ```python
   # In _build_menu() or similar
   m_tools = bar.addMenu(tr("menu.tools"))
   
   # Create QAction
   self.act_open_new_component = QAction(tr("menu.tools.text_to_data"), self)
   
   # Add to menu
   m_tools.addAction(self.act_open_new_component)
   
   # Connect signal
   self.act_open_new_component.triggered.connect(lambda: self.dock_new_component.show())
   ```

4. **Dock Initialized in __init__() or setup method**:
   ```python
   # In __init__() of MainWindow
   def __init__(self):
       super().__init__()
       # ... other docks ...
       self._open_new_component()  # â† Initialize dock
   
   def _open_new_component(self):
       self.dock_new_component = NewComponent(self)
       self.dock_new_component.open_in_other_component_requested.connect(self._load_data_from_source)
       self.addDockWidget(Qt.RightDockWidgetArea, self.dock_new_component)
       self.dock_new_component.hide()
   ```

5. **Signals Connected** (if applicable):
   ```python
   # Connect custom signals
   self.dock_new_component.open_in_other_component_requested.connect(self._load_data_from_source)
   
   def _load_data_from_source(self, data_str: str):
       """Callback to open DATA in editor"""
       if not hasattr(self, 'component_viewer'):
           self._open_component()
       self.component_viewer.load_data_string(data_str)
       self.component_viewer.show()
   ```

6. **i18n Translations Added**:
   ```data
   // src/i18n/en.data
   {
     "menu.tools.text_to_data": "Text to DATA Converter"
   }
   
   // src/i18n/pt_BR.data
   {
     "menu.tools.text_to_data": "Conversor de Texto para DATA"
   }
   ```

**Integration Test Checklist**:
- âœ… **Accessible menu**: Verify if item appears in the Tools menu
- âœ… **Dock opens**: Clicking the menu should open the dock correctly
- âœ… **Basic functionality**: Test simple conversion
- âœ… **Signals work**: Test integration with other components (e.g., Open in Editor)
- âœ… **No console errors**: There should be no ImportError, AttributeError, etc.
- âœ… **Translation working**: Menu in PT-BR should show translated text

**Real Example (Task Example - Text to DATA Converter)**:
```python
âœ… Import: from .gui import NewComponent
âœ… Export: __all__ = [..., 'NewComponent']
âœ… Menu: self.act_open_new_component = QAction(tr("menu.tools.text_to_data"), self)
âœ… Init: self._open_new_component() called in __init__()
âœ… Signal: open_in_other_component_requested.connect(self._load_data_from_source)
âœ… i18n: EN "Text to DATA Converter", PT-BR "Conversor de Texto para DATA"
âœ… Test: Menu opens dock, conversion works, signal to editor OK
```

**Questions to Validate Integration**:
1. â“ "Is the new module imported in the main file (app.py)?"
2. â“ "Is the module exported in the folder's __init__.py?"
3. â“ "Is there a menu item to access the functionality?"
4. â“ "Is the menu item connected to the correct method?"
5. â“ "Is the dock/component initialized at application startup?"
6. â“ "Are custom signals connected?"
7. â“ "Were translations added (EN and PT-BR)?"
8. â“ "Is the functionality accessible without errors?"

**Why?**: Ensure that the implemented code is **actually usable** by the end-user, not just "works in isolation."

---

### 8ï¸âƒ£ **Verify GUI Implementation + Code Review**
- **CRITICAL**: Verify that components are **integrated into the main program** and accessible
- **IMPORTANT**: During verification, apply the **9 Quality Criteria** to the GUI code
- It's not enough to implement the module/dock, it needs to be **accessible and functional** in the app
- Verify menu, imports, initialization, connections, and code quality

**Part A - Functional GUI Verification (Integration)**:

1. **Correct Import in app.py**:
   ```python
   # âœ… Verify if module was imported
   from .gui import (
       ComponentJ, ComponentK, ComponentI,
       ComponentC, ComponentD, ComponentA,
       ComponentB, ComponentF, ComponentG, ComponentH,
       ComponentE, NewComponent  # â† NEW module should be here
   )
   ```

2. **Export in Module's __init__.py**:
   ```python
   # src/gui/__init__.py
   from .text_to_data_dock import NewComponent
   
   __all__ = [
       'ComponentJ', 'ComponentK', 'ComponentI',
       'ComponentC', 'ComponentD', 'ComponentA',
       'ComponentB', 'ComponentF', 'ComponentG', 'ComponentH',
       'ComponentE', 'NewComponent'  # â† NEW module exported
   ]
   ```

3. **Menu Item Created and Connected**:
   ```python
   # In _build_menu() or similar
   m_tools = bar.addMenu(tr("menu.tools"))
   
   # Create QAction
   self.act_open_new_component = QAction(tr("menu.tools.text_to_data"), self)
   
   # Add to menu
   m_tools.addAction(self.act_open_new_component)
   
   # Connect signal
   self.act_open_new_component.triggered.connect(lambda: self.dock_new_component.show())
   ```

4. **Dock Initialized in __init__() or setup method**:
   ```python
   # In __init__() of MainWindow
   def __init__(self):
       super().__init__()
       # ... other docks ...
       self._open_new_component()  # â† Initialize dock
   
   def _open_new_component(self):
       self.dock_new_component = NewComponent(self)
       self.dock_new_component.open_in_other_component_requested.connect(self._load_data_from_source)
       self.addDockWidget(Qt.RightDockWidgetArea, self.dock_new_component)
       self.dock_new_component.hide()
   ```

5. **Signals Connected** (if applicable):
   ```python
   # Connect custom signals
   self.dock_new_component.open_in_other_component_requested.connect(self._load_data_from_source)
   
   def _load_data_from_source(self, data_str: str):
       """Callback to open DATA in editor"""
       if not hasattr(self, 'component_viewer'):
           self._open_component()
       self.component_viewer.load_data_string(data_str)
       self.component_viewer.show()
   ```

6. **i18n Translations Added**:
   ```data
   // src/i18n/en.data
   {
     "menu.tools.text_to_data": "Text to DATA Converter"
   }
   
   // src/i18n/pt_BR.data
   {
     "menu.tools.text_to_data": "Conversor de Texto para DATA"
   }
   ```

**GUI Integration Test Checklist**:
- âœ… **Accessible menu**: Verify if item appears in the Tools menu
- âœ… **Dock opens**: Clicking the menu should open the dock correctly
- âœ… **Basic functionality**: Test simple conversion
- âœ… **Signals work**: Test integration with other components (e.g., Open in Editor)
- âœ… **No console errors**: There should be no ImportError, AttributeError, etc.
- âœ… **Translation working**: Menu in PT-BR should show translated text

**Part B - GUI Code Quality Review (9 Criteria)**:

During GUI verification, simultaneously apply the following criteria:

1. **âŒ Omission** - Verify if GUI is complete:
   - [ ] All necessary widgets/controls implemented?
   - [ ] Error handling in handlers (e.g., FileNotFoundError)?
   - [ ] Resource cleanup (close files, disconnect signals)?
   - [ ] Visual feedback for long operations (QProgressBar, busy cursor)?

2. **ðŸ¤” Ambiguity** - GUI should be clear:
   - [ ] Descriptive and clear labels?
   - [ ] Informative tooltips on controls?
   - [ ] Descriptive error messages (QMessageBox)?
   - [ ] Intuitive method names (_on_button_clicked vs _handle)?

3. **â— Incorrect Fact** - Correct GUI logic:
   - [ ] Signals connected to correct slots?
   - [ ] Correct layouts (QVBoxLayout, QHBoxLayout, QSplitter)?
   - [ ] Enable/disable controls according to state?
   - [ ] Correct input validation (QValidator)?

4. **â™»ï¸ Redundancy** - Avoid repetition in GUI:
   - [ ] Widgets created only once?
   - [ ] Validations centralized (not duplicated)?
   - [ ] Initialization code not repeated?

5. **âš ï¸ Inconsistency** - Consistent GUI pattern:
   - [ ] Uniform nomenclature (ed_ for QLineEdit, btn_ for QPushButton)?
   - [ ] Consistent message style?
   - [ ] Consistent layout spacing/margin?

6. **ðŸ”— Lack of Integration** - GUI connected:
   - [ ] Dock added to MainWindow?
   - [ ] Menu item connected to dock.show()?
   - [ ] Custom signals connected?
   - [ ] Import present in app.py?

7. **ðŸ§© Lower Cohesion** - Focused dock:
   - [ ] Dock only does UI (not business logic)?
   - [ ] Complex logic in separate module?
   - [ ] Each method has a single responsibility?

8. **ðŸ”— Higher Coupling** - Decoupled GUI:
   - [ ] Dock does not depend on internal implementation of other docks?
   - [ ] Communication via signals/slots (not direct calls)?
   - [ ] GUI independently testable (mock logic)?

9. **ðŸ—‘ï¸ Strange Information** - Clean code:
   - [ ] No forgotten print() debugs?
   ] No unresolved TODOs?
   - [ ] No unused widgets?

**Example of Applied GUI Review**:
```python
# âŒ BEFORE - Omission, Ambiguity, Higher Coupling
class NewComponent(QDockWidget):
    def __init__(self):
        self.btn = QPushButton("Convert")  # Vague label
        self.btn.clicked.connect(self.convert)  # No error handling
    
    def convert(self):
        data = open(self.ed_file.text()).read()  # No validation, no close
        data_str = my_convert(data)  # Business logic in GUI
        print(data_str)  # Forgotten debug

# âœ… AFTER - Complete, Clear, Decoupled
class NewComponent(BaseDock):
    """Text to DATA Converter dock widget."""
    
    # Signal for communication
    open_in_other_component_requested = Signal(str)
    
    def __init__(self, parent=None):
        super().__init__(parent)
        self._create_widgets()
        self._setup_layout()
        self._connect_signals()
        
        # Controller for business logic
        self._converter = TextToJsonConverter()
    
    def _create_widgets(self):
        """Create UI widgets."""
        self.ed_file = QLineEdit()
        self.ed_file.setPlaceholderText("Enter file path or paste text")
        
        self.btn_convert = QPushButton("Convert to DATA")
        self.btn_convert.setToolTip("Convert text to DATA format")
        
        self.btn_open_component = QPushButton("Open in Editor")
        self.btn_open_component.setEnabled(False)  # Disabled until converted
    
    def _connect_signals(self):
        """Connect signals to slots."""
        self.btn_convert.clicked.connect(self._on_convert_clicked)
        self.btn_open_component.clicked.connect(self._on_open_component_clicked)
    
    def _on_convert_clicked(self):
        """Handle convert button click."""
        file_path = self.ed_file.text().strip()
        
        if not file_path:
            QMessageBox.warning(self, "Empty Input", "Please enter a file path or text.")
            return
        
        try:
            # Read file with context manager (ensures close)
            if Path(file_path).exists():
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
            else:
                text = file_path  # Treat as direct text
            
            # Convert using controller (decoupling)
            self._data_result = self._converter.convert(text)
            
            # Visual feedback
            QMessageBox.information(self, "Success", "Conversion successful!")
            self.btn_open_component.setEnabled(True)
        
        except FileNotFoundError:
            QMessageBox.critical(self, "File Not Found", f"File not found: {file_path}")
        except Exception as e:
            QMessageBox.critical(self, "Conversion Error", f"Error: {str(e)}")
    
    def _on_open_component_clicked(self):
        """Handle open in editor button click."""
        if hasattr(self, '_data_result'):
            self.open_in_other_component_requested.emit(self._data_result)  # Signal
```

**Recommended GUI Tools**:
```bash
# Check unused Qt imports
grep -r "from PySide6" src/gui/ | cut -d: -f2 | sort | uniq

# Check unconnected signals (manual review)
grep -r "Signal(" src/gui/ | grep -v ".connect("

# Check unused widgets (manual review)
grep -r "self\.\w\+ = Q" src/gui/

# Check debug prints (CRITICAL)
grep -r "print(" src/gui/ --exclude="*_test.py"
```

**Questions to Validate GUI**:
1. â“ "Is the dock fully integrated into the menu and MainWindow?"
2. â“ "Are all signals connected and working?"
3. â“ "Is there error handling with visual feedback (QMessageBox)?"
4. â“ "Is business logic separated from GUI code?"
5. â“ "Is the code free of debug prints and unresolved TODOs?"
6. â“ "Are labels, tooltips, and messages clear and descriptive?"
7. â“ "Are resources (files, connections) closed correctly?"

**Real Example (Task Example - Text to DATA Converter)**:
```python
âœ… Import: from .gui import NewComponent
âœ… Export: __all__ = [..., 'NewComponent']
âœ… Menu: self.act_open_new_component.triggered.connect(lambda: self.dock_new_component.show())
âœ… Init: self._open_new_component() called in __init__()
âœ… Signal: open_in_other_component_requested.connect(self._load_data_from_source)
âœ… i18n: EN "Text to DATA Converter", PT-BR "Conversor de Texto para DATA"
âœ… Review: No debug prints, error handling OK, logic decoupled
âœ… Test: Menu opens dock, conversion works, signal to editor OK
```

---

### 8ï¸âƒ£.5ï¸âƒ£ **Accessibility Checklist (WCAG 2.1)** (Optional - For GUIs)

**When to Apply**:
- âœ… Desktop applications with GUI
- âœ… Web applications
- âœ… Tools used by diverse teams
- âœ… Open-source projects
- âœ… Compliance with accessibility laws

**Do Not Apply If**:
- âŒ CLI/backend only
- âŒ Internal script for personal use
- âŒ Non-public prototype

**WCAG 2.1 Level AA - POUR Principles**:

**1. Perceivable - Users must perceive the information**

```python
# âœ… ACCESSIBLE - Descriptive labels
self.btn_save = QPushButton("Save File")
self.btn_save.setToolTip("Save current file to disk (Ctrl+S)")
self.btn_save.setAccessibleName("Save file button")
self.btn_save.setAccessibleDescription("Saves the current file to disk")

# âŒ NOT ACCESSIBLE - No context
self.btn = QPushButton("OK")  # OK for what?
self.btn.setToolTip("OK")     # Doesn't help
```

**2. Operable - Users must operate the interface**

```python
# âœ… ACCESSIBLE - Keyboard navigation
self.ed_input.setFocusPolicy(Qt.StrongFocus)
self.btn_save.setShortcut(QKeySequence("Ctrl+S"))
self.btn_cancel.setShortcut(QKeySequence("Esc"))

# Visual focus indicator
self.ed_input.setStyleSheet("""
    QLineEdit:focus {
        border: 2px solid #0078d4;
        background-color: #f0f8ff;
    }
""")

# âŒ NOT ACCESSIBLE - Mouse only
self.btn.clicked.connect(self.on_click)  # No keyboard shortcut
```

**3. Understandable - Information and operation must be understandable**

```python
# âœ… ACCESSIBLE - Clear error messages
QMessageBox.critical(
    self,
    "File Not Found",
    f"The file '{filename}' could not be found.\n\n"
    f"Please check:\n"
    f"â€¢ The file path is correct\n"
    f"â€¢ You have read permissions\n"
    f"â€¢ The file was not deleted"
)

# âŒ NOT ACCESSIBLE - Generic error
QMessageBox.critical(self, "Error", "Operation failed")
```

**4. Robust - Content must be robust for assistive technologies**

```python
# âœ… ACCESSIBLE - Roles and relationships
self.lbl_name = QLabel("Name:")
self.ed_name = QLineEdit()
self.lbl_name.setBuddy(self.ed_name)  # Associates label with input

# Group related
self.group_personal = QGroupBox("Personal Information")
self.group_personal.setAccessibleName("Personal information group")

# âŒ NOT ACCESSIBLE - No structure
# Just loose widgets without semantic relationship
```

**WCAG 2.1 Accessibility Checklist**:

```markdown
### Accessibility Checklist - [GUI Name]

#### 1. Perceivable
- [ ] **Contrast**: Colors have minimum contrast 4.5:1 (normal text)?
- [ ] **Contrast**: Colors have minimum contrast 3:1 (large text >18pt)?
- [ ] **Alternatives**: Icons have descriptive tooltips?
- [ ] **Labels**: All inputs have associated labels?
- [ ] **Colors**: Information does not depend only on color?
- [ ] **Size**: Text is resizable (up to 200%)?

#### 2. Operable
- [ ] **Keyboard**: All functions accessible via keyboard?
- [ ] **Tab Order**: Navigation order makes sense?
- [ ] **Focus**: Focused element has clear visual indication?
- [ ] **Shortcuts**: Important commands have keyboard shortcuts?
- [ ] **Esc**: Dialogs can be closed with Esc?
- [ ] **Enter**: Enter submits forms/confirms actions?
- [ ] **Time**: No unexpected timeouts?

#### 3. Understandable
- [ ] **Language**: Content language is defined (i18n)?
- [ ] **Labels**: Input labels are clear?
- [ ] **Instructions**: Complex inputs have instructions?
- [ ] **Errors**: Error messages are specific and actionable?
- [ ] **Help**: Help/documentation easily accessible?
- [ ] **Navigation**: Menus have a logical structure?

#### 4. Robust
- [ ] **Screen Reader**: Tested with screen reader (NVDA/Orca)?
- [ ] **Semantics**: Correct widgets (QPushButton vs QLabel)?
- [ ] **Roles**: AccessibleName and AccessibleDescription defined?
- [ ] **Relationships**: Labels associated with buddy()?
- [ ] **Groups**: Related controls grouped (QGroupBox)?
```

**Practical Test with Screen Reader**:

```bash
# Linux - Install Orca
sudo apt install orca

# Start screen reader
orca --replace &

# Test application:
# 1. Navigate with Tab (should read each element)
# 2. Press Enter/Space (should activate buttons)
# 3. Fill forms (should read labels correctly)
# 4. Activate shortcuts (Ctrl+S, Esc, etc.)

# Windows - Use NVDA (free)
# https://www.nvaccess.org/download/

# macOS - VoiceOver (native)
# Cmd+F5 to activate
```

**Validation Tools**:

```bash
# Check color contrast
pip install color-contrast-checker
color-contrast-checker --foreground "#333333" --background "#ffffff"
# Result: AAA (passes all levels)

# Web accessibility analyzer (if applicable)
npm install -g pa11y
pa11y http://localhost:8000

# Lighthouse (Chrome DevTools)
# Audits â†’ Accessibility â†’ Generate Report
```

**Example of Accessible GUI**:

```python
class AccessibleConverterDock(QDockWidget):
    """Accessible text to DATA converter with WCAG 2.1 Level AA compliance."""
    
    def __init__(self, parent=None):
        super().__init__("Text to DATA Converter", parent)
        self.setAccessibleName("Text to DATA Converter Dock")
        self.setAccessibleDescription(
            "Convert structured text files to DATA format with preview"
        )
        self._create_accessible_widgets()
        self._setup_shortcuts()
    
    def _create_accessible_widgets(self):
        # Label + Input with buddy
        self.lbl_input = QLabel("&Input File:")
        self.ed_input = QLineEdit()
        self.ed_input.setAccessibleName("Input file path")
        self.ed_input.setAccessibleDescription("Enter path to text file to convert")
        self.ed_input.setPlaceholderText("e.g., data.csv or config.ini")
        self.lbl_input.setBuddy(self.ed_input)  # Alt+I focuses input
        
        # Button with tooltip and shortcut
        self.btn_convert = QPushButton("&Convert to DATA")
        self.btn_convert.setAccessibleName("Convert button")
        self.btn_convert.setAccessibleDescription(
            "Convert input file to DATA format. Shortcut: Ctrl+Enter"
        )
        self.btn_convert.setToolTip("Convert text to DATA (Ctrl+Enter)")
        self.btn_convert.setShortcut(QKeySequence("Ctrl+Return"))
        
        # Focus indicator
        self.btn_convert.setStyleSheet("""
            QPushButton:focus {
                border: 2px solid #0078d4;
                outline: 2px solid #0078d4;
                outline-offset: 2px;
            }
        """)
        
        # Group for semantic organization
        self.group_options = QGroupBox("Conversion &Options")
        self.group_options.setAccessibleName("Conversion options group")
        
        self.chk_pretty = QCheckBox("&Pretty print DATA")
        self.chk_pretty.setAccessibleName("Pretty print option")
        self.chk_pretty.setAccessibleDescription(
            "Format DATA with indentation for readability"
        )
        self.chk_pretty.setToolTip("Format DATA with indentation")
        
        # High contrast for status
        self.lbl_status = QLabel("Ready")
        self.lbl_status.setAccessibleName("Conversion status")
        self.lbl_status.setStyleSheet("""
            QLabel {
                color: #000000;
                background-color: #f0f0f0;
                padding: 4px;
                border: 1px solid #cccccc;
                font-weight: bold;
            }
        """)
    
    def _setup_shortcuts(self):
        """Configure keyboard shortcuts for accessibility."""
        # Esc closes the dock
        self.shortcut_close = QShortcut(QKeySequence("Esc"), self)
        self.shortcut_close.activated.connect(self.close)
        
        # F1 opens help
        self.shortcut_help = QShortcut(QKeySequence("F1"), self)
        self.shortcut_help.activated.connect(self._show_help)
    
    def _show_help(self):
        """Show accessible help dialog."""
        QMessageBox.information(
            self,
            "Text to DATA Converter - Help",
            "<h3>Keyboard Shortcuts</h3>"
            "<ul>"
            "<li><b>Ctrl+Enter</b>: Convert file</li>"
            "<li><b>Alt+I</b>: Focus input field</li>"
            "<li><b>Alt+O</b>: Toggle options group</li>"
            "<li><b>Esc</b>: Close dock</li>"
            "<li><b>F1</b>: Show this help</li>"
            "</ul>"
            "<h3>Screen Reader Support</h3>"
            "<p>This interface is fully accessible with screen readers.</p>"
        )
```

**Why this step is important**:
- âœ… **Inclusion**: ~15% of the population has some disability
- âœ… **Legal**: ADA, Section 508, EN 301 549 may require it
- âœ… **UX**: Good accessibility = good UX for everyone
- âœ… **SEO**: Accessibility improves rankings (if web)
- âœ… **Reputation**: Shows social responsibility

---

### 9ï¸âƒ£ **Verify Integration with Main Program**
- **CRITICAL**: After implementing CLI and GUI, **verify that everything is integrated and working in the context of the main program**
- It's not enough to have code working in isolation; it needs to be **accessible and operational** in the application
- Verify complete flow: menu â†’ action â†’ result
- Manually test functionality in the running program

**Complete Integration Checklist**:

1. **Full GUI Flow Test**:
   ```bash
   # Start application
   python -m app --gui
   
   # Manually test:
   [ ] Menu item appears correctly?
   [ ] Clicking the menu opens the dock?
   [ ] Dock displays all controls?
   [ ] Basic functionality works (conversion, search, etc.)?
   [ ] Signals between components work (e.g., "Open in Editor")?
   [ ] Error messages appear when appropriate?
   [ ] i18n translation works (change language and verify)?
   ```

2. **Full CLI Flow Test**:
   ```bash
   # Test help
   python -m app convert --help
   
   # Test functionality
   python -m app convert test.txt --pretty -o output.data
   
   # Test pipes
   echo "name: John" | python -m app convert -
   
   # Verify:
   [ ] Help text appears?
   [ ] Arguments are recognized?
   [ ] Functionality executes without errors?
   [ ] Output is correct?
   [ ] Correct exit codes (0=success, 1=error)?
   ```

3. **Inter-Component Integration Test**:
   ```bash
   # Example: Convert text â†’ Open in editor
   [ ] Clicking "Open in Editor" in the Text to DATA Converter opens the Editor?
   [ ] DATA is loaded correctly in the Editor?
   [ ] Editor can save the result?
   
   # Example: Search â†’ Open file
   [ ] Clicking search result opens correct file?
   [ ] Cursor position goes to the correct line?
   ```

4. **Robustness Test**:
   ```bash
   # Error scenarios
   [ ] File not found displays clear message?
   [ ] Invalid input is handled gracefully?
   [ ] Canceled operation does not leave inconsistent state?
   [ ] Resources are correctly released (files closed, memory)?
   ```

5. **Performance Test** (if applicable):
   ```bash
   # Large files
   [ ] Processes files >10MB without freezing?
   [ ] Interface remains responsive during long operation?
   [ ] Progress bar/visual feedback works?
   [ ] Cancellation works during long operation?
   ```

**Real Example of Integration Problem**:
```python
# âŒ PROBLEM FOUND IN INTEGRATION:
# Task Example - Text to DATA Converter CLI
# Problem: Extractor() was being called without 3 mandatory parameters

# BEFORE (broke during integration):
def main():
    if args.command == 'convert':
        extractor = Extractor()  # âŒ TypeError: missing 3 required arguments

# AFTER (fixed):
def main():
    if args.command == 'convert':
        extractor = Extractor(
            avoid_keys="",
            avoid_keys_parameter="equals",
            with_quotation_marks=False
        )  # âœ… Works!
```

**Questions to Validate Integration**:
1. â“ "Can the end-user easily access the functionality?"
2. â“ "Do all usage flows work end-to-end?"
3. â“ "Are there any errors or warnings in the console during normal use?"
4. â“ "Is the functionality consistent with the rest of the application?"
5. â“ "Is the documentation (help text, tooltips) clear and correct?"

**Why is this step critical?**:
- âœ… Detects problems that unit tests don't catch
- âœ… Validates real user experience
- âœ… Ensures all work is truly usable
- âœ… Avoids surprises after commit (tested code â‰  integrated code)

---

### 9ï¸âƒ£.5ï¸âƒ£ **Peer Code Review** (Optional - For Teams)

**When to Apply**:
- âœ… Team projects (2+ developers)
- âœ… Critical changes (security, data)
- âœ… Complex features (>200 lines)
- âœ… Code that others will maintain
- âœ… Open-source with contributors

**Do Not Apply If**:
- âŒ Solo/personal project
- âŒ Critical emergency hotfix
- âŒ Trivial changes (typos in docs)
- âŒ Disposable prototype

**Pull Request Process**:

```bash
# 1. Create feature branch
git checkout -b feature/task-42-add-export

# 2. Implement and commit
git add project_app.py
git commit -m "Add CSV export feature (Task Example)"

# 3. Push and create PR
git push origin feature/task-42-add-export
# Open PR on GitHub/GitLab with template

# 4. Await review from at least 1 colleague
# 5. Implement requested changes
# 6. Merge after approval
```

**Pull Request Template**:

```markdown
## Description
Implements CSV export for Task Example of Simplicity Protocol.

## Type of Change
- [x] New feature
- [ ] Bug fix
- [ ] Refactoring
- [ ] Documentation

## Simplicity Protocol Checklist
- [x] Step 1: Task defined and selected (Task Example)
- [x] Step 2: Divided into subtasks
- [x] Step 6: Code implemented
- [x] Step 7: Manual code review (self-review)
- [x] Step 8: GUI manually tested
- [x] Step 9: Integration verified
- [x] Step 10: Unit tests (pytest)
- [x] Step 11: Code organized (PEP 8)

## How to Test
```bash
pytest tests/test_csv_export.py -v
python project_app.py --export tasks.csv
```

## Screenshots (if applicable)
![CSV Export Dialog](screenshots/csv-export.png)

## Related
- Closes #42
- Related to #38 (Data Export Epic)
```

**Reviewer Checklist**:

```markdown
### Code Review Checklist - [PR Name]

#### 1. Functionality
- [ ] **Feature**: Does the implementation solve the described problem?
- [ ] **Edge Cases**: Are edge cases handled (empty input, None, etc.)?
- [ ] **Errors**: Do errors have clear messages?
- [ ] **UX**: Is the interface intuitive and consistent?

#### 2. Code
- [ ] **Readability**: Is the code clear and self-documented?
- [ ] **Simplicity**: Is the solution as simple as possible?
- [ ] **Duplication**: Is there no duplicate code?
- [ ] **Names**: Do variables/functions have descriptive names?
- [ ] **Comments**: Do comments explain "why," not "what"?

#### 3. Architecture
- [ ] **Separation**: Is logic separated from presentation?
- [ ] **Dependencies**: Are new dependencies justified?
- [ ] **Patterns**: Does it follow project patterns?
- [ ] **SOLID**: Are SOLID principles respected?

#### 4. Tests
- [ ] **Coverage**: Does new code have unit tests?
- [ ] **Cases**: Do tests cover critical scenarios?
- [ ] **Pass**: Do all tests pass in CI?
- [ ] **Readability**: Are tests clear and maintainable?

#### 5. Security
- [ ] **Input**: Does input validation exist?
- [ ] **SQL**: No SQL injection (use parameterized queries)?
- [ ] **Secrets**: No credentials in code?
- [ ] **Permissions**: Adequate permission checks?

#### 6. Performance
- [ ] **Loops**: Are loops efficient (no unnecessary O(nÂ²))?
- [ ] **Queries**: Are DB queries optimized (indexes)?
- [ ] **Memory**: No memory leaks (close resources)?
- [ ] **Caching**: Is caching applied where it makes sense?

#### 7. Documentation
- [ ] **Docstrings**: Do public functions have docstrings?
- [ ] **README**: README updated if necessary?
- [ ] **CHANGELOG**: Is a changelog entry created?
- [ ] **Comments**: Are complex decisions documented?
```

**Example of Review Comment**:

```markdown
**âŒ Problem - Missing input validation**

```python
# Line 142
def export_to_csv(self, filename):
    with open(filename, 'w') as f:
        # ...
```

**Suggestion**:
```python
def export_to_csv(self, filename):
    if not filename:
        raise ValueError("Filename cannot be empty")
    
    if not filename.endswith('.csv'):
        raise ValueError("Filename must end with .csv")
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            # ...
    except IOError as e:
        raise IOError(f"Failed to write CSV: {e}")
```

**Rationale**: Without validation, the code can fail silently or generate invalid files.
```

**Tools for Code Review**:

```bash
# GitHub CLI - Create PR via terminal
gh pr create --title "Add CSV export" --body "Implements Task Example"

# Review PR locally
gh pr checkout 123
pytest
python project_app.py --test

# Approve PR
gh pr review 123 --approve --body "LGTM! Clean and tested code."

# Request changes
gh pr review 123 --request-changes --body "Please add input validation (see comments)"
```

**GitLab - Merge Request Template**:

```yaml
# .gitlab/merge_request_templates/feature.md
## Feature Description
<!-- Describe what was implemented -->

## Simplicity Protocol Checklist
- [ ] Task defined (Step 1)
- [ ] Code implemented (Step 6)
- [ ] Tests passing (Step 10)
- [ ] Documentation updated (Step 12)

## How to Test
<!-- Commands to test the feature -->

## Screenshots
<!-- If applicable -->

/label ~feature
/assign @reviewer-name
```

**Code Review Culture**:

```markdown
### Principles for Constructive Reviews

1. **Be Kind**: "Consider adding validation" > "This is wrong"
2. **Explain Why**: Not just "Change this," but "Change this because..."
3. **Ask**: "What do you think about...?" > "You should..."
4. **Approve Quickly**: If it's good enough, approve (don't seek perfection)
5. **Learn**: See reviews as an opportunity to learn
6. **Automate**: Use linters for style, focus on logic

### Anti-Patterns to Avoid
- âŒ Excessive nitpicking (spaces, commas)
- âŒ Rewriting everything your way
- âŒ Leaving PR stalled for days
- âŒ Approving without reading (rubber stamping)
- âŒ Vague comments ("This is bad")
```

**Code Review Metrics**:

```python
# Example: tracking review metrics
review_metrics = {
    "pr_number": 123,
    "author": "alice",
    "reviewer": "bob",
    "lines_changed": 250,
    "files_changed": 3,
    "comments": 8,
    "time_to_first_review_hours": 4,
    "time_to_merge_hours": 18,
    "result": "approved"
}

# Healthy metrics:
# - Time to first review: < 8h
# - Time to merge: < 48h
# - Comments per PR: 3-10 (not too many, not too few)
# - Approval rate: > 80% (if < 50%, reviews are too strict)
```

**Why Code Review is valuable**:
- âœ… **Quality**: Detects bugs before production (15-20% on average)
- âœ… **Knowledge**: Distributes code knowledge within the team
- âœ… **Mentoring**: Junior developers learn from seniors
- âœ… **Consistency**: Maintains uniform project standards
- âœ… **Documentation**: PR discussions = historical context

---

### 10. **Run Tests**
- **Mandatory**: Unit tests for each public function
- **Goal**: 100% coverage of implemented functionalities
- **Tools**: `unittest` (native) or `pytest`
- **CRITICAL**: Test the system **after integration** (integrated GUI + CLI)
- **IMPORTANT**: Execute **AFTER** code review (Steps 7 and 8)

**Test Categories**:
1. **Happy Path**: Normal use cases
2. **Edge Cases**: Empty values, None, long strings
3. **Error Handling**: Expected exceptions
4. **Integration**: Complete flow (including GUI/CLI integration)
5. **Quality Validation**: Tests that validate the absence of the 9 problems from Steps 7 and 8

**Task Example**:
```python
âœ… test_extract_from_dict_simple()
âœ… test_extract_from_obj_type()
âœ… test_simple_substitution_same_value()
âœ… test_different_values_no_substitution()
âœ… test_apply_substitutions_tsx_file()
âœ… test_update_multiple_files()
# ... 12 tests in total (100% passing)
```

**Why test AFTER integration and review?**:
- Ensures tests validate the **integrated system**, not isolated components
- Detects integration problems during testing
- Validates that features actually work in the context of the application
- Avoids false positives (tests pass but feature is not accessible)
- Code has already been reviewed, so tests validate **quality code**

**Why?**: Ensure quality, prevent regressions, facilitate future maintenance.

---

#### ðŸ›¡ï¸ **Step 9.1 - Security in Tests (CRITICAL)**

**Problem Identified** (Task Example - 01/12/2025):
- GUI tests hung in an **infinite loop** for >1 hour without timeout
- No automatic deadlock or hang detection
- Tests waited for a non-existent X11 display (headless environment)

**Mandatory Solutions**:

1. **â±ï¸ Mandatory Maximum Timeout** (30s per test):
   ```bash
   # ALWAYS use timeout in tests
   pytest tests/test_*.py --timeout=30 -v
   
   # Install pytest-timeout plugin if necessary
   pip install pytest-timeout
   ```

2. **ðŸš¨ Infinite Loop Detection** (warning in 10s):
   ```bash
   # More aggressive timeout to detect loops
   timeout 10s pytest tests/test_specific.py || echo "âš ï¸ TIMEOUT: Possible infinite loop detected!"
   ```

3. **ðŸ–¥ï¸ Mandatory Headless Environment** (GUI tests without display):
   ```bash
   # Use Qt offscreen platform
   QT_QPA_PLATFORM=offscreen pytest tests/test_gui_*.py -v --timeout=30
   
   # OR use pytest-xvfb for virtual X11 environment
   pip install pytest-xvfb
   pytest tests/test_gui_*.py --xvfb-backend xvfb --timeout=30
   ```

4. **âœ… Mandatory Dry-Run** (before executing):
   ```bash
   # 1. Check syntax
   python -m py_compile tests/test_*.py && echo "âœ… Valid syntax"
   
   # 2. Check imports
   python -c "from tests.test_module import *; print('âœ… Imports OK')"
   
   # 3. List tests without executing
   pytest tests/test_*.py --collect-only
   ```

5. **â²ï¸ Time Monitoring** (record duration):
   ```bash
   # Measure total time and save log
   time pytest tests/test_*.py -v --timeout=30 | tee test_output.log
   
   # Use pytest-benchmark for metrics
   pytest tests/test_*.py --benchmark-only --timeout=30
   ```

**Why?**: Prevent infinite hangs, protect development time, ensure reliable tests.

---

### 10.5 **Profiling and Optimization** (Optional - For Critical Features)

**When to Apply**:
- âœ… Performance-critical features (loops, data processing)
- âœ… Operations that process large files (>10MB)
- âœ… Code that runs frequently (hot paths)
- âœ… Applications with latency requirements (<100ms)
- âœ… When users report slowness

**Do Not Apply If**:
- âŒ Feature runs rarely (initial setup)
- âŒ Performance is already good enough (<1s for user)
- âŒ Configuration/initialization code
- âŒ Prototypes or POCs

**Profiling with cProfile**:

```bash
# CPU Profiling - find slow functions
python -m cProfile -s cumulative project_app.py > profile.txt

# Profiling with visualization
pip install snakeviz
python -m cProfile -o profile.stats project_app.py
snakeviz profile.stats  # Opens browser with flamegraph
```

**Example Analysis**:

```python
# âŒ SLOW - O(nÂ²) to process tasks
def find_duplicates_slow(tasks):
    """Finds duplicate tasks - SLOW VERSION."""
    duplicates = []
    for i, task1 in enumerate(tasks):
        for j, task2 in enumerate(tasks):
            if i != j and task1.title == task2.title:
                duplicates.append((task1, task2))
    return duplicates

# Profiling reveals: 85% of time in find_duplicates_slow()
# For 1000 tasks: 5.2 seconds

# âœ… FAST - O(n) using set
def find_duplicates_fast(tasks):
    """Finds duplicate tasks - OPTIMIZED VERSION."""
    seen = {}
    duplicates = []
    for task in tasks:
        if task.title in seen:
            duplicates.append((seen[task.title], task))
        else:
            seen[task.title] = task
    return duplicates

# After optimization: 0.02 seconds (260x faster)
```

**Memory Profiling**:

```bash
# Install memory_profiler
pip install memory_profiler

# Decorate function to profile
```

```python
from memory_profiler import profile

@profile
def load_large_file(filepath):
    """Load and process large DATA file."""
    with open(filepath, 'r') as f:
        data = data.load(f)  # Loads everything into memory
    
    # Process...
    results = []
    for item in data:
        results.append(process_item(item))
    
    return results

# Execute with profiling
# python -m memory_profiler project_app.py
```

**Memory Optimization Example**:

```python
# âŒ MEMORY LEAK - Loads entire file (500MB)
def process_large_csv_bad(filepath):
    with open(filepath, 'r') as f:
        lines = f.readlines()  # 500MB in memory!
    
    results = []
    for line in lines:
        results.append(process_line(line))
    return results

# Memory profiler shows: Peak of 520MB

# âœ… OPTIMIZED - Streaming (constant 5MB)
def process_large_csv_good(filepath):
    results = []
    with open(filepath, 'r') as f:
        for line in f:  # Reads line by line
            results.append(process_line(line))
    return results

# Memory profiler shows: Peak of 8MB (65x less)
```

**Line-by-Line Profiling**:

```python
# Install line_profiler
# pip install line_profiler

# Decorate suspicious function
@profile  # Requires kernprof
def complex_calculation(data):
    """Function to profile line-by-line."""
    # Line 1: setup
    total = 0
    
    # Line 2: main loop
    for item in data:
        # Line 3: heavy calculation
        result = expensive_operation(item)
        total += result
    
    return total

# Execute
# kernprof -l -v project_app.py
# Shows time per line of code
```

**Benchmarking Before/After**:

```python
import time

def benchmark(func, *args, iterations=100):
    """Benchmark function performance."""
    times = []
    for _ in range(iterations):
        start = time.perf_counter()
        func(*args)
        end = time.perf_counter()
        times.append(end - start)
    
    avg = sum(times) / len(times)
    return {
        "avg_ms": avg * 1000,
        "min_ms": min(times) * 1000,
        "max_ms": max(times) * 1000
    }

# Before optimization
before = benchmark(find_duplicates_slow, large_task_list)
print(f"BEFORE: {before['avg_ms']:.2f}ms")

# After optimization
after = benchmark(find_duplicates_fast, large_task_list)
print(f"AFTER: {after['avg_ms']:.2f}ms")
print(f"SPEEDUP: {before['avg_ms'] / after['avg_ms']:.1f}x")

# Output:
# BEFORE: 5240.32ms
# AFTER: 20.15ms
# SPEEDUP: 260.0x
```

**Optimization Checklist**:

```markdown
### Performance Checklist - [Feature Name]

#### Profiling Performed
- [ ] **CPU**: cProfile executed and analyzed
- [ ] **Memory**: memory_profiler executed (if > 100MB)
- [ ] **Hotspots**: Top 3 slowest functions identified
- [ ] **Baseline**: Time/memory before optimization documented

#### Optimizations Applied
- [ ] **Algorithm**: Complexity reduced (O(nÂ²) â†’ O(n log n) or O(n))
- [ ] **Structures**: Appropriate data structures (dict vs list)
- [ ] **I/O**: I/O optimized (buffering, streaming)
- [ ] **Cache**: Caching applied for repeated operations
- [ ] **Lazy**: Lazy loading for large data

#### Validation
- [ ] **Benchmark**: Before/after documented with speedup
- [ ] **Tests**: All tests still pass
- [ ] **Correctness**: Output identical to previous version
- [ ] **Limits**: Tested with realistic volume (10x typical data)

#### Documentation
- [ ] **Comments**: Non-obvious optimizations documented
- [ ] **Big-O**: Complexity documented in docstring
- [ ] **Trade-offs**: Trade-offs explained (memory vs speed)
```

**Advanced Tools**:

```bash
# py-spy - Sampling profiler (without modifying code)
pip install py-spy
py-spy record -o profile.svg -- python project_app.py
# Generates interactive flamegraph

# Scalene - CPU + Memory + GPU profiler
pip install scalene
scalene project_app.py
# Interactive dashboard in terminal

# pytest-benchmark for tests
pip install pytest-benchmark

# Example benchmark test
def test_find_duplicates_performance(benchmark):
    tasks = generate_large_task_list(1000)
    result = benchmark(find_duplicates_fast, tasks)
    assert len(result) > 0
    # benchmark automatically measures time
```

**When to Stop Optimizing**:

```python
# Pareto Rule: 80% of gains come from 20% of effort

# âœ… WORTH OPTIMIZING:
# - Reduction from 5s â†’ 0.5s (10x) = 4.5s saved per execution
# - If executed 100x/day = 450s (7.5min) saved/day

# âŒ NOT WORTH OPTIMIZING:
# - Reduction from 0.05s â†’ 0.02s (2.5x) = 0.03s saved
# - If executed 10x/day = 0.3s saved/day (insignificant)

# Criterion: Optimize if time saved Ã— frequency > 1 minute/day
```

**Why Profiling is important**:
- âœ… **Evidence**: Optimize based on data, not "gut feeling"
- âœ… **Focus**: Identify real bottlenecks (not where we think they are)
- âœ… **ROI**: Prioritize optimizations with the greatest impact
- âœ… **Avoid**: Premature micro-optimizations that complicate code
- âœ… **Scalability**: Ensure code scales with larger data

---

### 10.6 **CI/CD Quality Gates** â­ (Optional - HIGH PRIORITY)

**When to Apply**:
- âœ… Team projects (2+ people)
- âœ… Production or critical code
- âœ… Open-source with contributors
- âœ… When consistent quality needs to be ensured
- âœ… Environments with multiple branches

**Do Not Apply If**:
- âŒ Solo/experimental project
- âŒ Disposable prototype
- âŒ Single-use scripts
- âŒ No CI infrastructure (GitHub/GitLab/Jenkins)

**Pre-commit Hooks - Local Validation**:

```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-data
      - id: check-added-large-files
        args: ['--maxkb=500']
  
  - repo: https://github.com/psf/black
    rev: 23.12.1
    hooks:
      - id: black
        language_version: python3.11
  
  - repo: https://github.com/pycqa/flake8
    rev: 7.0.0
    hooks:
      - id: flake8
        args: ['--max-line-length=88', '--extend-ignore=E203']
  
  - repo: https://github.com/PyCQA/bandit
    rev: 1.7.6
    hooks:
      - id: bandit
        args: ['-ll', '-i']  # Low severity, ignore issues
  
  - repo: local
    hooks:
      - id: pytest
        name: pytest
        entry: pytest
        language: system
        pass_filenames: false
        args: ['tests/', '-v', '--tb=short']
```

```bash
# Install pre-commit
pip install pre-commit

# Activate hooks
pre-commit install

# Now every git commit executes validations automatically
# If it fails, commit is blocked until corrected
```

**GitHub Actions - CI Pipeline**:

```yaml
# .github/workflows/ci.yml
name: CI Quality Gates

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  quality-checks:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov flake8 black bandit mypy
    
    - name: Code Formatting (Black)
      run: |
        black --check project_app.py
      continue-on-error: false
    
    - name: Linting (Flake8)
      run: |
        flake8 project_app.py --max-line-length=88 --statistics
      continue-on-error: false
    
    - name: Type Checking (MyPy)
      run: |
        mypy project_app.py --ignore-missing-imports
      continue-on-error: true  # Warnings, not errors
    
    - name: Security Scan (Bandit)
      run: |
        bandit -r project_app.py -ll
      continue-on-error: false
    
    - name: Unit Tests with Coverage
      run: |
        pytest tests/ --cov=. --cov-report=xml --cov-report=term
      continue-on-error: false
    
    - name: Upload Coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: true
    
    - name: Coverage Threshold Check
      run: |
        coverage report --fail-under=80
      # Fails if coverage < 80%
```

**GitLab CI - Pipeline with Quality Gates**:

```yaml
# .gitlab-ci.yml
stages:
  - lint
  - test
  - security
  - deploy

variables:
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"

cache:
  paths:
    - .cache/pip

lint:black:
  stage: lint
  image: python:3.11
  script:
    - pip install black
    - black --check project_app.py
  allow_failure: false

lint:flake8:
  stage: lint
  image: python:3.11
  script:
    - pip install flake8
    - flake8 project_app.py --max-line-length=88 --statistics
  allow_failure: false

test:pytest:
  stage: test
  image: python:3.11
  script:
    - pip install -r requirements.txt
    - pip install pytest pytest-cov
    - pytest tests/ --cov=. --cov-report=term --cov-report=html
    - coverage report --fail-under=80
  coverage: '/TOTAL.*\s+(\d+%)$/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml
    paths:
      - htmlcov/
  allow_failure: false

security:bandit:
  stage: security
  image: python:3.11
  script:
    - pip install bandit
    - bandit -r project_app.py -f data -o bandit-report.data
  artifacts:
    reports:
      sast: bandit-report.data
  allow_failure: false

deploy:production:
  stage: deploy
  script:
    - echo "Deploying to production..."
    - # Deployment commands
  only:
    - main
  when: manual  # Manual deploy after quality gates pass
```

**Quality Metrics - Dashboards**:

```python
# Script to generate quality report
import subprocess
import data

def run_quality_checks():
    """Executes quality gates and generates report."""
    
    results = {
        "timestamp": datetime.now().isoformat(),
        "checks": {}
    }
    
    # 1. Code Coverage
    cov = subprocess.run(
        ["pytest", "--cov=.", "--cov-report=data"],
        capture_output=True
    )
    with open("coverage.data") as f:
        results["checks"]["coverage"] = data.load(f)["totals"]["percent_covered"]
    
    # 2. Linting Score
    flake8 = subprocess.run(
        ["flake8", "project_app.py", "--statistics"],
        capture_output=True,
        text=True
    )
    results["checks"]["linting_errors"] = len(flake8.stdout.splitlines())
    
    # 3. Security Issues
    bandit = subprocess.run(
        ["bandit", "-r", ".", "-f", "data"],
        capture_output=True
    )
    bandit_data = data.loads(bandit.stdout)
    results["checks"]["security_issues"] = len(bandit_data["results"])
    
    # 4. Type Coverage (MyPy)
    mypy = subprocess.run(
        ["mypy", "project_app.py", "--data-report", ".mypy"],
        capture_output=True
    )
    # Parse MyPy report...
    
    # Quality Score (0-100)
    score = (
        results["checks"]["coverage"] * 0.4 +
        (100 - min(results["checks"]["linting_errors"], 100)) * 0.3 +
        (100 - min(results["checks"]["security_issues"] * 10, 100)) * 0.3
    )
    results["quality_score"] = round(score, 2)
    
    # Pass/Fail Gates
    results["gates"] = {
        "coverage": results["checks"]["coverage"] >= 80,
        "linting": results["checks"]["linting_errors"] == 0,
        "security": results["checks"]["security_issues"] == 0
    }
    results["passed"] = all(results["gates"].values())
    
    return results

# Integrate with CI
if __name__ == "__main__":
    results = run_quality_checks()
    print(data.dumps(results, indent=2))
    
    if not results["passed"]:
        print("\nâŒ Quality gates FAILED!")
        exit(1)
    else:
        print("\nâœ… All quality gates PASSED!")
```

**Status Badge in README**:

```markdown
# MyProject - Task Management

[![CI Status](https://github.com/user/myproject/workflows/CI/badge.svg)](https://github.com/user/myproject/actions)
[![Coverage](https://codecov.io/gh/user/myproject/branch/main/graph/badge.svg)](https://codecov.io/gh/user/myproject)
[![Quality Gate](https://sonarcloud.io/api/project_badges/measure?project=myproject&metric=alert_status)](https://sonarcloud.io/dashboard?id=myproject)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

Badges visually show quality status
```

**Why CI/CD is crucial**:
- âœ… **Automation**: Validates quality without manual intervention
- âœ… **Consistency**: Same rules for all developers
- âœ… **Prevention**: Detects problems before merge/deploy
- âœ… **Confidence**: Team knows that broken code won't go to production
- âœ… **Speed**: Fast feedback (minutes, not hours)

---

### 11. **Organize Project Root Folder**
- âœ… Imports validated (module loads without errors)
- ðŸ“ **Documented limitation**: GUI tests require an unconfigured headless environment

---

#### ðŸ”¬ **Step 9.2 - Tests in Threads/Processes with Monitoring (ADVANCED)**

**Objective**: Full control over test execution with the possibility to **interrupt**, **monitor**, and **log** progress in real-time.

**When to Use**:
- GUI tests that might hang
- Long-running tests (>1 min)
- Tests with external dependencies (network, database)
- Need for real-time logging
- Need for manual cancellation during execution

**Implementation with `multiprocessing.Process`**:

```python
# tests/test_runner_monitored.py
import multiprocessing as mp
import time
import sys
from queue import Empty

def run_tests_in_process(test_module: str, queue: mp.Queue, timeout: int = 30):
    """
    Executes tests in a separate process with logging to a queue.
    
    Args:
        test_module: Test module (e.g., 'tests.test_file_list_dock')
        queue: Queue for progress communication
        timeout: Timeout in seconds
    """
    try:
        import pytest
        
        # Configure real-time logging
        class QueueReporter:
            def __init__(self, queue):
                self.queue = queue
            
            def pytest_runtest_logreport(self, report):
                """pytest hook to capture results."""
                if report.when == 'call':
                    status = 'âœ… PASS' if report.passed else 'âŒ FAIL'
                    self.queue.put({
                        'type': 'test_result',
                        'test': report.nodeid,
                        'status': status,
                        'duration': report.duration
                    })
        
        # Execute pytest with custom reporter
        queue.put({'type': 'info', 'msg': f'Starting tests: {test_module}'})
        
        result = pytest.main([
            test_module,
            '-v',
            f'--timeout={timeout}',
            '--tb=short',
            '-p', 'no:cacheprovider'  # Disable cache
        ])
        
        queue.put({'type': 'info', 'msg': f'Tests finished. Exit code: {result}'})
        queue.put({'type': 'exit', 'code': result})
        
    except Exception as e:
        queue.put({'type': 'error', 'msg': str(e)})
        queue.put({'type': 'exit', 'code': 1})

def monitor_test_execution(test_module: str, max_timeout: int = 300):
    """
    Monitors test execution with full control.
    
    Args:
        test_module: Test module
        max_timeout: Maximum timeout in seconds (default: 5 min)
    
    Returns:
        dict: Execution result with statistics
    """
    queue = mp.Queue()
    process = mp.Process(
        target=run_tests_in_process,
        args=(test_module, queue, 30)
    )
    
    print(f"ðŸš€ Starting tests: {test_module}")
    print(f"â±ï¸  Maximum timeout: {max_timeout}s")
    print(f"ðŸ“Š Monitoring active. Press Ctrl+C to cancel.\n")
    
    process.start()
    start_time = time.time()
    results = {'passed': 0, 'failed': 0, 'tests': []}
    
    try:
        while process.is_alive():
            elapsed = time.time() - start_time
            
            # Check global timeout
            if elapsed > max_timeout:
                print(f"\nâš ï¸  GLOBAL TIMEOUT ({max_timeout}s exceeded)")
                process.terminate()
                process.join(timeout=5)
                if process.is_alive():
                    process.kill()
                return {'status': 'timeout', 'elapsed': elapsed, 'results': results}
            
            # Read messages from the queue (non-blocking)
            try:
                msg = queue.get(timeout=0.5)
                
                if msg['type'] == 'test_result':
                    print(f"  {msg['status']} {msg['test']} ({msg['duration']:.2f}s)")
                    results['tests'].append(msg)
                    if 'âœ…' in msg['status']:
                        results['passed'] += 1
                    else:
                        results['failed'] += 1
                
                elif msg['type'] == 'info':
                    print(f"â„¹ï¸  {msg['msg']}")
                
                elif msg['type'] == 'error':
                    print(f"âŒ ERROR: {msg['msg']}")
                
                elif msg['type'] == 'exit':
                    process.join(timeout=2)
                    elapsed = time.time() - start_time
                    print(f"\nâœ… Tests finished in {elapsed:.2f}s")
                    return {
                        'status': 'completed',
                        'exit_code': msg['code'],
                        'elapsed': elapsed,
                        'results': results
                    }
            
            except Empty:
                # No message, continue monitoring
                pass
            
            # Show progress every 10s
            if int(elapsed) % 10 == 0 and int(elapsed) > 0:
                print(f"â³ Executing... {int(elapsed)}s ({results['passed']} passed, {results['failed']} failed)")
    
    except KeyboardInterrupt:
        print("\nâš ï¸  Manual cancellation (Ctrl+C)")
        process.terminate()
        process.join(timeout=5)
        if process.is_alive():
            process.kill()
        elapsed = time.time() - start_time
        return {'status': 'cancelled', 'elapsed': elapsed, 'results': results}
    
    finally:
        if process.is_alive():
            process.terminate()
            process.join(timeout=5)

# Example usage:
if __name__ == '__main__':
    result = monitor_test_execution('tests/test_advanced_file_search.py', max_timeout=300)
    
    print(f"\n{'='*60}")
    print(f"Status: {result['status']}")
    print(f"Time: {result['elapsed']:.2f}s")
    print(f"Passed: {result['results']['passed']}")
    print(f"Failed: {result['results']['failed']}")
    print(f"{'='*60}")
```

**Practical Usage**:

```bash
# 1. Create monitored runner
cat > tests/run_tests_monitored.py << 'EOF'
# [code above]
EOF

# 2. Execute with monitoring
python tests/run_tests_monitored.py

# 3. Cancel at any time (Ctrl+C)
# The process will be terminated gracefully
```

**Advantages**:
- âœ… **Full control**: Can cancel tests at any time
- âœ… **Real-time logging**: See progress of each test
- âœ… **Global + individual timeout**: Double protection
- âœ… **Statistics**: Pass/fail in real-time
- âœ… **Isolation**: Tests run in a separate process (don't block the terminal)
- âœ… **Guaranteed cleanup**: `terminate()` + `kill()` forced if necessary

**Optional Configurations**:

1. **File Logging** (in addition to stdout):
   ```python
   # Add to run_tests_in_process:
   import logging
   logging.basicConfig(
       filename=f'test_{time.time()}.log',
       level=logging.INFO,
       format='%(asctime)s - %(message)s'
   )
   ```

2. **Sound Notification** (upon completion):
   ```python
   import os
   # At the end of monitor_test_execution:
   os.system('paplay /usr/share/sounds/freedesktop/stereo/complete.oga')
   ```

3. **CI/CD Integration**:
   ```python
   # Return correct exit code:
   sys.exit(0 if result['status'] == 'completed' and result['results']['failed'] == 0 else 1)
   ```

**Additional Checklist (Step 9.2 - Optional)**:
```
[ ] Create test_runner_monitored.py with multiprocessing
[ ] Define global timeout (default: 5 min)
[ ] Define individual timeout per test (default: 30s)
[ ] Implement real-time logging (Queue)
[ ] Test manual cancellation (Ctrl+C)
[ ] Verify process cleanup (ps aux | grep pytest)
```

**When NOT to use**:
- Simple and fast tests (<10s total)
- Tests without GUI (pure backend)
- CI/CD with native timeout configured
- First execution of tests (unnecessary overhead)

---

### 11. **Organize Project Root Folder**
- **CRITICAL**: Before documentation and commit, **organize the root folder recursively**
- **MANDATORY**: Files must be organized in the correct folders before commit
- Remove temporary files, unnecessary backups
- Verify all files are in the correct places
- Clear cache and generated files (`__pycache__`, `.pyc`)
- Ensure `.gitignore` is updated

**Organization Checklist**:
1. **Removal of Temporary Files**:
   ```bash
   # Remove old backups
   rm -f *.backup_* *.bak *~
   
   # Clear Python cache
   find . -type d -name "__pycache__" -exec rm -rf {} +
   find . -type f -name "*.pyc" -delete
   find . -type f -name "*.pyo" -delete
   ```

2. **Directory Structure Verification (MANDATORY)**:
   - `src/` - source code
   - `tests/` - **ALL test files** (mandatory)
   - `docs/` - **ALL documents and markdown files** (mandatory)
   - Organized root files (README, setup.py, etc.)

3. **Mandatory Recursive Organization**:
   
   **âš ï¸ FUNDAMENTAL RULE**: 
   > Before commit, files must be organized in folders recursively. This is **mandatory** to keep the environment clean and organized.

   **Specific Rules by File Type**:
   
   a) **Test Files** â†’ `tests/`
      - âœ… `test_*.py`, `*_test.py` â†’ `tests/`
      - âœ… Test structure should mirror code structure
      - âœ… Example: `tests/unit/`, `tests/integration/`, `tests/fixtures/`
   
   b) **Documents and Markdown** â†’ `docs/`
      - âœ… All `.md` files (except root README.md) â†’ `docs/`
      - âœ… Documentation files â†’ `docs/`
      - âœ… **Recursive organization within `docs/`**:
        - `docs/api/` - API documentation
        - `docs/tutorials/` - Tutorials
        - `docs/architecture/` - Architectural decisions (ADRs)
        - `docs/user-guide/` - User guides
        - `docs/dev-guide/` - Development guides
        - `docs/adr/` - Architecture Decision Records (see Step 11.5)
      - âœ… Create subfolders that identify file context
   
   c) **Source Code** â†’ `src/` or appropriate folder
      - âœ… Organize by modules/features
      - âœ… Example: `src/core/`, `src/utils/`, `src/api/`

**Complete Example**:
```bash
# BEFORE (disorganized):
â”œâ”€â”€ src/
â”œâ”€â”€ test_utils.py              âŒ test outside tests/
â”œâ”€â”€ API_DOCS.md                âŒ doc outside docs/
â”œâ”€â”€ tutorial.md                âŒ doc outside docs/
â”œâ”€â”€ apply_v2913_patches.py     âŒ temporary
â”œâ”€â”€ test_temp.py               âŒ temporary test
â”œâ”€â”€ backup_old/                âŒ old backup
â”œâ”€â”€ __pycache__/               âŒ cache
â””â”€â”€ file.py.backup_v2913       âŒ unnecessary backup

# AFTER (recursively organized):
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ tests/                     âœ… ALL tests
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â””â”€â”€ test_utils.py     âœ… test moved
â”‚   â””â”€â”€ integration/
â”œâ”€â”€ docs/                      âœ… ALL documents
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ API_DOCS.md       âœ… doc moved
â”‚   â”œâ”€â”€ tutorials/
â”‚   â”‚   â””â”€â”€ tutorial.md       âœ… doc moved
â”‚   â””â”€â”€ adr/                   âœ… ADRs organized
â””â”€â”€ README.md                  âœ… root README kept
```

**Why?**: Keep repository clean, avoid committing garbage, facilitate navigation, professionalism, recursive organization ensures scalability. Document the **clean** and **organized** state of the project.

---

### 11.5 **Architecture Decision Records (ADR)** (Optional)

**When to Apply**:
- âœ… Important architectural decisions (framework, library, pattern)
- âœ… Significant trade-offs were made
- âœ… Long-term projects (> 6 months)
- âœ… Teams with turnover (onboarding)
- âœ… When "why did we do it this way?" will be asked

**Do Not Apply If**:
- âŒ Trivial decisions (naming, formatting)
- âŒ Short-term solo project
- âŒ Disposable prototype
- âŒ Obvious/conventional decisions

**What is ADR?**

ADR (Architecture Decision Record) documents **why** important decisions were made, not just **what** was decided. Useful for:
- Justifying choices for future developers
- Avoiding reopening already resolved discussions
- Learning from past decisions (good and bad)

**ADR Template**:

```markdown
# ADR-001: Choice of PyQt6 for GUI

## Status
âœ… **ACCEPTED** - 2024-01-15

## Context
The project needs a graphical user interface (GUI) to manage tasks in addition to the existing CLI.

**Requirements**:
- Cross-platform (Linux, Windows, macOS)
- Integration with existing Python code
- Ability to create complex layouts (docks, tabs, menus)
- Licensing compatible with GPL
- Active community and documentation

**Alternatives Considered**:
1. **Tkinter** (native Python)
2. **PyQt6** (Qt bindings)
3. **wxPython** (wxWidgets bindings)
4. **Kivy** (mobile-first)

## Decision
We chose **PyQt6** for the GUI implementation.

## Consequences

### Positive âœ…
- **Advanced Layout**: QDockWidget, QMainWindow allow professional layout
- **Rich Widgets**: QTreeWidget, QTableWidget already implemented and robust
- **Styling**: QSS (CSS-like) allows visual customization
- **Documentation**: Excellent official documentation + large community
- **Performance**: Native C++, faster than Tkinter
- **Cross-platform**: Works well on Linux, Windows, macOS

### Negative âŒ
- **License**: GPL or commercial (~$450/dev) - we chose GPL
- **Size**: Larger binary (~50MB) vs Tkinter (~5MB)
- **Learning Curve**: More complex than Tkinter
- **External Dependency**: Requires `pip install PyQt6`

### Risks ðŸš¨
- **GPL License**: Project must be open-source (OK for us)
- **Breaking Changes**: Qt6 is recent, there may be changes
- **Packaging**: PyInstaller needs special configuration for PyQt6

### Discarded Alternatives
- **Tkinter**: Primitive layout, no native dock widgets
- **wxPython**: Inferior documentation, smaller community
- **Kivy**: Mobile-focused, non-native desktop style

## Implementation
- Refactor existing code to separate presentation logic
- Create `ProjectGUI` class with QMainWindow
- Maintain CLI compatibility for existing users
- Document PyQt6 installation in README

## References
- [PyQt6 Documentation](https://www.riverbankcomputing.com/static/Docs/PyQt6/)
- [Qt6 Documentation](https://doc.qt.io/qt-6/)
- Task Example: "Add GUI with docking support"

## Notes
If in the future we need a more permissive license (MIT/Apache), consider:
- Migrating to PySide6 (Qt LGPL binding)
- Rewriting with Tkinter + ttkbootstrap
- Using Dear PyGui (MIT, but OpenGL, not native)

---
**Author**: JosuÃ©
**Date**: 2024-01-15
**Last Update**: 2024-01-15
```

**Directory Structure for ADRs**:

```
myproject/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ adr/
â”‚   â”‚   â”œâ”€â”€ 001-choice-of-pyqt6.md
â”‚   â”‚   â”œâ”€â”€ 002-data-storage-format.md
â”‚   â”‚   â”œâ”€â”€ 003-simplicity-protocol-versioning.md
â”‚   â”‚   â””â”€â”€ README.md  (ADR Index)
â”‚   â”œâ”€â”€ PROTOCOLO_SIMPLICIDADE_1.md
â”‚   â””â”€â”€ PROTOCOLO_SIMPLICIDADE_2.md
â”œâ”€â”€ project_app.py
â””â”€â”€ README.md
```

**ADR Index** (`docs/adr/README.md`):

```markdown
# Architecture Decision Records

## Active Decisions
- [ADR-001](001-choice-of-pyqt6.md): Choice of PyQt6 for GUI âœ… ACCEPTED
- [ADR-003](003-simplicity-protocol-versioning.md): Simplicity Protocol Versioning âœ… ACCEPTED

## Superseded Decisions
- [ADR-002](002-data-storage-format.md): DATA as storage format âš ï¸ SUPERSEDED
  - Superseded by SQLite in ADR-004 (2024-02-01)

## Rejected Decisions
- (none)

## Proposed (Pending Discussion)
- ADR-005: Implement plugin support
- ADR-006: Migrate from DATA to SQLite

---

## Template
New ADRs should follow the template in `adr-template.md`

## Numbering
ADRs are numbered sequentially: 001, 002, 003, etc.
```

**Example of Superseded ADR**:

```markdown
# ADR-002: DATA as Storage Format

## Status
âš ï¸ **SUPERSEDED** by [ADR-004](004-migrate-to-sqlite.md) on 2024-02-01

## Context
(original context...)

## Decision
Use DATA for task persistence.

## Consequences

### Why was it superseded?
DATA worked well for up to ~500 tasks, but performance degraded significantly:
- Read time: 2.5s for 1000 tasks (unacceptable)
- Concurrency: Does not support multiple simultaneous windows
- Queries: Difficult to filter/search without loading everything

**Solution**: Migrate to SQLite (ADR-004) while keeping DATA as an optional export.

---
**Author**: JosuÃ©
**Original Date**: 2023-11-10
**Superseded**: 2024-02-01
```

**Tools for ADRs**:

```bash
# adr-tools - CLI to create ADRs
npm install -g adr-log

# Create new ADR
adr new "Implement caching layer"

# List all ADRs
adr list

# Supersede old ADR
adr new -s 2 "Migrate from DATA to SQLite"
# Creates new ADR and marks #2 as superseded

# Generate visualization
adr generate graph > adr-graph.svg
```

**When to Create ADR**:

```python
# âœ… DESERVES ADR - Impactful decision
"""
We decided to use SQLite instead of DATA.
Impact:
- Changes data persistence (migration needed)
- Affects performance (10x faster)
- Adds dependency (sqlite3 - native)
- Data access code needs refactoring
"""

# âŒ DOES NOT DESERVE ADR - Trivial decision
"""
We decided to rename variable 'x' to 'task_count'.
Impact: Code clarity only.
"""

# âœ… DESERVES ADR - Significant trade-off
"""
We decided NOT to implement task encryption.
Rationale:
- High complexity (key management)
- Low benefit (tasks are not sensitive)
- Can be added later if needed
"""
```

**ADR Checklist**:

```markdown
### ADR Checklist - ADR-XXX: [Title]

#### Complete Content
- [ ] **Status**: Proposed/Accepted/Rejected/Superseded defined
- [ ] **Context**: Problem clearly described
- [ ] **Alternatives**: At least 2 alternatives considered
- [ ] **Decision**: Choice explicitly stated
- [ ] **Consequences**: Positive AND negative documented
- [ ] **Risks**: Risks identified and proposed mitigations

#### Quality
- [ ] **Justification**: "Why" is clear (not just "what")
- [ ] **Trade-offs**: Trade-offs explicitly stated
- [ ] **Reversibility**: Cost of reversing documented
- [ ] **References**: Links to relevant docs/issues/PRs

#### Process
- [ ] **Numbering**: ADR numbered sequentially
- [ ] **Index**: README.md updated with new ADR
- [ ] **Review**: ADR reviewed by at least 1 person
- [ ] **Commit**: Committed along with related code
```

**ADR in Pull Request**:

```markdown
## PR #145: Implement SQLite storage

### Description
Migrates persistence from DATA to SQLite storage (Task Example).

### Architecture Decision
This PR implements **ADR-004: Migrate to SQLite**.

**Trade-offs**:
- âœ… 10x faster (2.5s â†’ 0.2s for 1000 tasks)
- âœ… Supports concurrency (multiple windows)
- âŒ Migration required for existing users
- âŒ More complex persistence code

**Mitigated Risks**:
- Automatic migration on first use (v2.0.0)
- Automatic DATA backup before migration
- Rollback available if migration fails

### See ADR
- [ADR-004: Migrate to SQLite](docs/adr/004-migrate-to-sqlite.md)
- Supersedes ADR-002 (DATA storage)

### Checklist
- [x] ADR created and committed
- [x] Migration code implemented
- [x] Migration tests added
- [x] Documentation updated
```

**Why ADRs are valuable**:
- âœ… **Context**: Future devs understand "why" decisions were made
- âœ… **Avoids Rework**: Don't reopen already resolved discussions
- âœ… **Onboarding**: New members learn architecture quickly
- âœ… **Learning**: Team learns from past decisions (good and bad)
- âœ… **Auditing**: Stakeholders see transparent decision process

---

### 12. **Fill in New Documentation**
- **Update tasks/requirements file**: Mark tasks as `[X]` complete
- **Create SPECIFICATIONS.md**: Detailed version document
- **Update statistics**: Project completion percentage
- **ðŸ¤– [OPTIONAL] Manage AI task recommendations**

---

### âš ï¸ **MANDATORY REQUIREMENT: Complete Documentation of All AI Implementations**

> **CRITICAL FOR AIs**: Everything that the artificial intelligence does in the project, in each implementation cycle, in each code, each implemented functionality, **MUST BE DOCUMENTED IN THE `docs/` FOLDER AS A MANDATORY REQUIREMENT** to mark new functionalities and new behaviors.

**ðŸ“– See SIMPLICITY_PROTOCOL_1.md. - Step 12** for complete documentation requirements, templates, and validation checklists.

#### **ðŸ¢ Enterprise-Specific Documentation (Simplicity 2)**

In addition to base documentation requirements, Simplicity 2 adds:

**Additional Documentation for Enterprise**:
- âœ… **ADRs** (Architecture Decision Records) - Formal documentation in `docs/ADR/`
- âœ… **OWASP Security Checklist** - Complete and documented in `docs/SECURITY.md`
- âœ… **WCAG Accessibility Checklist** - For GUI applications in `docs/ACCESSIBILITY.md`
- âœ… **API Documentation** - Generated with Sphinx/pdoc in `docs/API/`
- âœ… **Performance Profiling Results** - For critical features
- âœ… **Code Review Records** - Approvals and feedback documented

**ðŸ“‚ Enterprise Documentation Structure**:

```
docs/
â”œâ”€â”€ REQUIREMENTS.md
â”œâ”€â”€ vX.Y.Z-SPECIFICATIONS.md
â”œâ”€â”€ CHANGELOG.md
â”œâ”€â”€ ARCHITECTURE.md
â”œâ”€â”€ ADR/                     # Architecture Decision Records
â”‚   â”œâ”€â”€ ADR-001-[decision].md
â”‚   â””â”€â”€ ADR-002-[decision].md
â”œâ”€â”€ SECURITY.md              # OWASP checklist and mitigations
â”œâ”€â”€ ACCESSIBILITY.md         # WCAG compliance (if GUI)
â”œâ”€â”€ API/                     # API documentation
â”‚   â””â”€â”€ api-reference.html   # Generated by Sphinx/pdoc
â”œâ”€â”€ CODE_REVIEWS/            # Code review records
â”‚   â””â”€â”€ review-vX.Y.Z.md
â””â”€â”€ [feature]-GUIDE.md
```

**ðŸ” Additional Validation for Enterprise**:

Before commit, AI must also verify:
- [ ] âœ… ADRs created for important architectural decisions
- [ ] âœ… OWASP security checklist complete in SECURITY.md
- [ ] âœ… Profiling results documented (if critical feature)
- [ ] âœ… API documentation generated (if public library)
- [ ] âœ… Code review approved and documented
- [ ] âœ… WCAG checklist complete (if GUI application)

**Rationale for Enterprise**: In large teams and regulated environments, comprehensive documentation is essential for compliance, auditing, team collaboration, and organizational knowledge preservation.

---

**ðŸ“‹ TASKS.md Management**:

**General Rule**:
- If a tasks/requirements file exists (e.g., `TASKS.md`, `TODO.md`, `requirements.md`):
  - âœ… **Mark tasks as complete** after implementation: `[ ]` â†’ `[X]`
  - âœ… **Update statistics** (percentages, counters)
  - âœ… **Add completion notes** (date, version, brief description)
  - ðŸ¤– **[OPTIONAL] Add new AI-recommended tasks** (see details in SIMPLICITY_PROTOCOL_1.md - Step 12)
  
- If a tasks/requirements file **DOES NOT exist**:
  - â“ **Ask the user** for the file location/path
  - â“ **Ask about next tasks and requirements** if no formal document
  - â“ **Suggest creating** `TASKS.md` as the default file

---

### ðŸ“Š **Task Classification Legend (Simplicity 2)**

**Objective**: Standardize task classification and prioritization to facilitate AI organization, team communication, and understanding between different artificial intelligence systems.

**Note for Simplicity 2**: In enterprise environments with large teams, task classification should be **integrated with the Decision Matrix (Step 2.5)** to ensure objective and traceable choices.

#### **Task Status**

- ðŸ”´ **Not Started** - Awaiting start, no work done
- ðŸŸ¡ **In Progress** - Active development, work underway
- ðŸŸ¢ **Done** - Implemented, tested, peer-reviewed and completed
- ðŸ”µ **Blocked** - Impeded by external dependency or technical issue

#### **Task Complexity**

- ðŸŸ¢ **Simple** (0-1h) - Low risk, few dependencies, clear scope
- ðŸŸ¡ **Medium** (1-2h) - Medium risk, some integrations, may require additional tests
- ðŸ”´ **Complex** (>2h) - High risk, many dependencies, open or ambiguous scope

#### **MoSCoW Prioritization**

- ðŸ”´ **Must Have** - Critical for system functionality, release blocker
- ðŸŸ¡ **Should Have** - Important but not blocking, can be postponed if needed
- ðŸŸ¢ **Could Have** - Desirable if time permits, low priority
- âšª **Won't Have** (Later) - Explicitly out of current scope, for future versions

#### **Integration with Decision Matrix (Simplicity 2)**

The Decision Matrix (Step 2.5) provides numerical scoring (0-35 points) complementary to visual indicators:

```markdown
## Sprint v3.2 - Prioritized Backlog

### ðŸ”´ MUST HAVE

| Task | Status | Complex. | Score | Order |
|------|--------|----------|-------|-------|
| #42 2FA Auth | ðŸ”´ | ðŸ”´ | 25.0 | 3rd |
| #43 Rate Limiting | ðŸ”´ | ðŸŸ¡ | 28.5 | 2nd |
| #44 Logging | ðŸ”´ | ðŸŸ¢ | 33.5 | 1st â­ START HERE |

**Justification**: Task #44 has highest score (33.5) despite being Must Have like others.
Starting with it reduces risks and allows team to warm up before complex tasks.
```

**Combining Decision Matrix + Visual Classification**:
1. Use **Decision Matrix** for objective scoring (5 numerical criteria)
2. Use **Visual Indicators** (ðŸ”´ðŸŸ¡ðŸŸ¢ðŸ”µ) for quick status in backlog
3. Use **MoSCoW** to define release scope
4. Use **Complexity** to balance sprints (not only difficult tasks)

#### **Advanced Prioritization Frameworks**

For enterprise teams that need to justify decisions to stakeholders:

##### **RICE Matrix** (Quantitative)

`RICE Score = (Reach Ã— Impact Ã— Confidence) / Effort`

Useful for:
- âœ… Product management decisions with multiple competing features
- âœ… Presentations to C-level (objective data)
- âœ… Long-term roadmap planning

**Enterprise Example**:
```markdown
| Feature | Reach | Impact | Conf. | Effort | RICE | Decision |
|---------|-------|--------|-------|--------|------|----------|
| SSO Integration | 5000 | 3 | 80% | 80h | 150 | Q1 2024 |
| Dashboard v2 | 2000 | 2 | 100% | 40h | 100 | Q2 2024 |
| Dark Mode | 8000 | 0.5 | 100% | 20h | 200 | Q1 2024 â­ |

Decision: Prioritize Dark Mode (RICE=200) over SSO (RICE=150)
Reason: Greater reach with less effort, despite lower individual impact
```

##### **Eisenhower Matrix** (Urgency Ã— Importance)

Useful for:
- âœ… Incident and crisis management
- âœ… Prioritization in contexts with many false "urgencies"
- âœ… Identifying tasks to delegate or automate

**Team Adaptation**:
- **Q1 (Urgent + Important)**: Senior team / Tech leads
- **Q2 (Not Urgent + Important)**: Mid-level team, planned
- **Q3 (Urgent + Not Important)**: Delegate to junior or automate
- **Q4 (Not Urgent + Not Important)**: Eliminate or distant backlog

#### **Complete Simplicity 2 Example**

```markdown
# TASKS.md - Sprint v4.1 (Enterprise Team)

## ðŸ“Š Legend
- **Status**: ðŸ”´ Not Started | ðŸŸ¡ In Progress | ðŸŸ¢ Done | ðŸ”µ Blocked
- **Complexity**: ðŸŸ¢ Simple (0-1h) | ðŸŸ¡ Medium (1-2h) | ðŸ”´ Complex (>2h)
- **MoSCoW**: ðŸ”´ Must | ðŸŸ¡ Should | ðŸŸ¢ Could | âšª Won't

## ðŸ“Š Statistics
- Progress: 65% (26/40 tasks)
- Velocity: 12 story points/sprint
- Open Bugs: 3 (1 critical, 2 medium)

## ðŸ”´ MUST HAVE - Release v4.1

### High Priority (Matrix Score > 25)
- ðŸ”´ðŸŸ¢ [ ] #101 Add rate limiting (Score: 33.5) â­ START
  - **Assignee**: @maria (Backend Lead)
  - **Review**: @joao (Security Review required)
  - **Estimate**: 3h
  - **Dependencies**: None
  
- ðŸŸ¡ðŸŸ¡ [ ] #102 Implement circuit breaker (Score: 28.0, 60% complete)
  - **Assignee**: @pedro (Mid-level)
  - **Review**: @maria (Code Review)
  - **Estimate**: 5h (2h remaining)
  - **Blocker Resolved**: âœ… Library updated to v3.2

### Medium Priority (Matrix Score 15-25)
- ðŸ”µðŸ”´ [ ] #103 Migrate to Kubernetes (Score: 22.0, BLOCKED)
  - **Assignee**: @infra-team
  - **Blocker**: Awaiting DevOps budget approval
  - **Estimate**: 16h
  - **Fallback**: Keep Docker Swarm for 1 more sprint

## ðŸŸ¡ SHOULD HAVE - Release v4.2
- ðŸ”´ðŸŸ¡ [ ] #104 Add Prometheus metrics (Score: 26.5)
- ðŸ”´ðŸŸ¢ [ ] #105 Help tooltips (Score: 30.0)

## ðŸŸ¢ COULD HAVE - Backlog
- ðŸ”´ðŸŸ¡ [ ] #106 Dark mode (RICE: 200, high backlog priority)

---
**Next Retrospective**: Friday 3pm (validate AI recommendations)
```

#### **Recommendations for AI in Enterprise Context**

**When classifying tasks for teams (Simplicity 2), AI should**:
1. âœ… **Consider Code Review**: Complex tasks need available senior reviewer
2. âœ… **Balance workload**: Don't allocate all complex tasks to same person
3. âœ… **Respect team dependencies**: Backend before Frontend in integrations
4. âœ… **Document decisions**: Use ADR (Step 11.5) for important architectural choices
5. âœ… **Communicate blockers**: Mark ðŸ”µ and notify team immediately
6. âœ… **Integrate with Decision Matrix**: Scoring + visual indicators complementary
7. âœ… **Validate with stakeholders**: MUST HAVE features confirmed in Sprint Planning

**Simplicity 2 vs 1 Differences**:
- **S2**: Decision Matrix (numerical scoring) is **MANDATORY** when 3+ tasks compete
- **S2**: Status should reflect **code review** (don't mark Done without peer approval)
- **S2**: AI recommendations validated in **Sprint Retrospective** (Step 13.5)
- **S2**: Complexity includes **review time** and **acceptance testing**

---

**ðŸ¤– AI Task Recommendations**:
For enterprise teams (Simplicity 2), AI recommendations should be **reviewed in sprint retrospectives** (Step 13.5) before being added to TASKS.md. This ensures team consensus and alignment with stakeholders.

ðŸ“˜ **Complete details of recommendation functionality**: See `SIMPLICITY_PROTOCOL_1.md` - Step 12 - Section "AI Task Recommendations"

**ðŸ“ TASKS.md File Location**:
- **Default preference**: The `TASKS.md` file, when created, should be placed in `docs/TASKS.md`
- **Create docs/ folder**: If the `docs/` folder does not exist in the project, it should be created automatically
- **Flexibility**: The user or programmer can choose to place it in another location if preferred
- **Creation example**:
  ```bash
  # Create docs folder if it doesn't exist
  mkdir -p docs
  
  # Create or update TASKS.md
  echo "# Tasks" > docs/TASKS.md
  ```

**Example of Marking (REQUIREMENTS.md)**:
```markdown
## ðŸŸ¢ COULD HAVE (Low Priority)

### âœ… Completed Tasks

#### Task Example - Integrated File Editor (vX.Y.Z)
**Status**: âœ… Complete - 30/11/2025

**Objective**: Implement an integrated text editor with scope differentiation by colors.

**Implementation**:
1. âœ… ComponentE with QTextEdit and syntax highlighting
2. âœ… Scope differentiation by colors (HTML tags, DATA keys, etc.)
3. âœ… Open/save files (.txt, .data, .html, .tsx, .py)
4. âœ… Integration with File menu â†’ Open Editor

**Files Created**:
- `src/gui/editor_dock.py` (500+ lines)
- `tests/test_editor_dock.py` (15 tests)

### ðŸ”¨ Pending Tasks
- **[]** Next unimplement task...
```

**Minimum Recommended Structure**:
```markdown
# Project - Tasks

## Categories
- MUST HAVE: [X/Y complete] (Z%)
- SHOULD HAVE: [X/Y complete] (Z%)
- COULD HAVE: [X/Y complete] (Z%)
- WOULD HAVE: [X/Y complete] (Z%)

## Statistics
- **TOTAL**: [X/Y complete] (Z%)
```

**Version Documentation Structure**:
```markdown
# MyProject v2.9.X - [Descriptive Name]

**Date**: DD/MM/AAAA
**Sprint**: X tasks in Y hours
**Methodology**: Simplicity Protocol 1

## ðŸ“‹ Sprint Objectives
- Task #X: [description]
- Task #Y: [description]

## ðŸŽ¯ Implemented Tasks
### Task #X: [Name]
- **Problem**: [description of original problem]
- **Solution**: [how it was solved]
- **Modified Files**: [list]
- **Tests**: [quantity and status]

## âœ… Quality (Simplicity Protocol 1)
- âœ… Modular Architecture
- âœ… Type Hints (100%)
- âœ… Complete Docstrings
- âœ… Error Handling
- âœ… Tests (X passing)
- âœ… Semantic Commits
- âœ… Complete Documentation
- âœ… Clean Code (PEP8)

## ðŸ“Š Statistics
- TOTAL: X% complete (Y/Z tasks)
- Commits: N pushed
```

---

### 12.5 **Rollback Plans** (Optional - For Critical Features)

**When to Apply**:
- âœ… Critical features in production
- âœ… Data schema changes/migrations
- âœ… Changes to public APIs
- âœ… Deploying high-risk features
- âœ… When downtime is unacceptable

**Do Not Apply If**:
- âŒ Experimental/beta feature (flag controlled)
- âŒ Internal change with no user impact
- âŒ Prototype or dev/staging environment only
- âŒ Trivial hotfix (typo, css)

**What is a Rollback Plan?**

A documented plan to **revert** a change if something goes wrong in production. Unlike "undoing a commit," rollback considers:
- Data state (migrations, schemas)
- External dependencies (APIs, services)
- Active users (downtime, data in transit)

**Rollback Plan Template**:

```markdown
# Rollback Plan - Task Example: SQLite Migration

## Change Summary
**Feature**: Migration from DATA to SQLite storage
**Version**: v2.0.0 â†’ v1.9.x
**Impact**: HIGH - Alters persistence format
**Risk**: MEDIUM - Data migration may fail

## Criteria for Rollback
Execute rollback IF:
- [ ] Error rate > 5% within 1 hour after deploy
- [ ] Users report data loss (tasks disappearing)
- [ ] Performance worse than previous version (> 2x slower)
- [ ] Frequent crashes (> 10 reports in 24h)
- [ ] Automatic migration fails for > 10% users

DO NOT execute rollback IF:
- âœ… Only 1-2 users report problems (investigate first)
- âœ… Minor bug that can be hotfixed quickly
- âœ… Acceptable performance (< 1s), even if not ideal

## Step-by-Step Rollback

### Phase 1: Preparation (5 minutes)
1. **Notify users**:
   ```bash
   # Create maintenance banner
   echo "âš ï¸ Maintenance in progress - Rolling back to v1.9.5" > maintenance.txt
   ```

2. **Backup current state**:
   ```bash
   # Backup current SQLite database
   cp ~/.config/myproject/myproject.db ~/.config/myproject/myproject.db.backup-$(date +%s)
   
   # Backup logs
   cp ~/.config/myproject/myproject.log /tmp/myproject-rollback-logs.txt
   ```

3. **Verify DATA backup available**:
   ```bash
   # Confirm DATA backup exists (created during migration)
   ls -lh ~/.config/myproject/tasks.data.backup
   # Should show file created during migration to v2.0.0
   ```

### Phase 2: Rollback (10 minutes)
1. **Revert code to previous version**:
   ```bash
   cd ~/myproject
   git checkout v1.9.5  # Tag of previous stable version
   
   # OR if in production via package manager
   pip install myproject==1.9.5 --force-reinstall
   ```

2. **Restore data from DATA backup**:
   ```bash
   # Copy DATA backup back
   cp ~/.config/myproject/tasks.data.backup ~/.config/myproject/tasks.data
   
   # Remove SQLite database (v1.9.5 does not use)
   rm ~/.config/myproject/myproject.db
   ```

3. **Verify data integrity**:
   ```bash
   # Validate DATA is not corrupted
   python -c "import data; data.load(open('~/.config/myproject/tasks.data'))"
   # Should complete without error
   
   # Count tasks
   python -c "import data; data = data.load(open('~/.config/myproject/tasks.data')); print(f'{len(data[\"tasks\"])} tasks restored')"
   ```

4. **Restart application**:
   ```bash
   # If process running, kill
   kill <myproject_pid>
   
   # Start v1.9.5
   python myproject.py
   ```

### Phase 3: Validation (5 minutes)
1. **Smoke Tests**:
   ```bash
   # Test 1: App starts without crash
   myproject --version
   # Expected: v1.9.5
   
   # Test 2: List tasks
   myproject list
   # Expected: Tasks displayed correctly
   
   # Test 3: Add task
   myproject add "Test rollback task"
   # Expected: Task added without error
   
   # Test 4: GUI opens (if applicable)
   myproject --gui &
   # Expected: GUI opens without crash
   ```

2. **Check logs**:
   ```bash
   tail -n 50 ~/.config/myproject/myproject.log
   # Verify absence of SQLite errors
   ```

3. **Contact affected users**:
   - Ask 3-5 users to test basic functionality
   - Confirm their data is intact

### Phase 4: Communication (Immediate)
1. **Notify stakeholders**:
   ```markdown
   ## Rollback Executed - v2.0.0 â†’ v1.9.5
   
   **Timestamp**: 2024-01-20 15:30 UTC
   **Reason**: Error rate 12% in SQLite migration (criterion: >5%)
   **Status**: âœ… Rollback complete, system stable
   **Impact**: v2.0.0 users must reinstall v1.9.5
   
   **User Action**:
   ```bash
   pip install myproject==1.9.5 --force-reinstall
   ```
   
   Data preserved via automatic DATA backup.
   
   **Next Steps**:
   - Root cause analysis of migration failure
   - Fix planned for v2.0.1 (ETA: 2024-01-25)
   - Expanded beta testing before release
   ```

2. **Create post-mortem issue**:
   ```markdown
   # Post-Mortem: Rollback v2.0.0 â†’ v1.9.5
   
   ## Timeline
   - 14:00 UTC: Deploy v2.0.0
   - 14:30 UTC: First reports of migration failure
   - 15:00 UTC: Error rate reaches 12% (rollback criterion: >5%)
   - 15:15 UTC: Rollback decision made
   - 15:30 UTC: Rollback complete
   
   ## Root Cause
   - SQLite migration failed for DATA files > 5MB
   - Cause: 30s timeout insufficient for complex tasks
   - Affected ~12% of users (heavy users with >500 tasks)
   
   ## Lessons Learned
   - âœ… Rollback plan worked perfectly
   - âœ… Automatic backup saved data
   - âŒ Testing did not cover heavy users (>500 tasks)
   - âŒ Migration timeout too short
   
   ## Action Items
   - [ ] Increase migration timeout to 5min (#145)
   - [ ] Add progress bar for long migrations (#146)
   - [ ] Create test suite with large datasets (#147)
   - [ ] Beta program with heavy users before release (#148)
   ```

## Estimated Rollback Time
- **Preparation**: 5 minutes
- **Execution**: 10 minutes
- **Validation**: 5 minutes
- **TOTAL**: ~20 minutes (expected downtime)

## External Dependencies
- âœ… DATA backup automatically created during migration
- âœ… Git tags of previous versions available
- âŒ Does not depend on external services (DB, APIs)

## Data at Risk
- **High Risk**: Tasks created/edited after v2.0.0 deploy (do not exist in backup)
- **Low Risk**: Tasks existing before v2.0.0 (preserved in backup)

**Mitigation**: Export SQLite â†’ DATA before rollback to preserve recent changes.

```bash
# Export script before rollback
python -c "
import sqlite3, data
conn = sqlite3.connect('~/.config/myproject/myproject.db')
cursor = conn.execute('SELECT * FROM tasks')
tasks = [dict(zip([col[0] for col in cursor.description], row)) for row in cursor.fetchall()]
data.dump({'tasks': tasks}, open('rollback-export.data', 'w'), indent=2)
"
# Users can manually merge changes later
```

## Contact Persons
- **Rollback Decision**: @lead-dev (JosuÃ©)
- **Technical Execution**: @dev-team
- **User Communication**: @support-team

---
**Created**: 2024-01-15
**Last Update**: 2024-01-15
**Tested**: âŒ No (run dry-run before deploy)
```

**Feature Flags - Alternative to Rollback**:

```python
# Instead of full rollback, use a feature flag to disable feature

class Config:
    """Configuration with feature flags."""
    
    # Feature flag - remote control
    SQLITE_STORAGE_ENABLED = os.getenv("MYPROJECT_SQLITE_ENABLED", "true").lower() == "true"
    
    def get_storage_backend(self):
        """Get storage backend based on feature flag."""
        if self.SQLITE_STORAGE_ENABLED:
            return SQLiteStorage()
        else:
            return DATAStorage()  # Safe fallback

# In case of problem, disable remotely:
# export MYPROJECT_SQLITE_ENABLED=false
# Or via config file / admin dashboard

# Users automatically revert to DATA without reinstalling
```

**Reversible Migrations**:

```python
# Migrations must be reversible

class MigrationV2:
    """Migration from DATA to SQLite - REVERSIBLE."""
    
    def up(self):
        """Migrate DATA â†’ SQLite."""
        # 1. Create DATA backup
        shutil.copy("tasks.data", "tasks.data.backup")
        
        # 2. Create SQLite schema
        self._create_sqlite_schema()
        
        # 3. Migrate data
        self._migrate_data_to_sqlite()
        
        # 4. DO NOT delete DATA (keep for rollback)
        # os.remove("tasks.data")  âŒ NEVER do this
    
    def down(self):
        """Rollback SQLite â†’ DATA."""
        if not os.path.exists("tasks.data.backup"):
            raise RollbackError("Backup DATA not found - cannot rollback!")
        
        # 1. Restore backup
        shutil.copy("tasks.data.backup", "tasks.data")
        
        # 2. Remove SQLite
        os.remove("myproject.db")
        
        print("âœ… Rollback complete - using DATA storage")
```

**Rollback Plan Checklist**:

```markdown
### Rollback Plan Checklist - Task #XX

#### Planning
- [ ] **Criteria**: Clear criteria for when to execute rollback
- [ ] **Steps**: Detailed step-by-step documented
- [ ] **Time**: Estimated rollback time calculated
- [ ] **Dependencies**: External dependencies identified
- [ ] **Data**: Data loss risk assessed

#### Preparation
- [ ] **Backup**: Automated backup mechanism implemented
- [ ] **Tags**: Git tags of stable versions created
- [ ] **Scripts**: Rollback scripts tested in staging
- [ ] **Contacts**: Contact persons defined

#### Validation
- [ ] **Dry-run**: Rollback tested in staging environment
- [ ] **Smoke Tests**: Smoke tests defined for post-rollback validation
- [ ] **Communication**: Communication template prepared
- [ ] **Post-mortem**: Post-mortem template created
```

**Why Rollback Plans are critical**:
- âœ… **Confidence**: Team can make bold deploys knowing they can revert
- âœ… **Downtime**: Minimizes downtime (20min vs hours debugging)
- âœ… **Data**: Protects user data (backup strategy)
- âœ… **Communication**: Prepared template = fast and clear communication
- âœ… **Learning**: Structured post-mortem generates learning

---

### 13. **Commit and Push**
- **Format**: Conventional Commits (MANDATORY)
- **Language**: All commit messages must be **EXCLUSIVELY IN ENGLISH** (mandatory requirement)
- **Message**: Descriptive, complete, with context
- **Frequency**: 1 commit per task or logical group of changes

**Standardized Commit Types** (MANDATORY):
- `feat`: Indicates a new feature
  - Example: `git commit -m "feat: add Header component"`
- `fix`: Indicates a bug fix
  - Example: `git commit -m "fix: remove wrong prop in Header"`
- `refactor`: Indicates code refactoring
  - Example: `git commit -m "refactor: add title in Header"`
- `test`: Indicates test changes
  - Example: `git commit -m "test: add test in title Header"`
- `style`: Indicates style/formatting changes
  - Example: `git commit -m "style: add Header title background"`
- `docs`: Indicates documentation changes
  - Example: `git commit -m "docs: add get started in readme"`
- `chore`: Indicates development environment changes
  - Example: `git commit -m "chore: change eslint rules"`
- `build`: Indicates dependency changes
  - Example: `git commit -m "build: add sass"`
- `revert`: Indicates reversion of a previous commit
  - Example: `git commit -m "revert: back to adc1234 commit"`

âš ï¸ **IMPORTANT**: All commit messages must be written **EXCLUSIVELY IN ENGLISH**!

**Commit Message Structure**:
```
<type>: <short description> (<version>)

<ORIGINAL PROBLEM>:
- [Problem context]
- [Why it was necessary to solve]

<IMPLEMENTED SOLUTION>:
âœ… [Feature/function 1]
   - [Technical detail]
âœ… [Feature/function 2]
   - [Technical detail]

âœ… [TESTS]:
   - [Quantity] unit tests ([status])
   - [Tested categories]

<MODIFIED FILES>:
- [file1.py] (+X lines)
- [file2.py] (~Y lines)
- [tests/test_X.py] (NEW - Z lines)
- [docs/REQUIREMENTS.md] (updated statistics)

<UPDATED STATISTICS>:
- [CATEGORY]: X â†’ Y complete (A% â†’ B%)
- TOTAL: X â†’ Y complete (A% â†’ B%)

<USAGE EXAMPLE>: (if applicable)
  [Practical demonstration]

Refs: [related documentation]
Closes: Task #X (vX.X.X)
```

**Real Example** (Task Example):
```bash
git add src/ tests/ docs/REQUIREMENTS.md
git commit -m "feat: complete Task Example - Feature Update System (vX.Y.Z)

ORIGINAL PROBLEM:
- vX.Y.Z implementation used string_similarity() (INCORRECT)
- Did not detect duplicate values, only name similarity
...

âœ… IMPLEMENTED SOLUTION:
âœ… extract_all_keys_from_obj()
   - Supports Obj AND dict type
   - Returns Dict[str, str] (path â†’ value)
...

Closes: Task Example (vX.Y.Z)"

git push
```

---

## ðŸ† Professional Quality Criteria

Every implementation must meet **100% of these criteria**:

| # | Criterion | Description | Validation |
|---|---|---|---|
| 1 | **Modular Architecture** | Each feature in a separate module | Own file in `src/` |
| 2 | **Type Hints** | 100% of parameters typed | `def func(x: int) -> str:` |
| 3 | **Docstrings** | All public functions documented | Args, Returns, Examples |
| 4 | **Error Handling** | Try/except with clear messages | `except Exception as e:` |
| 5 | **Tests** | Unit + integration (100% coverage) | `tests/test_*.py` passing |
| 6 | **Semantic Commits** | Conventional Commits | `feat:`, `fix:`, `docs:` |
| 7 | **Documentation** | REQUIREMENTS.md + SPECIFICATIONS.md | Updated and complete |
| 8 | **Clean Code** | PEP8, semantic names, DRY | Functions < 50 lines |

---

## ðŸ“Š Practical Application: Task Example (Complete Example)

### Initial Situation
```markdown
Pending tasks in the SHOULD HAVE category:
[ ] Complex Feature Example (VERY COMPLEX)
[ ] Semantic AI Search (VERY COMPLEX)
[âš ï¸] Feature Update (PARTIAL - simpler!) âœ… CHOSEN
[ ] Google Translate API integration (COMPLEX)
```

### Planned Sprint
```
vX.Y.Z: Complete Task Example
Estimate: 3-4 hours
Complexity: MEDIUM (simpler than the others)
```

### Execution (Simplicity Protocol 1)

**1. Read Documentation** âœ…
- Read: `docs/FEATURE_SPEC.md` (662 lines)
- Understood: string similarity vs. value equality problem

**2. Choose Simple Task** âœ…
- Task Example is **simpler** than text editor or AI
- Clear scope: 2 main functions + integration

**3. Ask Questions** âœ…
- Asked: "How many words to get? 3-5?"
- Answer: "Default 30 characters"
- Asked: "Convert to camelCase?"
- Answer: "Yes, remove accents"
- Asked: "Name conflicts?"
- Answer: "Smaller line wins, don't touch if values are different"

**4. Sprint** âœ…
- 6 subtasks planned (including questions)
- Estimated time: 3h45min

**5. Implement with Architecture** âœ…
```
Order executed:
1. extract_all_keys_from_obj() (helper function - High Cohesion)
2. build_substitution_map_by_value() (main function - Low Coupling)
3. Update cli_dedupe() (integration - Dependency Injection)
4. Create tests (validation)
5. Documentation (finalization)

Applied Patterns:
- âœ… Separate modules (Reusability)
- âœ… Type hints in all functions
- âœ… Information Expert (GRASP): each function has the info it needs
- âœ… Low coupling: independent functions
- âœ… High cohesion: each function does ONE thing
```

**6. Run Tests** âœ…
```
12 unit tests created:
- 4 tests for extract_all_keys_from_obj()
- 5 tests for build_substitution_map_by_value()
- 2 tests for apply_substitutions_to_file()
- 1 test for update_references_in_project()
Result: 12/12 passing (100%)
```

**7. Documentation** âœ…
```
Files created/updated:
- docs/REQUIREMENTS.md (Task Example marked [X])
- docs/FEATURE_SPEC.md (already existed)
- tests/test_reference_updater.py (NEW - 350 lines)
Statistics: 59.6% â†’ 60.6% (63 tasks complete)
```

**8. Commit and Push** âœ…
```bash
Commit: 903bca4
Message: 60 lines (complete and detailed)
Status: pushed to GitHub âœ…
```

### Final Result
âœ… **Task Example 100% complete**
âœ… **Simplicity Protocol 1: 10/10 steps completed** (v1.1 - 10 steps)
âœ… **Real time: ~3h (within estimate)**
âœ… **Zero bugs detected**
âœ… **Professional documentation**

**Note**: This example uses v1.1 of the protocol (10 steps). v1.2 adds 2 more steps (GUI and CLI integration).

---

## ðŸŽ“ Lessons Learned

### âœ… What Works
1. **Choose the simplest**: Task Example was easier than text editor
2. **Incrementality**: Helper function â†’ main â†’ integration
3. **Tests first**: Detected 2 necessary adjustments before committing
4. **Complete documentation**: Facilitates future maintenance

### âŒ Anti-patterns to Avoid
1. **Don't start with the most difficult task**
   - âŒ "I'll do the text editor first (50h)"
   - âœ… "I'll do the tooltip preview first (30min)"

2. **Don't do everything at once**
   - âŒ "I'll implement everything in one giant function"
   - âœ… "I'll split into 3 testable functions"

3. **Don't skip tests**
   - âŒ "I'll test manually later"
   - âœ… "I'll create 12 unit tests now"

4. **Don't make generic commits**
   - âŒ `git commit -m "updates"`
   - âœ… `git commit -m "feat: Task Example with VALUE EQUALITY (60 lines)"`

---

## ðŸ“š References

- **REQUIREMENTS.md**: Complete list of project tasks
- **vX.Y.Z-COMPARISON.md**: First protocol example
- **vX.Y.Z-SPECIFICATIONS.md**: Sprint with 3 simple tasks
- **vX.Y.Z-SPECIFICATIONS.md**: Rapid iterations
- **vX.Y.Z-SPECIFICATIONS.md**: 4 UX improvements
- **FEATURE_SPEC.md**: Example of detailed documentation

---

## ðŸ”„ Continuous Cycle

Simplicity Protocol 1 is an **iterative cycle**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Read Documentation                       â”‚
â”‚  2. Choose Simplest Tasks                    â”‚
â”‚  3. Ask the Programmer Questions             â”‚
â”‚  4. Analyze and Study the Project            â”‚
â”‚  5. Plan Sprint (2-4 tasks, 3-4h)            â”‚
â”‚  6. Implement (GoF + GRASP architecture)     â”‚
â”‚  7. Verify GUI Integration                   â”‚
â”‚  8. Verify CLI Implementation                â”‚
â”‚  9. Test (100% coverage)                     â”‚
â”‚  10. Organize Project Root Folder            â”‚
â”‚  11. Document (TASKS + vX.X.X-SPECS)         â”‚
â”‚  12. Commit + Push (conventional)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    REPEAT    â”‚ â† There are always simpler tasks!
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Result**: Constant progress, professional code, zero technical debt.

---

### 13.5 **Sprint Retrospectives** (Optional - For Continuous Improvement)

**When to Apply**:
- âœ… Long-term projects (>3 months)
- âœ… Team work (2+ people)
- âœ… Iterative cycles (sprints, milestones)
- âœ… When you want to continuously improve the process
- âœ… After completing an important epic/milestone

**Do Not Apply If**:
- âŒ Ad-hoc solo project (no repetition)
- âŒ One-off script or prototype
- âŒ Single-deadline project (non-iterative)
- âŒ No commitment to improvement (retrospective is only valuable if it generates actions)

**What is a Sprint Retrospective?**

A meeting (or document, if solo) at the end of each sprint/milestone to reflect on:
- âœ… **What went well** (keep doing)
- âŒ **What didn't go well** (stop doing)
- ðŸ’¡ **What we can improve** (start doing)
- ðŸ“Š **Progress metrics**

**Retrospective Template**:

```markdown
# Sprint Retrospective #5 - MyProject

**Date**: 2024-01-20
**Sprint**: 2024-01-08 â†’ 2024-01-20 (2 weeks)
**Participants**: JosuÃ© (dev), Alice (reviewer)
**Milestone**: v2.0.0 - SQLite Migration

---

## ðŸ“Š Sprint Metrics

### Progress
- **Tasks Completed**: 8/10 (80%)
- **Story Points**: 21/25 (84%)
- **Bugs Found**: 3
- **Bugs Fixed**: 3
- **Commits**: 24
- **PRs**: 6 (5 merged, 1 pending)

### Quality
- **Test Coverage**: 87% (â†‘ 5% since previous sprint)
- **Code Review**: 100% (all PRs reviewed)
- **CI/CD**: 23/24 builds successful (95.8%)
- **Average PR â†’ Merge Time**: 18h (goal: <24h) âœ…

### Velocity
- **Planned Velocity**: 25 SP
- **Actual Velocity**: 21 SP
- **Efficiency**: 84% (goal: >80%) âœ…

---

## âœ… What Went Well (Keep Doing)

### 1. Simplicity Protocol
**Impact**: HIGH
**What worked**: Following 13 steps ensured consistent quality.
**Evidence**: Zero production bugs in tasks that followed full protocol.
**Action**: Continue using, consider making mandatory for all devs.

### 2. Pre-commit Hooks
**Impact**: MEDIUM
**What worked**: Hooks caught 15 formatting errors before commit.
**Evidence**: Zero code review comments on formatting.
**Action**: Keep hooks, add bandit (security) to config.

### 3. Pair Programming on Complex Features
**Impact**: HIGH
**What worked**: SQLite migration (Task Example) done in pair = zero rework.
**Evidence**: PR approved first-time, no changes requested.
**Action**: Use pair programming for tasks with risk > MEDIUM.

---

## âŒ What Didn't Go Well (Stop Doing / Fix)

### 1. Too Much Manual GUI Testing
**Impact**: HIGH
**Problem**: GUI tested manually every time = 30min per task, repetitive.
**Evidence**: 8 tasks Ã— 30min = 4 hours spent on manual tests.
**Root Cause**: Lack of automated GUI tests.
**Action**:
- [ ] Implement pytest-qt for automated GUI tests (Task Example)
- [ ] Create smoke test suite that runs in CI (Task Example)
- **Owner**: JosuÃ© | **Deadline**: Sprint #6

### 2. Scope Creep in Task Example
**Impact**: MEDIUM
**Problem**: Task "Migrate to SQLite" grew from 8 SP â†’ 13 SP during sprint.
**Evidence**: Task took 3 days instead of 2 days estimated.
**Root Cause**: Underestimated complexity of migration + rollback plan.
**Action**:
- [ ] Add 25% buffer to "first-time" task estimates (Task Example)
- [ ] Split large epics into smaller tasks (<5 SP each)
- **Owner**: Alice | **Deadline**: Next planning

### 3. Delayed Documentation
**Impact**: LOW
**Problem**: ADRs created after PR merged, not during.
**Evidence**: ADR-004 committed 2 days after merge of PR #145.
**Root Cause**: Forgot to include ADR in PR checklist.
**Action**:
- [ ] Update PR template to include "ADR created?" (Task Example)
- [ ] Pre-commit hook to check if docs/adr/ was modified when src/ changes
- **Owner**: JosuÃ© | **Deadline**: Sprint #6

---

## ðŸ’¡ Ideas for Improvement (Start Doing)

### 1. Weekly Micro-Retrospectives
**Proposal**: Short retrospective (10min) every Friday.
**Rationale**: Retrospective every 2 weeks = some lessons forgotten.
**Experiment**: Test for 4 weeks, evaluate if it adds value.
**Action**:
- [ ] Create micro-retro template (3 questions only)
- [ ] Schedule 10min every Friday 4 PM
- **Owner**: Alice | **Status**: Experimental

### 2. Refactoring Fridays
**Proposal**: Last sprint afternoon dedicated to refactoring/tech debt.
**Rationale**: Tech debt accumulating (TODO comments: 23 â†’ 31 since last sprint).
**Experiment**: Dedicate 3h on Friday to clean tech debt.
**Action**:
- [ ] Create `tech-debt` tag in issue tracker
- [ ] Reserve 3h on Friday for tech debt sprint #6
- **Owner**: JosuÃ© | **Status**: Experimental

### 3. Automated Changelog Generation
**Proposal**: Automatically generate CHANGELOG.md from commits.
**Rationale**: Writing changelog manually = 20min repetitive per sprint.
**Solution**: Use `git-cliff` or `conventional-changelog`.
**Action**:
- [ ] Evaluate tools (git-cliff vs conventional-changelog)
- [ ] Integrate into CI pipeline
- **Owner**: Alice | **Deadline**: Sprint #7

---

## ðŸ“ˆ Comparison with Previous Sprints

| Metric | Sprint #3 | Sprint #4 | Sprint #5 | Trend |
|---|---|---|---|---|
| Velocity | 18 SP | 22 SP | 21 SP | â†”ï¸ Stable |
| Coverage | 78% | 82% | 87% | â†—ï¸ Improving |
| Production Bugs | 2 | 1 | 0 | â†—ï¸ Excellent |
| PRâ†’Merge Time | 36h | 24h | 18h | â†—ï¸ Improving |
| Tech Debt Items | 18 | 23 | 31 | â†˜ï¸ **ALERT** |

**Analysis**:
- âœ… Quality improving (coverage â†‘, bugs â†“)
- âœ… Efficiency improving (faster PRs)
- âš ï¸ **Tech debt accumulating** - needs attention (Refactoring Fridays)

---

## ðŸŽ¯ Action Items for Next Sprint

| # | Action | Owner | Deadline | Priority |
|---|---|---|---|---|
| #89 | Implement pytest-qt for GUI | JosuÃ© | Sprint #6 | ðŸ”´ HIGH |
| #90 | Create CI smoke test suite | JosuÃ© | Sprint #6 | ðŸ”´ HIGH |
| #91 | Add 25% buffer to estimates | Alice | Planning #6 | ðŸŸ¡ MEDIUM |
| #92 | Update PR template (ADR) | JosuÃ© | Sprint #6 | ðŸŸ¢ LOW |
| - | Test weekly micro-retros | Alice | Sprint #6 | ðŸ§ª Experimental |
| - | Dedicate 3h Friday tech debt | JosuÃ© | Sprint #6 | ðŸ§ª Experimental |

**Tracked in**: [GitHub Project - Sprint #6](link)

---

## ðŸ’¬ Team Feedback

### JosuÃ©
> "Simplicity Protocol is working very well. I feel that quality is better. Concerned about tech debt accumulating - let's try Refactoring Fridays."

### Alice
> "Code reviews are faster and smoother. Loved pair programming on the SQLite migration. Suggestion: can we do retrospectives more frequently? Every 2 weeks feels like a long time."

---

## ðŸ“š Lessons Learned

### Technical
1. **SQLite Migrations**: Always create automatic backup + rollback plan.
2. **Feature Flags**: Better than full rollback for large features.
3. **GUI Testing**: Pytest-qt saves significant time vs manual.

### Process
1. **Retrospectives**: 2 weeks = good, but weekly micro-retros can add value.
2. **Estimates**: First time doing something = add 25% buffer.
3. **Tech Debt**: Needs dedicated time, not "when there's leftover time."

### Personal
1. **Pair Programming**: Worth it for complex/critical tasks.
2. **Communication**: PRs with rich context = faster reviews.
3. **Documentation**: ADRs should be created DURING PR, not after.

---

**Next Retrospective**: 2024-02-03 (Sprint #6)
**Format**: In-person or updated document
**Facilitator**: Alice (rotating)
```

## ðŸŽ¯ Final Message

> "I want a complete and professional job!"

**This protocol guarantees**:
- âœ… Professional quality (13 mandatory + 10 advanced optional steps)
- âœ… Incremental progress (from simple to complex)
- âœ… Complete documentation (never forget what was done)
- âœ… Tested and secure code (100% reliable)
- âœ… Verified integration (functional GUI + CLI)
- âœ… Organized commits (clean history)
- âœ… **[NEW v2.0]** Enterprise practices (Security, CI/CD, ADRs, Retrospectives)

**Reread this document before each sprint!**

---

## ðŸ’¡ Programming Best Practices for AI

> **This section contains specific recommendations to improve the quality of code generated by artificial intelligences.**

### 1. ðŸ“– **Readable and Self-Documenting Code**

**Why it matters**: AIs should produce code that humans can easily understand and maintain.

**Practices**:
- âœ… **Descriptive names**: Use names that explain the purpose
  ```python
  # âŒ BAD
  def proc(d, x):
      return d[x] if x in d else None
  
  # âœ… GOOD
  def get_user_preference(preferences_dict, preference_key):
      """Returns user preference or None if it doesn't exist."""
      return preferences_dict.get(preference_key)
  ```

- âœ… **Small and focused functions**: One function = one responsibility
  ```python
  # âŒ BAD - Function does multiple things
  def process_user_data(user):
      # validates
      # transforms
      # saves to database
      # sends email
      # logs
      pass  # 150 lines
  
  # âœ… GOOD - Specialized functions
  def validate_user_data(user): pass
  def transform_user_data(user): pass
  def save_user_to_database(user): pass
  def send_welcome_email(user): pass
  def log_user_registration(user): pass
  ```

- âœ… **Avoid "magic numbers"**: Use named constants
  ```python
  # âŒ BAD
  if user.age > 18 and balance < 1000:
      apply_fee(balance * 0.05)
  
  # âœ… GOOD
  MINIMUM_ADULT_AGE = 18
  BALANCE_THRESHOLD = 1000
  SERVICE_FEE_RATE = 0.05
  
  if user.age > MINIMUM_ADULT_AGE and balance < BALANCE_THRESHOLD:
      apply_fee(balance * SERVICE_FEE_RATE)
  ```

### 2. ðŸŽ¯ **Consistent Naming Conventions**

**Why it matters**: Consistency facilitates navigation and code comprehension.

**Practices by language**:

**Python**:
- âœ… `snake_case` for functions and variables
- âœ… `PascalCase` for classes
- âœ… `SCREAMING_SNAKE_CASE` for constants
- âœ… `_private_method` for private methods

**JavaScript/TypeScript**:
- âœ… `camelCase` for functions and variables
- âœ… `PascalCase` for classes and components
- âœ… `SCREAMING_SNAKE_CASE` for constants
- âœ… `_privateMethod` or `#privateField` for private

**General conventions**:
- âœ… Verbs for functions: `get_user()`, `calculate_total()`, `validate_input()`
- âœ… Nouns for classes: `UserManager`, `PaymentProcessor`
- âœ… Booleans with prefixes: `is_valid`, `has_permission`, `can_edit`

### 3. ðŸ›¡ï¸ **Robust Error Handling**

**Why it matters**: Production code must gracefully handle failures.

**Practices**:
- âœ… **Always validate input**:
  ```python
  def divide(a, b):
      if not isinstance(a, (int, float)) or not isinstance(b, (int, float)):
          raise TypeError("Arguments must be numbers")
      if b == 0:
          raise ValueError("Divisor cannot be zero")
      return a / b
  ```

- âœ… **Use specific exceptions**:
  ```python
  # âŒ BAD - Generic exception
  try:
      process_payment(amount)
  except Exception as e:
      print("Error")
  
  # âœ… GOOD - Specific exceptions
  try:
      process_payment(amount)
  except PaymentDeclinedError as e:
      notify_user("Payment declined")
  except InsufficientFundsError as e:
      notify_user("Insufficient funds")
  except NetworkError as e:
      retry_payment(amount)
  ```

- âœ… **Adequate logging**:
  ```python
  import logging
  
  try:
      result = risky_operation()
  except Exception as e:
      logging.error(f"Failed in risky_operation: {e}", exc_info=True)
      raise  # Re-raise to allow handling at higher level
  ```

### 4. ðŸ§ª **Effective Testing Strategies**

**Why it matters**: Tests ensure code works and continues working.

**Practices**:
- âœ… **Unit tests for business logic**:
  ```python
  def test_calculate_discount():
      # Arrange
      original_price = 100
      discount_rate = 0.2
      
      # Act
      final_price = calculate_discount(original_price, discount_rate)
      
      # Assert
      assert final_price == 80
  ```

- âœ… **Test edge cases**:
  ```python
  def test_edge_cases():
      assert calculate_discount(0, 0.5) == 0  # Zero price
      assert calculate_discount(100, 0) == 100  # Zero discount
      assert calculate_discount(100, 1.0) == 0  # 100% discount
      
      with pytest.raises(ValueError):
          calculate_discount(100, -0.1)  # Negative discount
      
      with pytest.raises(ValueError):
          calculate_discount(-100, 0.1)  # Negative price
  ```

- âœ… **Mocks for external dependencies**:
  ```python
  from unittest.mock import Mock, patch
  
  def test_send_notification():
      with patch('email_service.send') as mock_send:
          notify_user("user@example.com", "Test message")
          mock_send.assert_called_once()
  ```

### 5. ðŸ”’ **Security First**

**Why it matters**: Vulnerabilities can have serious consequences.

**Practices**:
- âœ… **Never trust user input**:
  ```python
  # âŒ BAD - SQL Injection
  query = f"SELECT * FROM users WHERE id = {user_id}"
  
  # âœ… GOOD - Parameterization
  query = "SELECT * FROM users WHERE id = ?"
  cursor.execute(query, (user_id,))
  ```

- âœ… **Secrets in environment variables**:
  ```python
  # âŒ BAD
  API_KEY = "sk-1234567890abcdef"  # Hardcoded
  
  # âœ… GOOD
  import os
  API_KEY = os.getenv('API_KEY')
  if not API_KEY:
      raise ValueError("API_KEY not configured")
  ```

- âœ… **Sanitize output to prevent XSS**:
  ```python
  from html import escape
  
  # âŒ BAD
  html = f"<div>Hello {user_name}</div>"
  
  # âœ… GOOD
  html = f"<div>Hello {escape(user_name)}</div>"
  ```

### 6. âš¡ **Performance Optimization**

**Why it matters**: Slow code = unhappy users.

**Practices**:
- âœ… **Choose correct data structure**:
  ```python
  # âŒ BAD - List search O(n)
  if user_id in user_list:  # 1000 comparisons
      # ...
  
  # âœ… GOOD - Set search O(1)
  if user_id in user_set:  # 1 comparison
      # ...
  ```

- âœ… **Avoid unnecessary loops**:
  ```python
  # âŒ BAD - Double loop O(nÂ²)
  for item in list1:
      for item2 in list2:
          if item == item2:
              # ...
  
  # âœ… GOOD - Set intersection O(n)
  common_items = set(list1) & set(list2)
  for item in common_items:
      # ...
  ```

- âœ… **Lazy loading when appropriate**:
  ```python
  # âŒ BAD - Load everything into memory
  all_users = User.objects.all()  # 1 million records
  for user in all_users:
      process(user)
  
  # âœ… GOOD - Iterator that loads on demand
  for user in User.objects.iterator():
      process(user)
  ```

### 7. ðŸ“ **Clear and Useful Documentation**

**Why it matters**: Code is read much more often than it is written.

**Practices**:
- âœ… **Complete docstrings**:
  ```python
  def calculate_shipping(weight, distance, express=False):
      """
      Calculate shipping cost based on weight and distance.
      
      Args:
          weight (float): Package weight in kg
          distance (float): Distance in km
          express (bool): If True, uses express shipping (default: False)
      
      Returns:
          float: Shipping cost in dollars
      
      Raises:
          ValueError: If weight or distance is negative
      
      Examples:
          >>> calculate_shipping(2.5, 100)
          25.0
          >>> calculate_shipping(2.5, 100, express=True)
          37.5
      """
      if weight < 0 or distance < 0:
          raise ValueError("Weight and distance must be positive")
      
      base_cost = weight * distance * 0.1
      return base_cost * 1.5 if express else base_cost
  ```

- âœ… **Comments explain "why", not "what"**:
  ```python
  # âŒ BAD - Comments the obvious
  x = x + 1  # Increment x
  
  # âœ… GOOD - Explains the reason
  # Increment counter to include current element in count
  # since range() excludes the last element
  x = x + 1
  ```

- âœ… **README with practical examples**:
  ```markdown
  # How to use
  
  ## Installation
  ```bash
  pip install mypackage
  ```
  
  ## Basic example
  ```python
  from mypackage import Calculator
  
  calc = Calculator()
  result = calc.add(2, 3)
  print(result)  # Output: 5
  ```
  ```

### 8. ðŸ—ï¸ **Organization and Modularity**

**Why it matters**: Organized code is easier to maintain and scale.

**Practices**:
- âœ… **Separation of concerns**:
  ```
  project/
  â”œâ”€â”€ models/       # Data structures
  â”œâ”€â”€ services/     # Business logic
  â”œâ”€â”€ controllers/  # Flow coordination
  â”œâ”€â”€ views/        # User interface
  â”œâ”€â”€ utils/        # Helper functions
  â””â”€â”€ tests/        # Automated tests
  ```

- âœ… **DRY (Don't Repeat Yourself)**:
  ```python
  # âŒ BAD - Duplicated code
  def process_order_a():
      validate()
      calculate()
      save()
  
  def process_order_b():
      validate()
      calculate()
      save()
  
  # âœ… GOOD - Reused code
  def process_order_common():
      validate()
      calculate()
      save()
  
  def process_order_a():
      process_order_common()
      # specific logic A
  
  def process_order_b():
      process_order_common()
      # specific logic B
  ```

- âœ… **Single responsibility principle**:
  ```python
  # âŒ BAD - Class does many things
  class User:
      def __init__(self): pass
      def save_to_database(self): pass
      def send_email(self): pass
      def generate_pdf_report(self): pass
  
  # âœ… GOOD - Specialized classes
  class User:
      def __init__(self): pass
  
  class UserRepository:
      def save(self, user): pass
  
  class EmailService:
      def send(self, to, message): pass
  
  class ReportGenerator:
      def generate_pdf(self, user): pass
  ```

### 9. ðŸ”„ **Effective Version Control**

**Why it matters**: Clean history facilitates debugging and collaboration.

**Practices**:
- âœ… **Atomic and descriptive commits**:
  ```bash
  # âŒ BAD
  git commit -m "fixes"
  git commit -m "updates"
  
  # âœ… GOOD
  git commit -m "feat: add email validation in registration form"
  git commit -m "fix: correct discount calculation for amounts over $1000"
  ```

- âœ… **Branches for features**:
  ```bash
  # Create branch for new feature
  git checkout -b feature/user-authentication
  
  # Develop and commit
  git commit -m "feat: implement JWT login"
  
  # Merge after review
  git checkout main
  git merge feature/user-authentication
  ```

- âœ… **Appropriate .gitignore**:
  ```gitignore
  # Python
  __pycache__/
  *.pyc
  .env
  venv/
  
  # JavaScript
  node_modules/
  dist/
  .env.local
  
  # IDEs
  .vscode/
  .idea/
  *.swp
  
  # OS
  .DS_Store
  Thumbs.db
  ```

### 10. ðŸ“¦ **Dependency Management**

**Why it matters**: Poorly managed dependencies cause compatibility problems.

**Practices**:
- âœ… **Pin versions**:
  ```
  # âŒ BAD - requirements.txt
  flask
  requests
  
  # âœ… GOOD - requirements.txt
  flask==2.3.2
  requests==2.31.0
  ```

- âœ… **Use virtual environments**:
  ```bash
  # Python
  python -m venv venv
  source venv/bin/activate
  pip install -r requirements.txt
  
  # Node.js
  npm install  # Uses package-lock.json
  ```

- âœ… **Check for vulnerabilities**:
  ```bash
  # Python
  pip install pip-audit
  pip-audit
  
  # Node.js
  npm audit
  npm audit fix
  ```

### 11. ðŸ”„ **Frequent Code Refactoring**

**Why it matters**: Code that isn't regularly refactored tends to deteriorate over time, becoming difficult to maintain, understand, and evolve.

> **CRITICAL FOR AIs**: Remember to **frequently** refactor code during development to maintain quality and avoid accumulation of technical debt.

**Mandatory practices**:

- âœ… **Avoid excessively large files**:
  ```
  # ðŸš¨ SIZE ALERTS
  - File > 500 lines â†’ Consider splitting
  - File > 1000 lines â†’ MUST split
  - Class > 300 lines â†’ Refactor into smaller classes
  - Function > 50 lines â†’ Split into helper functions
  ```
  
  **Refactoring example**:
  ```python
  # âŒ BAD - 1500-line file
  # user_manager.py (everything in one file)
  class UserManager:
      def create_user(): pass  # 100 lines
      def validate_user(): pass  # 150 lines
      def authenticate_user(): pass  # 200 lines
      def send_email(): pass  # 100 lines
      # ... 950 more lines
  
  # âœ… GOOD - Split into specialized modules
  # user/
  #   __init__.py
  #   manager.py (200 lines)
  #   validator.py (150 lines)
  #   authenticator.py (200 lines)
  #   notifications.py (100 lines)
  ```

- âœ… **Increase cohesion (Single Responsibility Principle)**:
  ```python
  # âŒ BAD - Low cohesion (does many different things)
  class OrderProcessor:
      def process_order(self):
          self.validate_payment()
          self.send_email()
          self.update_inventory()
          self.generate_invoice()
          self.log_analytics()
  
  # âœ… GOOD - High cohesion (each class has one responsibility)
  class PaymentValidator:
      def validate(self): pass
  
  class EmailNotifier:
      def send_order_confirmation(self): pass
  
  class InventoryManager:
      def update_stock(self): pass
  
  class InvoiceGenerator:
      def generate(self): pass
  
  class AnalyticsLogger:
      def log_order(self): pass
  ```

- âœ… **Constantly improve readability**:
  ```python
  # âŒ BAD - Hard to understand
  def p(d, x, y):
      return sum([d[i][x] * d[i][y] for i in range(len(d)) if x in d[i] and y in d[i]])
  
  # âœ… GOOD - Self-explanatory
  def calculate_correlation_between_features(dataset, feature_x, feature_y):
      """
      Calculates the correlation between two features in a dataset.
      
      Args:
          dataset: List of dictionaries containing features
          feature_x: Name of the first feature
          feature_y: Name of the second feature
      
      Returns:
          float: Sum of feature products when both exist
      """
      correlation_sum = 0
      for data_point in dataset:
          if feature_x in data_point and feature_y in data_point:
              correlation_sum += data_point[feature_x] * data_point[feature_y]
      return correlation_sum
  ```

- âœ… **Eliminate redundancies and increase reusability**:
  ```python
  # âŒ BAD - Duplicated code (redundancy)
  def get_active_users():
      users = db.query("SELECT * FROM users")
      active = [u for u in users if u.status == 'active' and u.verified == True]
      return active
  
  def get_active_admins():
      users = db.query("SELECT * FROM users")
      active = [u for u in users if u.status == 'active' and u.verified == True and u.role == 'admin']
      return active
  
  # âœ… GOOD - Reusable code (DRY - Don't Repeat Yourself)
  def get_verified_active_users(role=None):
      """Returns active and verified users, optionally filtered by role."""
      users = db.query("SELECT * FROM users")
      filtered = [u for u in users if u.status == 'active' and u.verified == True]
      
      if role:
          filtered = [u for u in filtered if u.role == role]
      
      return filtered
  
  def get_active_users():
      return get_verified_active_users()
  
  def get_active_admins():
      return get_verified_active_users(role='admin')
  ```

- âœ… **Hierarchize code into folders and directories**:
  ```
  # âŒ BAD - Everything in root (hard to navigate)
  project/
    main.py
    user_stuff.py
    payment_things.py
    email_sender.py
    validators.py
    helpers.py
    utils.py
    config.py
    constants.py
  
  # âœ… GOOD - Logical hierarchy (easy to understand and maintain)
  project/
    main.py
    config/
      __init__.py
      settings.py
      constants.py
    core/
      __init__.py
      models.py
      exceptions.py
    features/
      users/
        __init__.py
        manager.py
        validator.py
      payments/
        __init__.py
        processor.py
        validator.py
    services/
      email/
        __init__.py
        sender.py
        templates.py
    utils/
      __init__.py
      helpers.py
      formatters.py
  ```

- âœ… **Search for orphaned code after refactoring** (â­ **MANDATORY**):
  
  > **CRITICAL**: After any refactoring, it is **MANDATORY** to search for orphaned code - code that was implemented but is no longer being used.
  
  **What is orphaned code?**
  - âŒ Unused functions (defined but never called)
  - âŒ Unused variables (declared but never referenced)
  - âŒ Unused imports (imported but never used)
  - âŒ Dead/unreachable code
  - âŒ Uninstantiated classes (defined but never created)
  - âŒ Uncalled methods (defined but never invoked)
  
  **Why search for orphaned code?**
  - âœ… **Reduces complexity**: Less code = easier to understand
  - âœ… **Improves maintenance**: Don't waste time on unused code
  - âœ… **Avoids confusion**: Orphaned code can mislead developers
  - âœ… **Performance**: Less code = faster startup
  - âœ… **Security**: Orphaned code may contain forgotten vulnerabilities
  
  **Tools to detect orphaned code**:
  ```bash
  # Python - Unused code (functions, classes, variables)
  pip install vulture
  vulture src/ --min-confidence 80
  # Output: unused functions/classes/variables
  
  # Python - Unused imports
  pip install autoflake
  autoflake --remove-all-unused-imports --check -r src/
  # Or use pylint
  pylint --disable=all --enable=unused-import src/
  
  # JavaScript/TypeScript - Unused code
  npm install -g ts-prune  # For TypeScript
  ts-prune
  # Or ESLint
  npm run lint -- --rule 'no-unused-vars: error'
  
  # For any language - Search for unused definitions
  # 1. Generate list of definitions (functions, classes)
  # 2. Search for references to each definition in code
  # 3. If no reference found â†’ orphaned code
  ```
  
  **Usage example (Python)**:
  ```python
  # Before refactoring - 500-line file
  
  # Refactoring: split into 3 smaller files
  # Now search for orphaned code:
  
  $ vulture src/ --min-confidence 80
  src/old_module.py:45: unused function 'process_legacy_format' (100% confidence)
  src/utils.py:123: unused function 'deprecated_helper' (90% confidence)
  src/models.py:67: unused class 'OldDataModel' (100% confidence)
  
  # Action: Remove or document why keeping
  # If truly unused â†’ DELETE
  # If will be used in future â†’ Mark with comment and issue
  ```
  
  **Orphaned code checklist** (execute AFTER refactoring):
  ```markdown
  - [ ] Run vulture (Python) or ts-prune (TypeScript)
  - [ ] Review unused functions (confirm if truly orphaned)
  - [ ] Remove unused imports (autoflake or similar tool)
  - [ ] Check uninstantiated classes
  - [ ] Search for old commented code (also orphaned code)
  - [ ] Document if any "orphaned" code should be kept (e.g., public API)
  ```
  
  **When NOT to remove**:
  - âœ… **Public APIs**: Even if not used internally, external clients may use them
  - âœ… **Hooks/callbacks**: May be called by frameworks
  - âœ… **Test code**: Test helpers may appear unused
  - âœ… **Planned code**: If there's an issue/task to use soon, keep (but document)

**When to refactor**:

1. **During new feature implementation**:
   - Before adding new code, check if existing files are organized
   - If you find poorly structured code, refactor BEFORE adding new functionality

2. **After completing a feature**:
   - Review the implemented code
   - Identify improvement opportunities (DRY, SRP, better names)
   - Refactor immediately while context is fresh
   - **â­ MANDATORY**: Search for orphaned code (vulture, autoflake, etc.)

3. **When reviewing code (Steps 7 and 8)**:
   - Use the 9 quality criteria as a guide
   - If you detect redundancy, lower cohesion, or higher coupling â†’ Refactor

4. **Before committing (Step 13)**:
   - Last checkpoint: is the code as clean as possible?
   - Is there anything that can be simplified?

5. **Minimum periodicity**:
   - âš ï¸ **NEVER** let more than 3-5 features pass without refactoring
   - ðŸš¨ If project has > 10 files with > 500 lines â†’ PRIORITIZE refactoring
   - â­ **Always search for orphaned code after refactoring** (not optional)

**Benefits of frequent refactoring**:
- âœ… **Simpler maintenance**: Organized code is easier to modify
- âœ… **Fewer bugs**: Clean code has fewer places for bugs to hide
- âœ… **Faster onboarding**: New developers understand the code faster
- âœ… **Speed**: Paradoxically, frequent refactoring ACCELERATES development
- âœ… **Easier validation**: Modular code is easier to test and verify

**Tools to identify refactoring needs**:
```bash
# Python - Cyclomatic complexity
pip install radon
radon cc . -a -nb  # Show complex functions

# Python - Duplicated code
pip install pylint
pylint --disable=all --enable=duplicate-code .

# Python - Dead code
pip install vulture
vulture .

# JavaScript - Complexity analysis
npm install -g complexity-report
cr --format json src/
```

### ðŸŽ¯ **Quick Checklist for AI**

Before generating/committing code, verify:

- [ ] Names are descriptive and follow language conventions?
- [ ] Functions have single responsibility and are small?
- [ ] Is there error handling for exceptional cases?
- [ ] Code is tested (unit tests + edge cases)?
- [ ] No obvious security vulnerabilities?
- [ ] Performance is acceptable (no unnecessary O(nÂ²) algorithms)?
- [ ] Is there documentation (docstrings, useful comments)?
- [ ] Code is organized in logical modules?
- [ ] **Code was recently refactored?** (files < 500 lines, no duplication)
- [ ] **Folder hierarchy is logical?** (clear separation of responsibilities)
- [ ] Commits are descriptive (conventional commits)?
- [ ] Dependencies have pinned versions?

### ðŸ“š **Additional Resources**

- **Clean Code** (Robert C. Martin) - Clean code principles
- **SOLID Principles** - Well-done object orientation
- **Design Patterns** (GoF) - Common solutions to common problems
- **OWASP Top 10** - Main security vulnerabilities
- **PEP 8** (Python) - Python style guide
- **Google Style Guides** - Style guides by language

---

**Reread this document before each sprint!**

---

**Version**: 2.3
**Last updated**: December 16, 2025
**Maintained by**: JosuÃ© Amaral
**Status**: ACTIVE - Advanced protocol for critical/enterprise projects
```
