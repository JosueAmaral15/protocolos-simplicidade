Here's the English translation of the provided document:

```markdown
# Simplicity Protocol 1

**Author**: Josu√© Amaral
**Creation Date**: November 30, 2025
**Version**: 2.0
**Last Update**: December 10, 2025
**Objective**: Professional methodology for incremental quality development

**Changelog v2.0** (10/12/2025):
- ‚úÖ **[COMPLEMENTATION]** Added section "üéØ When to Use Simplicity 1?"
- ‚úÖ Clear criteria: ‚úÖ When to use (8 criteria) | ‚ùå When NOT to use (6 criteria)
- ‚úÖ Migration: When to evolve to Simplicity 2 (teams) or 3 (solo production)
- ‚úÖ Detailed Rationale: Why Simplicity 1 is agile but insufficient for production
- ‚úÖ Inspiration: Concepts adapted from Simplicity 3 v3.1 (comparative tables, criteria)

**Changelog v1.9** (09/12/2025):
- ‚úÖ **[STEP 3]** Added recommendation for AI to provide suggestions and hunches for questions
- ‚úÖ Recommended format: "‚ùì Question + üí° AI Suggestion + Options A/B/C"
- ‚úÖ Rationale: Accelerates decisions, reduces cognitive load, maintains consistency with existing code
- ‚úÖ Classification: **OPTIONAL but HIGHLY RECOMMENDED**

**Changelog v1.8** (02/12/2025):
- ‚úÖ **[REORGANIZATION]** Code Review integrated into CLI and GUI steps
- ‚úÖ Step 7: Verify CLI Implementation (includes 9 quality criteria)
- ‚úÖ Step 8: Verify GUI Implementation (includes 9 quality criteria)
- ‚úÖ Step 9: Verify Integration with Main Program (kept as a separate step)
- ‚úÖ 9 Criteria: Omission, Ambiguity, Incorrect Fact, Redundancy, Inconsistency, Lack of Integration, Lower Cohesion, Higher Coupling, Strange Information
- ‚úÖ Review integrated into the CLI/GUI verification process
- ‚úÖ Total steps: 12 ‚Üí 13 (added integration verification after GUI)

**Changelog v1.7** (02/12/2025):
- ‚úÖ **[CRITICAL]** Added Step 8.5: Code Review (BEFORE tests)
- ‚úÖ 9 Quality Criteria: Omission, Ambiguity, Incorrect Fact, Redundancy, Inconsistency, Lack of Integration, Lower Cohesion, Higher Coupling, Strange Information
- ‚úÖ Complete review checklist (36 verification items)
- ‚úÖ Recommended tools (pylint, vulture, radon, black, isort)
- ‚úÖ Detailed CLI and GUI review process
- ‚úÖ Practical examples of problems and corrections
- ‚úÖ Integration with Step 9 (test after review)
- ‚úÖ Total steps: 12 ‚Üí 13 (8.5 added between 8 and 9)

**Changelog v1.6**:
- ‚úÖ **[ADVANCED]** Added Step 9.2: Tests in Threads/Processes with Monitoring
- ‚úÖ Test execution in a separate process (`multiprocessing.Process`)
- ‚úÖ Real-time logging via `Queue` (progress of each test)
- ‚úÖ Manual cancellation at any time (graceful Ctrl+C)
- ‚úÖ Global + individual timeout (double protection)
- ‚úÖ Real-time statistics (passed/failed/elapsed)
- ‚úÖ Full implementation of `test_runner_monitored.py` (~150 lines)
- ‚úÖ Optional additional checklist (6 items)

**Changelog v1.5**:
- ‚úÖ **[CRITICAL]** Added Step 9.1: Security in Tests
- ‚úÖ 7 mandatory solutions to avoid infinite loops and timeouts
- ‚úÖ Mandatory maximum timeout (30s per test)
- ‚úÖ Mandatory headless environment for GUI tests (QT_QPA_PLATFORM=offscreen)
- ‚úÖ Mandatory dry-run before executing tests (syntax + import + collect)
- ‚úÖ Security checklist with 6 mandatory items
- ‚úÖ Golden rules and safe commands documented
- ‚úÖ Lessons learned from critical production bugs

**Changelog v1.4**:
- ‚úÖ Reorganized final order: Implement ‚Üí Integrate GUI ‚Üí CLI ‚Üí Test ‚Üí Organize ‚Üí Document ‚Üí Commit
- ‚úÖ Tests moved to AFTER integration checks (test integrated system)
- ‚úÖ Organize root folder moved to BEFORE documentation (document clean state)
- ‚úÖ Logic: Integrate ‚Üí Test integration ‚Üí Clean repository ‚Üí Document final state

**Changelog v1.3**:
- ‚úÖ Reorganized step order: GUI and CLI Integration Verification now come BEFORE Documentation
- ‚úÖ New order: Tests ‚Üí GUI Integration ‚Üí CLI Integration ‚Üí Documentation ‚Üí Organize ‚Üí Commit
- ‚úÖ Logic: Verifying integration before documenting ensures that the documentation reflects the actual state

**Changelog v1.2**:
- ‚úÖ Added Step 8: Verify integration with main program
- ‚úÖ Added Step 9: Verify CLI implementation with parameter passing
- ‚úÖ Total steps: 10 ‚Üí 12

---

## üéØ Core Philosophy

> "There will always be complex tasks to do, but also those that are more difficult and those that are easier. **I want you to always start with the easier ones**."

**Principle**: From simple to complex, incremental, professional, and complete.

---

## ‚ö†Ô∏è Golden Rule: Absolute Priority for Workspace Errors

> **CRITICAL FOR AIs**: Before implementing new features or continuing with tasks, **all workspace errors must be fixed BY YOU (AI)**.

### ü§ñ This Rule is For AI Assistants

**If you are an AI (Cursor, GitHub Copilot, etc.):**
- ‚úÖ **YOU MUST** fix all existing errors BEFORE implementing new features
- ‚úÖ **YOU MUST** resolve issues proactively, not wait for humans to fix them
- ‚úÖ **YOU MUST** treat error correction as the highest priority
- ‚úÖ **YOU MUST** clean the workspace before adding new code

**This rule does NOT mean:**
- ‚ùå That human developers must stop implementing when errors exist
- ‚ùå That the project cannot advance while errors are present
- ‚ùå That humans need to manually fix the errors

### üö® Types of Errors That Block Development

Consider the existence of errors in the workspace (visible in the IDE's "Problems" tab) as **undesirable and blocking**. If any of the following types of errors occur, **fixing them is an absolute priority** before continuing:

1. **‚ùå Syntax Issues**
   - Code parsing errors
   - Unclosed parentheses, braces, or brackets
   - Incorrect indentation (Python)
   - Missing semicolons (JavaScript, C, Java)

2. **‚ùå Code Inconsistencies**
   - Variables declared but not used
   - Unused or missing imports
   - Dead code (unreachable code)
   - Type mismatches (TypeScript, Python with type hints)

3. **‚ùå Unexpected Omissions**
   - Functions declared but not implemented
   - Missing required parameters
   - Missing return statements when expected
   - Missing mandatory documentation

4. **‚ùå Incorrect Facts**
   - References to non-existent variables
   - Function calls with wrong number of arguments
   - Access to non-existent properties
   - Imports of non-existent modules

5. **‚ùå Ambiguities**
   - Type checking warnings
   - Possible null/undefined references
   - Variable shadowing
   - Dangerous implicit type conversions

6. **‚ùå Missing Files**
   - Dependencies not installed
   - Imported modules not found
   - Missing configuration files
   - Referenced but non-existent assets

7. **‚ùå Execution Failures**
   - Build failures
   - Compilation errors
   - Failing tests
   - Linter errors (when configured)

### ‚úÖ When You Can Continue

**ONLY** continue with development of new features when:

- ‚úÖ **Zero errors** in the workspace "Problems" tab
- ‚úÖ **All builds** complete successfully
- ‚úÖ **All tests** pass (if they already exist)
- ‚úÖ **Linter/formatter** doesn't report critical errors
- ‚úÖ **Type checker** doesn't report errors (if applicable)

### üìã Checklist Before Each Task

```markdown
Before starting any new task:

[ ] Check IDE "Problems" tab (0 errors)
[ ] Run project build (success)
[ ] Run existing tests (all passing)
[ ] Run linter/formatter (no critical errors)
[ ] Verify imports and dependencies (all resolved)
[ ] Confirm code is in clean state (committable)
```

### ‚è±Ô∏è Estimated Time for Fixing

- **Syntax Errors**: ~2-5 minutes per error
- **Imports/Dependencies**: ~5-10 minutes
- **Type Errors**: ~5-15 minutes per error
- **Failing Tests**: ~10-30 minutes (depends on complexity)

**Rule of Thumb**: If you have >10 errors in the workspace, **dedicate 1-2 hours** to clean everything before proceeding.

### üéØ Rationale

**Why is this rule critical?**

1. **Cascade Prevention**: One uncorrected error can generate 10 new errors
2. **Code Quality**: Code with errors = immediate technical debt
3. **Reliability**: New features on top of broken code = guaranteed bugs
4. **Productivity**: Fixing old + new errors is more time-consuming than fixing only old ones
5. **Professionalism**: Clean, error-free code is a minimum requirement

**Message for AIs**: 
> "Until the errors are resolved BY YOU (AI), tasks and features cannot continue being implemented BY YOU (AI). Fix the errors first, then continue with implementation."

---

## üåê Code Language: Variable Naming and Comments

> **IMPORTANT FOR AIs**: The choice of language for variable names and comments should be defined at the beginning of the project, preferably during the first session of interaction with the programmer.

### üìã Default Rule

**By default**, when programming with artificial intelligence:
- ‚úÖ **Variable names**: Should be in **English** (recommended for international projects)
- ‚úÖ **Comments**: Should be in **English** (recommended for international projects)
- ‚úÖ **Docstrings**: Should be in **English** (recommended for international projects)

**Note**: For Portuguese-speaking developers working on national projects, **Portuguese is the recommended default**. The AI should adapt based on the programmer's language preference.

**Justification**: Facilitates understanding and maintenance of code for developers, maintaining consistency with project documentation and communication. English is recommended for international projects, while native language (e.g., Portuguese) is recommended for national projects.

### ü§î Mandatory Question in First Session

**The AI MUST ask the programmer at the first moment (or during the first session)**:

```
‚ùì Code Language Preferences

To maintain consistency in the project, I need to define the default 
language for variable names and comments in the code:

üí° Suggestion: English (recommended for international projects)
   or Native Language (recommended for national projects)

Options:
A) üá∫üá∏ English - Variables and comments in English (RECOMMENDED for international)
B) üáßüá∑ Native Language - Variables and comments in native language (RECOMMENDED for national)
C) üåç Mixed - Variables in English, comments in native language
D) ‚öôÔ∏è Custom - Specify custom preference

What is your preference?
```

### ‚úÖ Available Options

#### Option A: üá∫üá∏ English (RECOMMENDED for International Projects)
```python
# ‚úÖ Example in English
def calculate_total_price(items: List[Item]) -> float:
    """
    Calculates the total price of a list of items.
    
    Args:
        items: List of items to be summed
        
    Returns:
        Total price with taxes included
    """
    subtotal_price = sum(item.price for item in items)
    tax_rate = 0.15
    final_price = subtotal_price * (1 + tax_rate)
    return final_price
```

#### Option B: üáßüá∑ Native Language (e.g., Portuguese)
```python
# ‚úÖ Exemplo em Portugu√™s
def calcular_preco_total(itens: List[Item]) -> float:
    """
    Calcula o pre√ßo total de uma lista de itens.
    
    Args:
        itens: Lista de itens a serem somados
        
    Returns:
        Pre√ßo total com impostos inclu√≠dos
    """
    preco_subtotal = sum(item.preco for item in itens)
    taxa_imposto = 0.15
    preco_final = preco_subtotal * (1 + taxa_imposto)
    return preco_final
```

#### Option C: üåç Mixed (Variables in English, Comments in Native Language)
```python
# ‚úÖ Mixed Example
def calculate_total_price(items: List[Item]) -> float:
    """
    Calcula o pre√ßo total de uma lista de itens.
    
    Args:
        items: Lista de itens a serem somados
        
    Returns:
        Pre√ßo total com impostos inclu√≠dos
    """
    subtotal_price = sum(item.price for item in items)
    tax_rate = 0.15  # Taxa de imposto de 15%
    final_price = subtotal_price * (1 + tax_rate)
    return final_price
```

### üìù Register the Preference

After the programmer's response, the AI should:

1. **Register the preference** in a visible location (e.g., README.md, CONTRIBUTING.md)
2. **Apply consistently** throughout all generated code
3. **Remember the preference** in future sessions of the same project

**Example Registration in README.md**:
```markdown
## üåê Code Conventions

- **Code Language**: English
- **Variables**: Names in English (e.g., `active_user`, `calculate_total`)
- **Comments**: In English
- **Documentation**: In English
```

### üîÑ Preference Change

The programmer can request a language change at any time:
- ‚úÖ "Switch to English from now on"
- ‚úÖ "I prefer comments in Portuguese, but variables in English"
- ‚úÖ "Use English only for public APIs"

**The AI should confirm the change** and update the conventions documentation.

### ‚ö†Ô∏è Common Exceptions

Regardless of the language choice, **keep in English**:
- ‚úÖ Library and framework names (e.g., `import pandas`, `from flask import`)
- ‚úÖ Language keywords (e.g., `def`, `class`, `if`, `for`)
- ‚úÖ Public API names (if code is distributed internationally)
- ‚úÖ Technical terms without adequate translation (e.g., `callback`, `payload`, `refactoring`)

### üéØ Rationale

**Why ask the programmer?**

1. **Project Context**: National vs. international projects have different needs
2. **Team**: Brazilian team may prefer Portuguese; international team needs English
3. **Readability**: Code is read more times than written - should be clear for maintainers
4. **Consistency**: Defining standard at the start avoids confusing language mixing
5. **Professionalism**: Demonstrates attention to detail and respect for developer preferences

**Why English as recommended for international?**

For international/open-source projects:
- ‚úÖ Universal programming language
- ‚úÖ Easier collaboration with developers worldwide
- ‚úÖ Better integration with English documentation and resources
- ‚úÖ Industry standard for libraries and frameworks

**Why Native Language for national projects?**

For national/regional projects (e.g., Portuguese for Brazil/Portugal):
- ‚úÖ Developers read and understand faster
- ‚úÖ Facilitates onboarding of new team members
- ‚úÖ Documentation and code in same language = less mental translation
- ‚úÖ Variables represent business concepts in native language

**When to prefer English?**

- üåç International open-source project
- üåç Multicultural team
- üåç Product aimed at global market
- üåç Library/framework for public distribution

---

## üìä Recursive Division of Complex Tasks

> **IMPORTANT**: If the task is very long or complex, and there are time limits or response length limits, the artificial intelligence should divide the task into smaller parts, recursively, until achieving a task that can provide a satisfactory response according to the determined response limit.

### üîÑ Division Strategy

**When to Apply**:
- ‚úÖ Task estimated at >4 hours (divide into 2+ sprints)
- ‚úÖ Very long response (>1000 lines of code)
- ‚úÖ Multiple interdependent functionalities
- ‚úÖ Unclear or ambiguous scope
- ‚úÖ Risk of timeout or response limit

**How to Divide** (Recursively):

1. **Level 1 - Division by Functionality**:
   ```
   Large Task: "Complete Authentication System"
   ‚Üì Divide into:
   ‚îú‚îÄ‚îÄ Task 1.1: Basic login (username/password)
   ‚îú‚îÄ‚îÄ Task 1.2: Password recovery
   ‚îú‚îÄ‚îÄ Task 1.3: 2FA (two-factor authentication)
   ‚îî‚îÄ‚îÄ Task 1.4: OAuth/Social login
   ```

2. **Level 2 - Division by Component** (if still too large):
   ```
   Task 1.1: Basic login
   ‚Üì Divide into:
   ‚îú‚îÄ‚îÄ Task 1.1.1: Backend - Authentication API
   ‚îú‚îÄ‚îÄ Task 1.1.2: Frontend - Login form
   ‚îú‚îÄ‚îÄ Task 1.1.3: Validation and security
   ‚îî‚îÄ‚îÄ Task 1.1.4: Unit tests
   ```

3. **Level 3 - Division by Step** (if still too large):
   ```
   Task 1.1.1: Backend - Authentication API
   ‚Üì Divide into:
   ‚îú‚îÄ‚îÄ Task 1.1.1.1: User model (database schema)
   ‚îú‚îÄ‚îÄ Task 1.1.1.2: Password hash (bcrypt)
   ‚îú‚îÄ‚îÄ Task 1.1.1.3: JWT token generation
   ‚îî‚îÄ‚îÄ Task 1.1.1.4: /api/login endpoint
   ```

**Stopping Criteria**:
- ‚è±Ô∏è Task can be completed in <3 hours
- üìù Response fits within reasonable limit (single file, <500 lines)
- ‚úÖ Clear and well-defined scope
- üß™ Can be tested in isolation

**Division Principles**:
1. **Independence**: Each subtask should be as independent as possible
2. **Cohesion**: Related subtasks should be close in sequence
3. **Incremental Value**: Each subtask should add value to the project
4. **Testability**: Each subtask should be testable in isolation

**Practical Example**:
```markdown
‚ùå BAD - Task too large:
[ ] Implement complete task management system (estimated: 20h)

‚úÖ GOOD - Recursively divided:
Sprint 1 (3h):
‚îú‚îÄ‚îÄ [x] Task 1.1: Task model (database schema)
‚îî‚îÄ‚îÄ [x] Task 1.2: Basic CRUD (create/read)

Sprint 2 (3h):
‚îú‚îÄ‚îÄ [ ] Task 2.1: Update and Delete
‚îî‚îÄ‚îÄ [ ] Task 2.2: Filters and search

Sprint 3 (3h):
‚îú‚îÄ‚îÄ [ ] Task 3.1: GUI - Task list
‚îî‚îÄ‚îÄ [ ] Task 3.2: GUI - Edit form

Sprint 4 (2h):
‚îú‚îÄ‚îÄ [ ] Task 4.1: Unit tests
‚îî‚îÄ‚îÄ [ ] Task 4.2: Documentation
```

**Why?**: Dividing large tasks ensures constant progress, avoids timeouts, facilitates debugging, and maintains focus on incremental deliveries.

---

## üéØ When to Use Simplicity 1?

### ‚úÖ Use Simplicity 1 IF:
- ‚úÖ **Solo** project or small team (1-3 devs)
- ‚úÖ **Simple to medium** features
- ‚úÖ **Rapid prototyping** or POC
- ‚úÖ First development of a functionality
- ‚úÖ **Speed** is more important than perfection
- ‚úÖ **Non-critical internal** projects
- ‚úÖ **Learning** new technologies or experimenting
- ‚úÖ **Single-use** scripts or temporary tools

### ‚ùå DO NOT use Simplicity 1 IF:
- ‚ùå **Critical production** application ‚Üí Use **Simplicity 3** (solo) or **Simplicity 2** (team)
- ‚ùå System with **security requirements** (sensitive data, GDPR) ‚Üí Use **Simplicity 3**
- ‚ùå **High impact/risk** features ‚Üí Use **Simplicity 2** or **3**
- ‚ùå **Large teams** (>5 devs) ‚Üí Use **Simplicity 2**
- ‚ùå **Public** library/API ‚Üí Use **Simplicity 2**
- ‚ùå System with critical **performance requirements** ‚Üí Use **Simplicity 2** or **3**

### üîÑ When to Migrate to Other Protocols?
- **‚Üí Simplicity 3**: When an internal project goes into production with real users
- **‚Üí Simplicity 2**: When the team grows to 3+ developers

**Rationale**: Simplicity 1 is **agile and pragmatic** for rapid development, but **lacks critical security layers for production** (security checklist, CI/CD, rollback plans). It's perfect for **learning, prototyping, and iterating quickly**, but should be **upgraded** when the code goes to production or the team grows.

---

## üìã Protocol Backbone (13 Steps)

**Executive Summary**:
1. üìö Read the documentation
2. ‚úÖ Choose the simplest tasks
3. ‚ùì Ask questions until 100% of doubts are clarified
4. üîç Analyze and study the project
5. üéØ Do sprints for the simplest tasks
6. üíª Implement with professional architecture (GoF + GRASP)
7. ‚å®Ô∏è **Verify CLI Implementation + Code Review (9 criteria)**
8. üñ•Ô∏è **Verify GUI Implementation + Code Review (9 criteria)**
9. üîó **Verify Integration with Main Program**
üîü üß™ Run tests (100% coverage)
1Ô∏è‚É£1Ô∏è‚É£ üßπ Organize root folder
1Ô∏è‚É£2Ô∏è‚É£ üìù Fill in documentation
1Ô∏è‚É£3Ô∏è‚É£ üöÄ Commit and push

### 1Ô∏è‚É£ **Read the Documentation**
- Consult `TASKS.md` (or equivalent file defined by the user) to see pending tasks
- Consult `docs/REQUIREMENTS.md` to understand the project context
- Review previous specifications (`v2.9.X-SPECIFICATIONS.md`)
- Understand existing dependencies and architecture
- Check examples in `tests/files/` when applicable

**üìã About the Task File**:

The `TASKS.md` file is the **default file** for managing project tasks, but you can use any ASCII format file (`.txt`, `.md`, etc.) according to your preference.

**Task File Requirements**:
- ‚úÖ **ASCII format mandatory**: `.md`, `.txt` or similar (readable as plain text)
- ‚ùå **NOT accepted**: `.docx`, `.pdf`, or binary formats
- üìç **Location**: Project root or in `docs/` (e.g., `TASKS.md`, `TODO.md`, `requirements.md`)
- üîÑ **Alternative**: If you prefer another name/location, specify at project start

**If no task file exists**:
1. AI should ask the user: "Which file do you use to manage tasks?"
2. If none exists, suggest creating the default `TASKS.md`
3. Confirm file location and name with the user

**Why?**: Avoid rework and ensure consistency with existing code. The task file centralizes project planning and progress.

---

### 2Ô∏è‚É£ **Choose the Simplest Tasks**
- **Golden Rule**: Always start with the tasks **easiest to implement**
- Even in a list of complex tasks, **there are always some simpler than others**
- Proportionality: balance simplicity vs. impact

**Simplicity Criteria**:
- ‚úÖ Fewer dependencies
- ‚úÖ Well-defined and clear scope
- ‚úÖ Fewer files to modify
- ‚úÖ Lower risk of breaking existing functionalities
- ‚úÖ Can be tested in isolation

**Real Example**:
```
List of remaining complex tasks:
[ ] Complex Feature Example (VERY COMPLEX - 50h)
[ ] Semantic AI Search (COMPLEX - 20h)
[ ] Tooltip preview on hover (SIMPLE - 30min) ‚úÖ START HERE!
```

---

### 3Ô∏è‚É£ **Ask More and More Questions to the Programmer**
- **CRITICAL**: Never assume or guess requirements
- Ask **all necessary questions** until **100% of doubts** are clarified
- Validate understanding before starting implementation
- ü§ñ **[NEW v1.9]** The AI **CAN and IS HIGHLY RECOMMENDED** to provide **suggestions and hunches** for answers to each question (optional, but encouraged)

**Recommended Question Format with Suggestions**:
```
‚ùì Question: "How should it behave when [scenario X]?"
üí° AI Suggestion: "Based on existing code, I suggest [option A] because [reason Y]."
Options: A) [option A] | B) [option B] | C) [option C]
```

**Why AI Suggestions Are Important**:
- ‚úÖ Accelerates decisions when the programmer is undecided
- ‚úÖ AI has context of existing code and can suggest consistent patterns
- ‚úÖ Reduces programmer's cognitive load (they just validate, don't create from scratch)
- ‚úÖ Maintains quality: AI suggests based on good practices already implemented

**Question Categories**:
1. **Functional Requirements**:
   - "How should it behave when [scenario X]?"
   - "What happens if the user [action Y]?"
   - "What is the priority between [option A] and [option B]?"

2. **Technical Requirements**:
   - "Should I use [library X] or build from scratch?"
   - "What is the expected output format?"
   - "Are there any performance restrictions?"

3. **Edge Cases**:
   - "What if the file is empty?"
   - "What if there are special characters?"
   - "How to handle None/null values?"

4. **Integration**:
   - "Does it need to integrate with [existing module]?"
   - "Should I maintain compatibility with [previous version]?"
   - "Where should the results be saved?"

5. **Understanding Validation**:
   - "I understand you want [X]. Is that correct?"
   - "My proposed solution is [Y]. Does it make sense?"
   - "Can I start or did I forget something?"

**Example of Requirements Validation**:
```
‚ùì "How many characters/elements should be processed? (default: 30?)"
‚úÖ Answer: "Default can be 30 characters"

‚ùì "Should text normalization be applied (remove accents, convert case)?"
‚úÖ Answer: "Yes, they should be normalized"

‚ùì "How to resolve conflicts when there are duplicates?"
‚úÖ Answer: "Use specific priority criteria (e.g., oldest wins)"
```

**Why?**: Saves time, avoids rework, ensures the solution meets exactly what was requested.

---

### 4Ô∏è‚É£ **Analyze and Study the Project**
- **CRITICAL**: After clarifying all doubts, **study the code before implementing**
- Read relevant documentation (README, docs/, code comments)
- Understand existing architecture and patterns used
- Check necessary dependencies and imports
- Identify reusable functions/classes

**Analysis Checklist**:
1. **Documentation Reading**:
   - `docs/` - General project context and specifications
   - Design and architecture documents
   - `README.md` - Overview and usage instructions
   - Docstrings of related modules

2. **Existing Code Analysis**:
   - Find modules similar to what will be implemented
   - Identify design patterns already used (GoF, GRASP)
   - Check naming conventions and structure
   - Locate reusable helper functions

3. **Dependency Mapping**:
   - Which modules need to be imported?
   - Are there name or version conflicts?
   - Which base classes or mixins should be inherited?
   - Where should new files be created?

4. **Compatibility Validation**:
   - Will the solution break existing code?
   - Is it necessary to refactor anything before implementing?
   - Are there tests that need to be updated?
   - Will the public API be maintained?

**Why?**: Avoid refactoring, save time, ensure code consistent with the existing base.

**Example of Existing Code Analysis**:
```
‚úÖ Analyzed: Similar existing implementations in the project
‚úÖ Identified: Used patterns of base classes and mixins
‚úÖ Verified: Reusable UI widgets and components
‚úÖ Studied: How other modules solve similar problems
‚úÖ Located: Where to add new imports in the main code
‚úÖ Confirmed: Integration structure with existing system
‚Üí Result: Faster and more consistent implementation (60% savings)
```

**Why?**: Avoid refactoring, save time, ensure code consistent with the existing base.

---

### 5Ô∏è‚É£ **Do Sprints for the Simplest Tasks**
- Group 2-4 related tasks into a sprint
- Estimate total time: **maximum 3-4 hours** per sprint
- Maintain focus: **one sprint = one incremental version**

**‚ö†Ô∏è Important - Task Division into Subtasks**:
> Tasks should be divided into smaller parts **only if really necessary**, that is:
> - ‚úÖ When there is **higher probability of exceeding the maximum time** (>4h)
> - ‚úÖ When there is **higher possibility the response will be too long** (complex implementation)
> - ‚ùå **DO NOT divide** if the task is reasonably simple and fits within the time limit
> 
> This decision should be made by the **artificial intelligence responsible for programming** the project, based on the real complexity of the task.

**Sprint Structure**:
```
Sprint vX.Y.Z (Feature Example):
‚îú‚îÄ‚îÄ Task: Feature Implementation (3h estimated)
‚îÇ   ‚îú‚îÄ‚îÄ Subtask 1: Ask questions to the programmer (15min)
‚îÇ   ‚îú‚îÄ‚îÄ Subtask 2: Implement main helper function (45min)
‚îÇ   ‚îú‚îÄ‚îÄ Subtask 3: Implement processing function (45min)
‚îÇ   ‚îú‚îÄ‚îÄ Subtask 4: Integration with existing code (30min)
‚îÇ   ‚îú‚îÄ‚îÄ Subtask 5: Unit tests (60min)
‚îÇ   ‚îî‚îÄ‚îÄ Subtask 6: Documentation (30min)
‚îî‚îÄ‚îÄ Total: 3h45min ‚úÖ
```

---

### 6Ô∏è‚É£ **Implement from Simple to Complex with Professional Architecture**
- **Within each task**, start with the easiest part
- Build incrementally: helper function ‚Üí main function ‚Üí integration
- Test each part before moving on

**Implementation Order**:
1. **Helper functions** (e.g., `extract_all_keys_from_obj()`)
2. **Main functions** (e.g., `build_substitution_map_by_value()`)
3. **Integration** (e.g., update `cli_dedupe()`)
4. **GUI/UX** (if applicable)
5. **Optimizations** (last step)

**Architectural Principles (Mandatory)**:

#### üîÑ **Code Reuse with Modules**
- Create separate modules for each responsibility
- Avoid duplication (DRY - Don't Repeat Yourself)
- Generic functions reusable in multiple contexts

**Example**:
```python
# ‚úÖ GOOD: Reusable module
# src/utils/file_utils.py
def read_file_safe(path: str) -> Optional[str]:
    """Function reused in 10+ places"""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        logger.error(f"Error reading {path}: {e}")
        return None

# ‚ùå BAD: Duplicate code in each module
# (repeats try/except 20 times)
```

#### üì¶ **Hierarchies and Encapsulation**
- Use classes when there is shared state
- Encapsulate private attributes (`_attribute`)
- Expose only necessary public interface

**Example**:
```python
# ‚úÖ GOOD: Proper encapsulation
class ReferenceUpdater:
    def __init__(self, project_dir: str):
        self._project_dir = project_dir
        self._substitutions = {}
    
    def update_references(self) -> Dict[str, int]:
        """Clear public interface"""
        self._scan_files()  # Private method
        self._build_map()   # Private method
        return self._apply_changes()

# ‚ùå BAD: Everything exposed, no structure
def do_everything(dir, old, new, backup, ext):
    # 200 lines without organization
```

#### üéØ **High Cohesion and Low Coupling**
- **High Cohesion**: Each module/class has a single clear responsibility
- **Low Coupling**: Independent modules, communication via interfaces

**Example**:
```python
# ‚úÖ HIGH COHESION: Each class does ONE thing
class KeyExtractor:
    """Only extracts keys from structures"""
    def extract(self, data) -> Dict[str, str]: ...

class SubstitutionMapBuilder:
    """Only builds substitution map"""
    def build(self, old, new) -> Dict[str, str]: ...

class FileUpdater:
    """Only updates files"""
    def update(self, files, map) -> int: ...

# ‚úÖ LOW COUPLING: Communication via interfaces
class ReferenceUpdater:
    def __init__(self, extractor: KeyExtractor, builder: SubstitutionMapBuilder):
        self._extractor = extractor  # Dependency injection
        self._builder = builder

# ‚ùå BAD: Low cohesion, high coupling
class EverythingManager:
    def do_all(self):
        # Does extraction + build + update + logging + GUI
        # Imports 20 different modules
        # Impossible to test in isolation
```

#### üèóÔ∏è **GoF (Gang of Four) Patterns**
Apply design patterns when appropriate:

1. **Strategy Pattern** (algorithm choice at runtime):
```python
class CaseConverter:
    def __init__(self, strategy: CaseStrategy):
        self._strategy = strategy
    
    def convert(self, text: str) -> str:
        return self._strategy.apply(text)

class CamelCaseStrategy(CaseStrategy):
    def apply(self, text: str) -> str: ...

class SnakeCaseStrategy(CaseStrategy):
    def apply(self, text: str) -> str: ...
```

2. **Factory Pattern** (complex object creation):
```python
class ProcessorFactory:
    @staticmethod
    def create(type: str) -> Processor:
        if type == "type_a":
            return ProcessorA()
        elif type == "type_b":
            return ProcessorB()
```

3. **Observer Pattern** (event notification):
```python
class ProcessingModal(QDialog):
    cancel_requested = Signal()  # Observer pattern
    
    def _on_cancel_clicked(self):
        self.cancel_requested.emit()  # Notifies observers
```

4. **Command Pattern** (undo/redo):
```python
class ReplaceCommand:
    def __init__(self, file: str, old: str, new: str):
        self._file = file
        self._old = old
        self._new = new
    
    def execute(self): ...
    def undo(self): ...
```

#### üé® **GRASP (General Responsibility Assignment Software Patterns)**

1. **Information Expert**: Assign responsibility to the one who has the information
```python
# ‚úÖ GOOD: Class has the information, so it has the method
class DataStore:
    def __init__(self, data: dict):
        self._data = data
    
    def get_value(self, key_path: str) -> Optional[str]:
        """Class knows its structure"""
        return self._navigate_path(key_path)

# ‚ùå BAD: External class manipulates internal structure
def get_value_from_data(data_store, key_path):
    # Direct access to the internal dictionary structure
```

2. **Creator**: Class A creates B if A contains/aggregates B
```python
# ‚úÖ GOOD: RewriterDock creates its own widgets
class ComponentB(BaseDock):
    def __init__(self):
        self._create_widgets()  # Creator pattern
        self._setup_layout()
    
    def _create_widgets(self):
        self.ed_input = QLineEdit()  # Creates its children
        self.btn_process = QPushButton()
```

3. **Controller**: Delegate system operations to a controller
```python
# ‚úÖ GOOD: Controller coordinates operations
class RewriterController:
    def process_file(self, path: str):
        data = self._reader.read(path)
        processed = self._processor.process(data)
        self._writer.write(path, processed)

# ‚ùå BAD: GUI does everything directly
class RewriterDock:
    def on_button_click(self):
        # 50 lines of business logic in the GUI
```

4. **Low Coupling**: Minimize dependencies
```python
# ‚úÖ GOOD: Generic interface
def update_references(updater: ReferenceUpdater):
    """Accepts any updater that implements the interface"""
    updater.update()

# ‚ùå BAD: Concrete dependency
def update_references(file_path: str, backup: bool, ext: list):
    """Many parameters, high coupling"""
```

5. **High Cohesion**: One class, one responsibility
```python
# ‚úÖ GOOD: High cohesion
class FileReader:
    """Only reads files"""
    def read(self, path: str) -> str: ...

class DataValidator:
    """Only validates data"""
    def validate(self, data: dict) -> bool: ...

# ‚ùå BAD: Low cohesion
class FileManager:
    def read(self): ...
    def write(self): ...
    def validate(self): ...
    def send_email(self): ...  # ?!
```

**Anti-pattern** ‚ùå:
```python
# DO NOT do everything at once:
def complex_function_with_everything():
    # 500 lines of code
    # Multiple responsibilities
    # Difficult to test
    # High coupling
    # No reuse
```

**Correct Pattern** ‚úÖ:
```python
# Module: src/processor/extractor.py
class DataExtractor:
    """High cohesion: only extracts data"""
    def extract_from_source(self, data) -> Dict[str, str]:
        return self._recurse(data, prefix='item')

# Module: src/processor/transformer.py
class DataTransformer:
    """High cohesion: only transforms data"""
    def transform(self, old, new) -> Dict[str, str]:
        return self._match_values(old, new)

# Module: src/processor/updater.py
class DataUpdater:
    """Low coupling: uses interfaces"""
    def __init__(self, extractor: DataExtractor, transformer: DataTransformer):
        self._extractor = extractor  # Dependency injection
        self._transformer = transformer
    
    def update_project(self, dir: str) -> Dict[str, int]:
        """Coordinates but doesn't implement everything"""
        old = self._extractor.extract(self._read_old())
        new = self._extractor.extract(self._read_new())
        mapping = self._transformer.transform(old, new)
        return self._apply_to_files(dir, mapping)
```

---

### 7Ô∏è‚É£ **Verify CLI Implementation + Code Review**
- **CRITICAL**: Verify that the new functionality is available via **CLI (Command Line Interface)**
- **IMPORTANT**: During verification, apply the **9 Quality Criteria** to the CLI code
- It's not enough to implement GUI, important functionalities must have a **CLI interface** for automation
- Check subcommands, arguments, help text, integration, and code quality

**CLI Implementation Checklist**:

1. **Correct Import in Main File**:
   ```python
   # ‚úÖ Verify if module was imported
   from .modules import (
       ModuleA, ModuleB, ModuleC,
       ModuleD, ModuleE, ModuleF,
       ModuleG, ModuleH, NewModule  # ‚Üê NEW module should be here
   )
   ```

2. **Export in Module's __init__.py**:
   ```python
   # src/modules/__init__.py
   from .new_module import NewModule
   
   __all__ = [
       'ModuleA', 'ModuleB', 'ModuleC',
       'ModuleD', 'ModuleE', 'ModuleF',
       'ModuleG', 'ModuleH', 'NewModule'  # ‚Üê NEW module exported
   ]
   ```

3. **Interface/Menu Item Created and Connected**:
   ```python
   # In _build_interface() or similar
   menu = self.create_menu("Tools")
   
   # Create action
   self.action_new_feature = Action("New Feature", self)
   
   # Add to menu/interface
   menu.add_action(self.action_new_feature)
   
   # Connect signal
   self.action_new_feature.triggered.connect(lambda: self.new_module.execute())
   ```

4. **Dock Initialized in __init__() or setup method**:
   ```python
   # In __init__() of MainWindow
   def __init__(self):
       super().__init__()
       # ... other docks ...
       self._open_new_component()  # ‚Üê Initialize dock
   
   def _open_new_component(self):
       self.dock_new_component = NewComponent(self)
       self.dock_new_component.open_in_other_component_requested.connect(self._load_data_from_source)
       self.addDockWidget(Qt.RightDockWidgetArea, self.dock_new_component)
       self.dock_new_component.hide()
   ```

5. **Signals Connected** (if applicable):
   ```python
   # Connect custom signals
   self.dock_new_component.open_in_other_component_requested.connect(self._load_data_from_source)
   
   def _load_data_from_source(self, data_str: str):
       """Callback to open DATA in editor"""
       if not hasattr(self, 'component_viewer'):
           self._open_component()
       self.component_viewer.load_data_string(data_str)
       self.component_viewer.show()
   ```

6. **i18n Translations Added**:
   ```data
   // src/i18n/en.data
   {
     "menu.tools.text_to_data": "Text to DATA Converter"
   }
   
   // src/i18n/pt_BR.data
   {
     "menu.tools.text_to_data": "Conversor de Texto para DATA"
   }
   ```

**Integration Test Checklist**:
- ‚úÖ **Accessible menu**: Verify if item appears in the Tools menu
- ‚úÖ **Dock opens**: Clicking the menu should open the dock correctly
- ‚úÖ **Basic functionality**: Test simple conversion
- ‚úÖ **Signals work**: Test integration with other components (e.g., Open in Editor)
- ‚úÖ **No console errors**: There should be no ImportError, AttributeError, etc.
- ‚úÖ **Translation working**: Menu in PT-BR should show translated text

**Real Example (Task Example - Text to DATA Converter)**:
```python
‚úÖ Import: from .gui import NewComponent
‚úÖ Export: __all__ = [..., 'NewComponent']
‚úÖ Menu: self.act_open_new_component = QAction(tr("menu.tools.text_to_data"), self)
‚úÖ Init: self._open_new_component() called in __init__()
‚úÖ Signal: open_in_other_component_requested.connect(self._load_data_from_source)
‚úÖ i18n: EN "Text to DATA Converter", PT-BR "Conversor de Texto para DATA"
‚úÖ Test: Menu opens dock, conversion works, signal to editor OK
```

**Questions to Validate Integration**:
1. ‚ùì "Is the new module imported in the main file (app.py)?"
2. ‚ùì "Is the module exported in the folder's __init__.py?"
3. ‚ùì "Is there a menu item to access the functionality?"
4. ‚ùì "Is the menu item connected to the correct method?"
5. ‚ùì "Is the dock/component initialized at application startup?"
6. ‚ùì "Are custom signals connected?"
7. ‚ùì "Are translations added (EN and PT-BR)?"
8. ‚ùì "Is the functionality accessible without errors?"

**Why?**: Ensure that the implemented code is **actually usable** by the end-user, not just "works in isolation".

---

### 8Ô∏è‚É£ **Verify GUI Implementation + Code Review**
- **CRITICAL**: Verify that the components are **integrated into the main program** and accessible
- **IMPORTANT**: During verification, apply the **9 Quality Criteria** to the GUI code
- It's not enough to implement the module/dock, it needs to be **accessible and functional** in the app
- Check menu, imports, initialization, connections, and code quality

**Part A - Functional GUI Verification (Integration)**:

1. **Correct Import in app.py**:
   ```python
   # ‚úÖ Verify if module was imported
   from .gui import (
       ComponentJ, ComponentK, ComponentI,
       ComponentC, ComponentD, ComponentA,
       ComponentB, ComponentF, ComponentG, ComponentH,
       ComponentE, NewComponent  # ‚Üê NEW module should be here
   )
   ```

2. **Export in Module's __init__.py**:
   ```python
   # src/gui/__init__.py
   from .text_to_data_dock import NewComponent
   
   __all__ = [
       'ComponentJ', 'ComponentK', 'ComponentI',
       'ComponentC', 'ComponentD', 'ComponentA',
       'ComponentB', 'ComponentF', 'ComponentG', 'ComponentH',
       'ComponentE', 'NewComponent'  # ‚Üê NEW module exported
   ]
   ```

3. **Menu Item Created and Connected**:
   ```python
   # In _build_menu() or similar
   m_tools = bar.addMenu(tr("menu.tools"))
   
   # Create QAction
   self.act_open_new_component = QAction(tr("menu.tools.text_to_data"), self)
   
   # Add to menu
   m_tools.addAction(self.act_open_new_component)
   
   # Connect signal
   self.act_open_new_component.triggered.connect(lambda: self.dock_new_component.show())
   ```

4. **Dock Initialized in __init__() or setup method**:
   ```python
   # In __init__() of MainWindow
   def __init__(self):
       super().__init__()
       # ... other docks ...
       self._open_new_component()  # ‚Üê Initialize dock
   
   def _open_new_component(self):
       self.dock_new_component = NewComponent(self)
       self.dock_new_component.open_in_other_component_requested.connect(self._load_data_from_source)
       self.addDockWidget(Qt.RightDockWidgetArea, self.dock_new_component)
       self.dock_new_component.hide()
   ```

5. **Signals Connected** (if applicable):
   ```python
   # Connect custom signals
   self.dock_new_component.open_in_other_component_requested.connect(self._load_data_from_source)
   
   def _load_data_from_source(self, data_str: str):
       """Callback to open DATA in editor"""
       if not hasattr(self, 'component_viewer'):
           self._open_component()
       self.component_viewer.load_data_string(data_str)
       self.component_viewer.show()
   ```

6. **i18n Translations Added**:
   ```data
   // src/i18n/en.data
   {
     "menu.tools.text_to_data": "Text to DATA Converter"
   }
   
   // src/i18n/pt_BR.data
   {
     "menu.tools.text_to_data": "Conversor de Texto para DATA"
   }
   ```

**GUI Integration Test Checklist**:
- ‚úÖ **Accessible menu**: Verify if item appears in the Tools menu
- ‚úÖ **Dock opens**: Clicking the menu should open the dock correctly
- ‚úÖ **Basic functionality**: Test simple conversion
- ‚úÖ **Signals work**: Test integration with other components (e.g., Open in Editor)
- ‚úÖ **No console errors**: There should be no ImportError, AttributeError, etc.
- ‚úÖ **Translation working**: Menu in PT-BR should show translated text

**Part B - GUI Code Quality Review (9 Criteria)**:

During GUI verification, simultaneously apply the following criteria:

1. **‚ùå Omission** - Verify if GUI is complete:
   - [ ] All necessary widgets/controls implemented?
   - [ ] Error handling in handlers (e.g., FileNotFoundError)?
   - [ ] Resource cleanup (close files, disconnect signals)?
   - [ ] Visual feedback for long operations (QProgressBar, busy cursor)?

2. **ü§î Ambiguity** - GUI should be clear:
   - [ ] Descriptive and clear labels?
   - [ ] Informative tooltips on controls?
   - [ ] Descriptive error messages (QMessageBox)?
   - [ ] Intuitive method names (_on_button_clicked vs _handle)?

3. **‚ùó Incorrect Fact** - Correct GUI logic:
   - [ ] Signals connected to correct slots?
   - [ ] Correct layouts (QVBoxLayout, QHBoxLayout, QSplitter)?
   
- [ ] Enable/disable controls according to state?
   - [ ] Correct input validation (QValidator)?

4. **‚ôªÔ∏è Redundancy** - Avoid repetition in GUI:
   - [ ] Widgets created only once?
   - [ ] Validations centralized (not duplicated)?
   - [ ] Initialization code not repeated?

5. **‚ö†Ô∏è Inconsistency** - Consistent GUI pattern:
   - [ ] Uniform nomenclature (ed_ for QLineEdit, btn_ for QPushButton)?
   - [ ] Consistent message style?
   - [ ] Consistent layout spacing/margin?

6. **üîó Lack of Integration** - GUI connected:
   - [ ] Dock added to MainWindow?
   - [ ] Menu item connected to dock.show()?
   - [ ] Custom signals connected?
   - [ ] Import present in app.py?

7. **üß© Lower Cohesion** - Dock focused:
   - [ ] Dock only does UI (not business logic)?
   - [ ] Complex logic in separate module?
   - [ ] Each method has a single responsibility?

8. **üîó Higher Coupling** - Decoupled GUI:
   - [ ] Dock does not depend on internal implementation of other docks?
   - [ ] Communication via signals/slots (not direct calls)?
   - [ ] GUI testable independently (mock logic)?

9. **üóëÔ∏è Strange Information** - Clean code:
   - [ ] No forgotten `print()` debugs?
   - [ ] No unresolved TODOs?
   - [ ] No unused widgets?

**Example of Applied GUI Review**:
```python
# ‚ùå BEFORE - Omission, Ambiguity, Higher Coupling
class NewComponent(QDockWidget):
    def __init__(self):
        self.btn = QPushButton("Convert")  # Vague label
        self.btn.clicked.connect(self.convert)  # No error handling
    
    def convert(self):
        data = open(self.ed_file.text()).read()  # No validation, no close
        data_str = my_convert(data)  # Business logic in GUI
        print(data_str)  # Forgotten debug

# ‚úÖ AFTER - Complete, Clear, Decoupled
class NewComponent(BaseDock):
    """Text to DATA Converter dock widget."""
    
    # Signal for communication
    open_in_other_component_requested = Signal(str)
    
    def __init__(self, parent=None):
        super().__init__(parent)
        self._create_widgets()
        self._setup_layout()
        self._connect_signals()
        
        # Controller for business logic
        self._converter = TextToJsonConverter()
    
    def _create_widgets(self):
        """Create UI widgets."""
        self.ed_file = QLineEdit()
        self.ed_file.setPlaceholderText("Enter file path or paste text")
        
        self.btn_convert = QPushButton("Convert to DATA")
        self.btn_convert.setToolTip("Convert text to DATA format")
        
        self.btn_open_component = QPushButton("Open in Editor")
        self.btn_open_component.setEnabled(False)  # Disabled until converted
    
    def _connect_signals(self):
        """Connect signals to slots."""
        self.btn_convert.clicked.connect(self._on_convert_clicked)
        self.btn_open_component.clicked.connect(self._on_open_component_clicked)
    
    def _on_convert_clicked(self):
        """Handle convert button click."""
        file_path = self.ed_file.text().strip()
        
        if not file_path:
            QMessageBox.warning(self, "Empty Input", "Please enter a file path or text.")
            return
        
        try:
            # Read file with context manager (ensures closing)
            if Path(file_path).exists():
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
            else:
                text = file_path  # Treat as direct text
            
            # Convert using controller (decoupling)
            self._data_result = self._converter.convert(text)
            
            # Visual feedback
            QMessageBox.information(self, "Success", "Conversion successful!")
            self.btn_open_component.setEnabled(True)
        
        except FileNotFoundError:
            QMessageBox.critical(self, "File Not Found", f"File not found: {file_path}")
        except Exception as e:
            QMessageBox.critical(self, "Conversion Error", f"Error: {str(e)}")
    
    def _on_open_component_clicked(self):
        """Handle open in editor button click."""
        if hasattr(self, '_data_result'):
            self.open_in_other_component_requested.emit(self._data_result)  # Signal
```

**Recommended GUI Tools**:
```bash
# Check unused Qt imports
grep -r "from PySide6" src/gui/ | cut -d: -f2 | sort | uniq

# Check unconnected signals (manual review)
grep -r "Signal(" src/gui/ | grep -v ".connect("

# Check unused widgets (manual review)
grep -r "self\.\w\+ = Q" src/gui/

# Check debug prints (CRITICAL)
grep -r "print(" src/gui/ --exclude="*_test.py"
```

**Questions to Validate GUI**:
1. ‚ùì "Is the dock fully integrated into the menu and MainWindow?"
2. ‚ùì "Are all signals connected and working?"
3. ‚ùì "Is there error handling with visual feedback (QMessageBox)?"
4. ‚ùì "Is business logic separated from GUI code?"
5. ‚ùì "Is the code free of debug prints and unresolved TODOs?"
6. ‚ùì "Are labels, tooltips, and messages clear and descriptive?"
7. ‚ùì "Are resources (files, connections) closed correctly?"

**Real Example (Task Example - Text to DATA Converter)**:
```python
‚úÖ Import: from .gui import NewComponent
‚úÖ Export: __all__ = [..., 'NewComponent']
‚úÖ Menu: self.act_open_new_component.triggered.connect(lambda: self.dock_new_component.show())
‚úÖ Init: self._open_new_component() called in __init__()
‚úÖ Signal: open_in_other_component_requested.connect(self._load_data_from_source)
‚úÖ i18n: EN "Text to DATA Converter", PT-BR "Conversor de Texto para DATA"
‚úÖ Review: No debug prints, error handling OK, decoupled logic
‚úÖ Test: Menu opens dock, conversion works, signal to editor OK
```

---

### 9Ô∏è‚É£ **Verify Integration with Main Program**
- **CRITICAL**: After implementing CLI and GUI, **verify that everything is integrated and working in the context of the main program**
- It's not enough to have code working in isolation, it needs to be **accessible and operational** in the application
- Check full flow: menu ‚Üí action ‚Üí result
- Manually test the functionality in the running program

**Complete Integration Checklist**:

1. **Full GUI Flow Test**:
   ```bash
   # Start application
   python -m app --gui
   
   # Manually test:
   [ ] Does the menu item appear correctly?
   [ ] Does clicking the menu open the dock?
   [ ] Does the dock display all controls?
   [ ] Does basic functionality work (conversion, search, etc)?
   [ ] Do signals between components work (e.g., "Open in Editor")?
   [ ] Do error messages appear when appropriate?
   [ ] Does i18n translation work (change language and verify)?
   ```

2. **Full CLI Flow Test**:
   ```bash
   # Test help
   python -m app convert --help
   
   # Test functionality
   python -m app convert test.txt --pretty -o output.data
   
   # Test pipes
   echo "name: John" | python -m app convert -
   
   # Verify:
   [ ] Does help text appear?
   [ ] Are arguments recognized?
   [ ] Does functionality execute without errors?
   [ ] Is the output correct?
   [ ] Are exit codes correct (0=success, 1=error)?
   ```

3. **Inter-Component Integration Test**:
   ```bash
   # Example: Convert text ‚Üí Open in editor
   [ ] Does clicking "Open in Editor" in the Text to DATA Converter open the Editor?
   [ ] Is DATA correctly loaded in the Editor?
   [ ] Can the Editor save the result?
   
   # Example: Search ‚Üí Open file
   [ ] Does clicking a search result open the correct file?
   [ ] Does the cursor position go to the correct line?
   ```

4. **Robustness Test**:
   ```bash
   # Error scenarios
   [ ] Does "File not found" display a clear message?
   [ ] Is invalid input handled gracefully?
   [ ] Does a cancelled operation leave no inconsistent state?
   [ ] Are resources released correctly (files closed, memory)?
   ```

5. **Performance Test** (if applicable):
   ```bash
   # Large files
   [ ] Processes files >10MB without freezing?
   [ ] Interface remains responsive during long operation?
   [ ] Progress bar/visual feedback works?
   [ ] Cancellation works during long operation?
   ```

**Real Example of Integration Problem**:
```python
# ‚ùå PROBLEM FOUND IN INTEGRATION:
# Task Example - Text to DATA Converter CLI
# Problem: Extractor() was called without 3 mandatory parameters

# BEFORE (broke on integration):
def main():
    if args.command == 'convert':
        extractor = Extractor()  # ‚ùå TypeError: missing 3 required arguments

# AFTER (fixed):
def main():
    if args.command == 'convert':
        extractor = Extractor(
            avoid_keys="",
            avoid_keys_parameter="equals",
            with_quotation_marks=False
        )  # ‚úÖ Works!
```

**Questions to Validate Integration**:
1. ‚ùì "Can the end-user easily access the functionality?"
2. ‚ùì "Do all usage flows work end-to-end?"
3. ‚ùì "Are there any errors or warnings in the console during normal use?"
4. ‚ùì "Is the functionality consistent with the rest of the application?"
5. ‚ùì "Is the documentation (help text, tooltips) clear and correct?"

**Why is this step critical?**:
- ‚úÖ Detects problems that unit tests don't catch
- ‚úÖ Validates real user experience
- ‚úÖ Ensures all work is truly usable
- ‚úÖ Avoids surprises after commit (tested code ‚â† integrated code)

---

### üîü **Run Tests**
- **Mandatory**: Unit tests for each public function
- **Goal**: 100% coverage of implemented functionalities
- **Tools**: `unittest` (native) or `pytest`
- **CRITICAL**: Test the system **after integration** (integrated GUI + CLI)
- **IMPORTANT**: Execute **AFTER** code review (Steps 7 and 8)

**Test Categories**:
1. **Happy Path**: Normal use cases
2. **Edge Cases**: Empty values, None, long strings
3. **Error Handling**: Expected exceptions
4. **Integration**: Full flow (including GUI/CLI integration)
5. **Quality Validation**: Tests that validate the absence of the 9 problems from Steps 7 and 8

**Example Test Suite**:
```python
‚úÖ test_basic_functionality()
‚úÖ test_with_valid_input()
‚úÖ test_edge_case_empty()
‚úÖ test_edge_case_large_input()
‚úÖ test_error_handling()
‚úÖ test_integration_complete_flow()
# ... tests covering normal cases, edge cases, and integration
```

**Why test AFTER integration and review?**:
- Ensures that tests validate the **integrated system**, not isolated components
- Detects integration problems during tests
- Validates that features actually work in the application context
- Avoids false positives (tests pass but feature is not accessible)
- Code has already been reviewed, so tests validate **quality code**

**Why?**: Ensure quality, avoid regressions, facilitate future maintenance.

---

#### üõ°Ô∏è **Step 9.1 - Security in Tests (CRITICAL)**

**Common Problem in Tests**:
- GUI tests can get stuck in an **infinite loop** without timeout
- Lack of automatic deadlock or freeze detection
- Tests wait for unavailable resources (e.g., X11 display in a headless environment)

**Mandatory Solutions**:

1. **‚è±Ô∏è Mandatory Maximum Timeout** (30s per test):
   ```bash
   # ALWAYS use timeout in tests
   pytest tests/test_*.py --timeout=30 -v
   
   # Install pytest-timeout plugin if necessary
   pip install pytest-timeout
   ```

2. **üö® Infinite Loop Detection** (warning in 10s):
   ```bash
   # More aggressive timeout to detect loops
   timeout 10s pytest tests/test_specific.py || echo "‚ö†Ô∏è TIMEOUT: Possible infinite loop detected!"
   ```

3. **üñ•Ô∏è Mandatory Headless Environment** (GUI tests without display):
   ```bash
   # Use Qt offscreen platform
   QT_QPA_PLATFORM=offscreen pytest tests/test_gui_*.py -v --timeout=30
   
   # OR use pytest-xvfb for virtual X11 environment
   pip install pytest-xvfb
   pytest tests/test_gui_*.py --xvfb-backend xvfb --timeout=30
   ```

4. **‚úÖ Mandatory Dry-Run** (before executing):
   ```bash
   # 1. Verify syntax
   python -m py_compile tests/test_*.py && echo "‚úÖ Valid syntax"
   
   # 2. Verify imports
   python -c "from tests.test_module import *; print('‚úÖ Imports OK')"
   
   # 3. List tests without executing
   pytest tests/test_*.py --collect-only
   ```

5. **‚è≤Ô∏è Time Monitoring** (record duration):
   ```bash
   # Measure total time and save log
   time pytest tests/test_*.py -v --timeout=30 | tee test_output.log
   
   # Use pytest-benchmark for metrics
   pytest tests/test_*.py --benchmark-only --timeout=30
   ```

**Why?**: Avoid infinite freezes, protect development time, ensure reliable tests.

---

### 1Ô∏è‚É£1Ô∏è‚É£ **Organize Project Root Folder**
- ‚úÖ Imports validated (module loads without errors)
- üìù **Documented limitation**: GUI tests require an unconfigured headless environment

---

#### üî¨ **Step 9.2 - Tests in Threads/Processes with Monitoring (ADVANCED)**

**Objective**: Full control over test execution with the possibility to **interrupt**, **monitor**, and **log** progress in real-time.

**When to Use**:
- GUI tests that may freeze
- Long-running tests (>1 min)
- Tests with external dependencies (network, database)
- Need for real-time logging
- Need for manual cancellation during execution

**Implementation with `multiprocessing.Process`**:

```python
# tests/test_runner_monitored.py
import multiprocessing as mp
import time
import sys
from queue import Empty

def run_tests_in_process(test_module: str, queue: mp.Queue, timeout: int = 30):
    """
    Executes tests in a separate process with logging to a queue.
    
    Args:
        test_module: Test module (ex: 'tests.test_file_list_dock')
        queue: Queue for progress communication
        timeout: Timeout in seconds
    """
    try:
        import pytest
        
        # Configure real-time logging
        class QueueReporter:
            def __init__(self, queue):
                self.queue = queue
            
            def pytest_runtest_logreport(self, report):
                """pytest hook to capture results."""
                if report.when == 'call':
                    status = '‚úÖ PASS' if report.passed else '‚ùå FAIL'
                    self.queue.put({
                        'type': 'test_result',
                        'test': report.nodeid,
                        'status': status,
                        'duration': report.duration
                    })
        
        # Execute pytest with custom reporter
        queue.put({'type': 'info', 'msg': f'Starting tests: {test_module}'})
        
        result = pytest.main([
            test_module,
            '-v',
            f'--timeout={timeout}',
            '--tb=short',
            '-p', 'no:cacheprovider'  # Disable cache
        ])
        
        queue.put({'type': 'info', 'msg': f'Tests finished. Exit code: {result}'})
        queue.put({'type': 'exit', 'code': result})
        
    except Exception as e:
        queue.put({'type': 'error', 'msg': str(e)})
        queue.put({'type': 'exit', 'code': 1})

def monitor_test_execution(test_module: str, max_timeout: int = 300):
    """
    Monitors test execution with full control.
    
    Args:
        test_module: Test module
        max_timeout: Maximum timeout in seconds (default: 5 min)
    
    Returns:
        dict: Execution result with statistics
    """
    queue = mp.Queue()
    process = mp.Process(
        target=run_tests_in_process,
        args=(test_module, queue, 30)
    )
    
    print(f"üöÄ Starting tests: {test_module}")
    print(f"‚è±Ô∏è  Maximum timeout: {max_timeout}s")
    print(f"üìä Active monitoring. Press Ctrl+C to cancel.\n")
    
    process.start()
    start_time = time.time()
    results = {'passed': 0, 'failed': 0, 'tests': []}
    
    try:
        while process.is_alive():
            elapsed = time.time() - start_time
            
            # Check global timeout
            if elapsed > max_timeout:
                print(f"\n‚ö†Ô∏è  GLOBAL TIMEOUT ({max_timeout}s exceeded)")
                process.terminate()
                process.join(timeout=5)
                if process.is_alive():
                    process.kill()
                return {'status': 'timeout', 'elapsed': elapsed, 'results': results}
            
            # Read messages from the queue (non-blocking)
            try:
                msg = queue.get(timeout=0.5)
                
                if msg['type'] == 'test_result':
                    print(f"  {msg['status']} {msg['test']} ({msg['duration']:.2f}s)")
                    results['tests'].append(msg)
                    if '‚úÖ' in msg['status']:
                        results['passed'] += 1
                    else:
                        results['failed'] += 1
                
                elif msg['type'] == 'info':
                    print(f"‚ÑπÔ∏è  {msg['msg']}")
                
                elif msg['type'] == 'error':
                    print(f"‚ùå ERROR: {msg['msg']}")
                
                elif msg['type'] == 'exit':
                    process.join(timeout=2)
                    elapsed = time.time() - start_time
                    print(f"\n‚úÖ Tests finished in {elapsed:.2f}s")
                    return {
                        'status': 'completed',
                        'exit_code': msg['code'],
                        'elapsed': elapsed,
                        'results': results
                    }
            
            except Empty:
                # No message, continue monitoring
                pass
            
            # Show progress every 10s
            if int(elapsed) % 10 == 0 and int(elapsed) > 0:
                print(f"‚è≥ Executing... {int(elapsed)}s ({results['passed']} passed, {results['failed']} failed)")
    
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Manual cancellation (Ctrl+C)")
        process.terminate()
        process.join(timeout=5)
        if process.is_alive():
            process.kill()
        elapsed = time.time() - start_time
        return {'status': 'cancelled', 'elapsed': elapsed, 'results': results}
    
    finally:
        if process.is_alive():
            process.terminate()
            process.join(timeout=5)

# Example usage:
if __name__ == '__main__':
    result = monitor_test_execution('tests/test_advanced_file_search.py', max_timeout=300)
    
    print(f"\n{'='*60}")
    print(f"Status: {result['status']}")
    print(f"Time: {result['elapsed']:.2f}s")
    print(f"Passed: {result['results']['passed']}")
    print(f"Failed: {result['results']['failed']}")
    print(f"{'='*60}")
```

**Practical Use**:

```bash
# 1. Create monitored runner
cat > tests/run_tests_monitored.py << 'EOF'
# [code above]
EOF

# 2. Execute with monitoring
python tests/run_tests_monitored.py

# 3. Cancel at any time (Ctrl+C)
# The process will be terminated gracefully
```

**Advantages**:
- ‚úÖ **Full control**: Can cancel tests at any time
- ‚úÖ **Real-time logging**: See progress of each test
- ‚úÖ **Global + individual timeout**: Double protection
- ‚úÖ **Statistics**: Pass/fail in real time
- ‚úÖ **Isolation**: Tests run in a separate process (don't freeze the terminal)
- ‚úÖ **Guaranteed cleanup**: `terminate()` + forced `kill()` if necessary

**Optional Configurations**:

1. **File Logging** (in addition to stdout):
   ```python
   # Add to run_tests_in_process:
   import logging
   logging.basicConfig(
       filename=f'test_{time.time()}.log',
       level=logging.INFO,
       format='%(asctime)s - %(message)s'
   )
   ```

2. **Sound Notification** (upon completion):
   ```python
   import os
   # At the end of monitor_test_execution:
   os.system('paplay /usr/share/sounds/freedesktop/stereo/complete.oga')
   ```

3. **CI/CD Integration**:
   ```python
   # Return correct exit code:
   sys.exit(0 if result['status'] == 'completed' and result['results']['failed'] == 0 else 1)
   ```

**Additional Checklist (Step 9.2 - Optional)**:
```
[ ] Create test_runner_monitored.py with multiprocessing
[ ] Define global timeout (default: 5 min)
[ ] Define individual timeout per test (default: 30s)
[ ] Implement real-time logging (Queue)
[ ] Test manual cancellation (Ctrl+C)
[ ] Verify process cleanup (ps aux | grep pytest)
```

**When NOT to use**:
- Simple and fast tests (<10s total)
- Tests without GUI (pure backend)
- CI/CD with native timeout configured
- First execution of tests (unnecessary overhead)

---

### 1Ô∏è‚É£1Ô∏è‚É£ **Organize Project Root Folder**
- **CRITICAL**: Before documentation and commit, **organize the root folder recursively**
- **MANDATORY**: Files must be organized in the correct folders before commit
- Remove temporary files, unnecessary backups
- Verify all files are in the correct places
- Clear cache and generated files (`__pycache__`, `.pyc`)
- Ensure `.gitignore` is updated

**Organization Checklist**:
1. **Removal of Temporary Files**:
   ```bash
   # Remove old backups
   rm -f *.backup_* *.bak *~
   
   # Clear Python cache
   find . -type d -name "__pycache__" -exec rm -rf {} +
   find . -type f -name "*.pyc" -delete
   find . -type f -name "*.pyo" -delete
   ```

2. **Directory Structure Verification (MANDATORY)**:
   - `src/` - source code
   - `tests/` - **ALL test files** (mandatory)
   - `docs/` - **ALL documents and markdown files** (mandatory)
   - Organized root files (README, setup.py, etc.)

3. **Mandatory Recursive Organization**:
   
   **‚ö†Ô∏è FUNDAMENTAL RULE**: 
   > Before commit, files must be organized in folders recursively. This is **mandatory** to keep the environment clean and organized.

   **Specific Rules by File Type**:
   
   a) **Test Files** ‚Üí `tests/`
      - ‚úÖ `test_*.py`, `*_test.py` ‚Üí `tests/`
      - ‚úÖ Test structure should mirror code structure
      - ‚úÖ Example: `tests/unit/`, `tests/integration/`, `tests/fixtures/`
   
   b) **Documents and Markdown** ‚Üí `docs/`
      - ‚úÖ All `.md` files (except root README.md) ‚Üí `docs/`
      - ‚úÖ Documentation files ‚Üí `docs/`
      - ‚úÖ **Recursive organization within `docs/`**:
        - `docs/api/` - API documentation
        - `docs/tutorials/` - Tutorials
        - `docs/architecture/` - Architectural decisions
        - `docs/user-guide/` - User guides
        - `docs/dev-guide/` - Development guides
      - ‚úÖ Create subfolders that identify file context
   
   c) **Source Code** ‚Üí `src/` or appropriate folder
      - ‚úÖ Organize by modules/features
      - ‚úÖ Example: `src/core/`, `src/utils/`, `src/api/`

**Complete Example**:
```bash
# BEFORE (disorganized):
‚îú‚îÄ‚îÄ src/
‚îú‚îÄ‚îÄ test_utils.py              ‚ùå test outside tests/
‚îú‚îÄ‚îÄ API_DOCS.md                ‚ùå doc outside docs/
‚îú‚îÄ‚îÄ tutorial.md                ‚ùå doc outside docs/
‚îú‚îÄ‚îÄ apply_v2913_patches.py     ‚ùå temporary
‚îú‚îÄ‚îÄ test_temp.py               ‚ùå temporary test
‚îú‚îÄ‚îÄ backup_old/                ‚ùå old backup
‚îú‚îÄ‚îÄ __pycache__/               ‚ùå cache
‚îî‚îÄ‚îÄ file.py.backup_v2913       ‚ùå unnecessary backup

# AFTER (recursively organized):
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îú‚îÄ‚îÄ tests/                     ‚úÖ ALL tests
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py     ‚úÖ test moved
‚îÇ   ‚îî‚îÄ‚îÄ integration/
‚îú‚îÄ‚îÄ docs/                      ‚úÖ ALL documents
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ API_DOCS.md       ‚úÖ doc moved
‚îÇ   ‚îî‚îÄ‚îÄ tutorials/
‚îÇ       ‚îî‚îÄ‚îÄ tutorial.md       ‚úÖ doc moved
‚îî‚îÄ‚îÄ README.md                  ‚úÖ root README kept
```

**Why?**: Keep repository clean, avoid committing junk, facilitate navigation, professionalism, recursive organization ensures scalability. Document the **clean** and **organized** state of the project.

---

### 1Ô∏è‚É£2Ô∏è‚É£ **Fill in New Documentation**
- **Update tasks/requirements file**: Mark tasks as `[X]` complete
- **Create SPECIFICATIONS.md**: Detailed version document
- **Update statistics**: Project completion percentage
- **ü§ñ [OPTIONAL] Manage AI task recommendations**

**üìã TASKS.md Management**:

**General Rule**:
- If a tasks/requirements file exists (e.g., `TASKS.md`, `TODO.md`, `requirements.md`):
  - ‚úÖ **Mark tasks as complete** after implementation: `[ ]` ‚Üí `[X]`
  - ‚úÖ **Update statistics** (percentages, counters)
  - ‚úÖ **Add completion notes** (date, version, brief description)
  - ü§ñ **[OPTIONAL] Add new AI-recommended tasks** (see section below)
  
- If a tasks/requirements file **DOES NOT exist**:
  - ‚ùì **Ask the user** for the file location/path
  - ‚ùì **Ask about next tasks and requirements** if no formal document exists
  - ‚ùì **Suggest creating** `TASKS.md` as the default file

---

### üìä **Task Classification Legend**

**Objective**: Standardize task classification and prioritization to facilitate AI organization and understanding between different artificial intelligence systems.

#### **Task Status**

Tasks should be marked with status indicators for visual tracking:

- üî¥ **Not Started** - Awaiting start, no work done
- üü° **In Progress** - Active development, work underway
- üü¢ **Done** - Implemented, tested and completed
- üîµ **Blocked** - Impeded by external dependency or technical issue

**Usage example**:
```markdown
- üî¥ [ ] Implement OAuth2 authentication
- üü° [ ] Add form validation (50% complete)
- üü¢ [x] Configure PostgreSQL database
- üîµ [ ] Production deployment (awaiting infra approval)
```

#### **Task Complexity**

Classification based on estimated time, risk and number of dependencies:

- üü¢ **Simple** (0-1h) - Low risk, few dependencies, clear and well-defined scope
  - Examples: Adjust text, fix typo, add tooltip, small bugfix
  - Characteristics: Modification of 1-2 files, no impact on other modules
  
- üü° **Medium** (1-2h) - Medium risk, some integrations, may require additional tests
  - Examples: New simple feature, module refactoring, API integration
  - Characteristics: Modification of 3-5 files, some integration with existing system
  
- üî¥ **Complex** (>2h) - High risk, many dependencies, open or ambiguous scope
  - Examples: New architecture, database migration, critical feature with many edge cases
  - Characteristics: Multiple affected files, high algorithmic complexity, requires research

**Usage example**:
```markdown
## Backlog by Complexity

### üü¢ Simple Tasks (0-1h)
- [ ] Add loading spinner to submit button
- [ ] Fix header alignment

### üü° Medium Tasks (1-2h)
- [ ] Implement pagination in listing
- [ ] Add advanced search filters

### üî¥ Complex Tasks (>2h)
- [ ] Migrate authentication to SSO
- [ ] Implement distributed cache system
```

#### **MoSCoW Prioritization**

Framework for classifying the relative importance of each task:

- üî¥ **Must Have** - Critical for system functionality, release blocker
  - Without this, the product doesn't work or doesn't meet fundamental requirement
  - Examples: Login, data saving, product core functionality
  
- üü° **Should Have** - Important but not blocking, can be postponed if needed
  - Adds significant value but system works without it
  - Examples: Report export, email notifications, dark mode
  
- üü¢ **Could Have** - Desirable if time permits, low priority
  - Nice to have, improves experience but not essential
  - Examples: Animations, easter eggs, experimental features
  
- ‚ö™ **Won't Have** (Later) - Explicitly out of current scope, for future versions
  - Good idea but not for now, document for future backlog
  - Examples: Mobile app version, legacy system integration

**Usage example**:
```markdown
## MoSCoW Prioritization - Sprint v1.0

### üî¥ MUST HAVE (Required)
- [ ] Functional authentication system
- [ ] Complete user CRUD
- [ ] Data persistence

### üü° SHOULD HAVE (Important)
- [ ] Password recovery
- [ ] Email validation
- [ ] Audit logs

### üü¢ COULD HAVE (Desirable)
- [ ] Customizable avatar
- [ ] Dark theme
- [ ] Keyboard shortcuts

### ‚ö™ WON'T HAVE (Future)
- [ ] Social media integration
- [ ] Native mobile app
```

#### **Advanced Prioritization Frameworks (OPTIONAL)**

For complex projects requiring more sophisticated quantitative analysis:

##### **RICE Matrix** (Reach, Impact, Confidence, Effort)

Score: `RICE Score = (Reach √ó Impact √ó Confidence) / Effort`

- **Reach** (Reach): How many people will be impacted? (e.g., 100 users/month)
- **Impact** (Impact): How much impact per person? (0.25=minimal, 3=massive)
- **Confidence** (Confidence): How certain are we? (50%=low, 100%=high)
- **Effort** (Effort): How many person-hours? (e.g., 2h, 10h, 40h)

**Example**:
```markdown
| Task | Reach | Impact | Confidence | Effort | RICE Score |
|------|-------|--------|------------|--------|-----------|
| Feature A | 1000 | 3 | 100% | 5h | 600 |
| Feature B | 500 | 2 | 80% | 10h | 80 |
| Feature C | 100 | 1 | 50% | 2h | 25 |

Priority: A > B > C
```

##### **Eisenhower Matrix** (Urgent vs Important)

Classification in quadrants for time management:

- ‚≠ê **Q1: Urgent + Important** ‚Üí Do IMMEDIATELY
  - Crises, critical production bugs, imminent deadlines
  
- üìÖ **Q2: Not Urgent + Important** ‚Üí SCHEDULE and do later
  - Strategic planning, refactoring, documentation, tests
  
- üîÄ **Q3: Urgent + Not Important** ‚Üí DELEGATE or automate
  - Interruptions, some meetings, non-critical emails
  
- üóëÔ∏è **Q4: Not Urgent + Not Important** ‚Üí ELIMINATE
  - Distractions, tasks that don't add real value

**Example**:
```markdown
## Eisenhower Matrix - Current Sprint

### ‚≠ê Q1: DO NOW (Urgent + Important)
- [ ] üî¥ Fix reported security bug
- [ ] üî¥ Implement blocking feature for client

### üìÖ Q2: SCHEDULE (Important + Not Urgent)
- [ ] üü° Refactor authentication module
- [ ] üü° Write technical documentation
- [ ] üü° Implement missing unit tests

### üîÄ Q3: DELEGATE (Urgent + Not Important)
- [ ] üü¢ Respond to stakeholder emails
- [ ] üü¢ Update status report

### üóëÔ∏è Q4: ELIMINATE (Not Urgent + Not Important)
- [ ] ‚ö™ Research new library X (not needed now)
```

#### **Combining Indicators**

For maximum clarity, combine status + complexity + prioritization:

```markdown
## Sprint v2.3 - Organized Backlog

### üî¥ MUST HAVE
- üî¥üü¢ [ ] Add logout button (Not Started, Simple, 0.5h)
- üü°üü° [ ] Implement password reset (In Progress, Medium, 1.5h, 60% complete)
- üü¢üü¢ [x] Configure HTTPS (Done, Simple, 1h)
- üîµüî¥ [ ] Migrate to PostgreSQL (Blocked, Complex, 4h, awaiting DBA)

### üü° SHOULD HAVE  
- üî¥üü° [ ] Add search filters (Not Started, Medium, 2h)
- üü°üü¢ [ ] Loading states (In Progress, Simple, 0.5h)

### üü¢ COULD HAVE
- üî¥üü° [ ] Dark mode (Not Started, Medium, 1.5h)
```

**Combined Indicators Interpretation**:
- **First emoji** = Status (üî¥ Not Started, üü° In Progress, üü¢ Done, üîµ Blocked)
- **Second emoji** = Complexity (üü¢ Simple, üü° Medium, üî¥ Complex)
- **Section** = MoSCoW Priority (Must/Should/Could/Won't)

#### **Recommendations for AI**

**When classifying tasks, AI should**:
1. ‚úÖ **Start with simplest tasks** within each priority category
2. ‚úÖ **Consider dependencies** before marking as "Blocked"
3. ‚úÖ **Update status** proactively as progress is made
4. ‚úÖ **Use MoSCoW** to define sprint/release scope
5. ‚úÖ **Apply RICE/Eisenhower** when there are 10+ tasks to prioritize
6. ‚úÖ **Balance complexity**: Don't accumulate only complex tasks in backlog
7. ‚úÖ **Be consistent**: Maintain same classification pattern throughout project

**Example of AI decision**:
```
Scenario: 15 tasks in backlog, all "MUST HAVE"

AI Decision:
1. Filter by complexity ‚Üí Identify 5 simple, 7 medium, 3 complex
2. Order by dependencies ‚Üí 2 tasks are blocked
3. Calculate RICE score ‚Üí Prioritize the 3 with highest impact/effort
4. Suggest order: Start with 3 simple + 2 independent medium tasks
5. Leave 3 complex for later (when team is warmed up)
```

**When to use each framework**:
- **Only Status + Complexity**: Small projects (< 20 tasks)
- **+ MoSCoW**: Medium projects, define release scope
- **+ RICE**: When multiple features compete for limited resources
- **+ Eisenhower**: When there's time pressure and many false "urgencies"
- **Decision Matrix (Step 2.5 of Simplicity 2/3)**: When choice between tasks isn't obvious

---

### ü§ñ **AI Task Recommendations (OPTIONAL)**

**When to Use**:
- ‚úÖ After completing implementations or sprints
- ‚úÖ When the project is evolving and can benefit from new functionalities
- ‚úÖ To identify improvement opportunities and refine requirements
- ‚ùå DO NOT use for disposable projects or temporary prototypes

**Initial Question to User** (ask ONCE at project start):
```
‚ùì Would you like AI to dynamically recommend new tasks in TASKS.md 
   as the project evolves?
   
Options:
A) ‚úÖ Yes, add recommendations from time to time
B) ‚ùå No, maintain only tasks I define manually
C) üî¢ Yes, but with a maximum limit of [X] new tasks (default: 30)
```

**If user accepts (option A or C)**:

#### **Recommendation Dynamics (Quadratic Curve)**

AI should follow a recommendation pattern that **grows, reaches a peak, and then decreases**:

```
AI-Recommended Tasks Throughout the Project:

Project Start (0-20% complete):
‚îú‚îÄ‚îÄ üü¢ PHASE 1: INITIAL GROWTH (0-5 tasks)
‚îÇ   ‚îú‚îÄ‚îÄ Recommendations: Few and essential
‚îÇ   ‚îú‚îÄ‚îÄ Focus: Establish solid project foundation
‚îÇ   ‚îî‚îÄ‚îÄ Examples: CI/CD setup, test structure, basic documentation

Early Development (20-40% complete):
‚îú‚îÄ‚îÄ üü¢ PHASE 2: ACCELERATION (5-15 tasks)
‚îÇ   ‚îú‚îÄ‚îÄ Recommendations: Gradually increasing
‚îÇ   ‚îú‚îÄ‚îÄ Focus: Main features, important integrations
‚îÇ   ‚îî‚îÄ‚îÄ Examples: Essential APIs, core features, UX improvements

Mid Development (40-70% complete):
‚îú‚îÄ‚îÄ üü° PHASE 3: MAXIMUM PEAK (15-30 total tasks)
‚îÇ   ‚îú‚îÄ‚îÄ Recommendations: Maximum ideas and opportunities
‚îÇ   ‚îú‚îÄ‚îÄ Focus: Polishing, secondary features, optimizations
‚îÇ   ‚îî‚îÄ‚îÄ Examples: Performance tuning, accessibility, i18n, analytics

Late Development (70-90% complete):
‚îú‚îÄ‚îÄ üü† PHASE 4: DECELERATION (10-15 remaining tasks)
‚îÇ   ‚îú‚îÄ‚îÄ Recommendations: Decreasing, only critical
‚îÇ   ‚îú‚îÄ‚îÄ Focus: Finalization, bugfixes, stability
‚îÇ   ‚îî‚îÄ‚îÄ Examples: Edge cases, integration tests, final documentation

Final Stage (90-100% complete):
‚îî‚îÄ‚îÄ üî¥ PHASE 5: EXHAUSTION (0-5 final tasks)
    ‚îú‚îÄ‚îÄ Recommendations: STOP adding new features
    ‚îú‚îÄ‚îÄ Focus: Release readiness, final review
    ‚îî‚îÄ‚îÄ Examples: Only critical adjustments or blocking bugfixes
```

**Curve Formula** (for AI implementers):
```
num_recommended_tasks = -4 * (progress - 0.5)¬≤ + 30
where:
- progress = completion percentage (0.0 to 1.0)
- num_recommended_tasks = cumulative total of recommended tasks
- Peak maximum at ~50% project completion (30 tasks if default not changed)
```

#### **Limits and Controls**

**Configurable Maximum Limit**:
- üìä **Default**: 30 new tasks/ideas recommended by AI
- ‚öôÔ∏è **Configurable**: User can specify another value (e.g., 10, 50, 100)
- üî¢ **Question**: "What is the maximum number of tasks AI can recommend? (default: 30)"

**Scope Control**:
```markdown
### ‚úÖ CRITERIA for AI Recommendations

1. **Within Scope**:
   - ‚úÖ Aligned with project theme/purpose
   - ‚úÖ Based on user feedback (real or simulated)
   - ‚úÖ Improvement of existing requirements
   - ‚úÖ Product professionalism and quality

2. **OUT of Scope** (DO NOT recommend):
   - ‚ùå Features unrelated to main theme
   - ‚ùå "Cool but unnecessary" ideas (feature creep)
   - ‚ùå Unjustified technologies/frameworks
   - ‚ùå Generic recommendations without project context

3. **Prioritization**:
   - üî¥ MUST HAVE: Critical for the project
   - üü° SHOULD HAVE: Important but not blocking
   - üü¢ COULD HAVE: Nice to have, low priority
   - ‚ö™ WON'T HAVE: Explicitly out of scope
```

#### **Recommendation Format in TASKS.md**

```markdown
## ü§ñ AI-Recommended Tasks

_These tasks were suggested by AI based on project progress and 
user feedback. Review and approve before implementing._

### üî¥ MUST HAVE (Critical)
- [ ] **[AI-001]** Implement 2-factor authentication
  - **Reason**: Critical security for user data
  - **Impact**: High (GDPR compliance requirement)
  - **Effort**: 8-12 hours
  - **Priority**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

### üü° SHOULD HAVE (Important)
- [ ] **[AI-002]** Add analytics dashboard
  - **Reason**: Stakeholders requested usage metrics
  - **Impact**: Medium (improves decision making)
  - **Effort**: 4-6 hours
  - **Priority**: ‚≠ê‚≠ê‚≠ê‚≠ê

### üü¢ COULD HAVE (Improvements)
- [ ] **[AI-003]** Dark mode in application theme
  - **Reason**: Frequent request from end users
  - **Impact**: Low (UX enhancement)
  - **Effort**: 2-3 hours
  - **Priority**: ‚≠ê‚≠ê‚≠ê

---
**üìä AI Recommendation Statistics**:
- Total recommended: 3/30 (10% of limit)
- Current phase: PHASE 2 - ACCELERATION (progress: 35%)
- Next review: After next sprint
```

#### **Addition Frequency**

**When AI should add new tasks**:
- ‚úÖ **After each completed sprint/milestone**
- ‚úÖ **When progress reaches milestones**: 25%, 50%, 75%
- ‚úÖ **When user explicitly requests**: "Suggest new tasks"
- ‚ùå **NEVER** add tasks in the middle of active implementation

**User Approval**:
```
‚ùì After each sprint, ask:
"Would you like to review [X] new AI-recommended tasks for TASKS.md?"

A) ‚úÖ Yes, add to TASKS.md for review
B) üìã Yes, but show preview before adding
C) ‚è≠Ô∏è Skip for now (don't add this sprint)
D) üõë Stop recommendations (disable permanently)
```

#### **Complete Example**

```markdown
# TASKS.md

## üìä Project Statistics
- **Overall Progress**: 45% complete (18/40 tasks)
- **Current Phase**: PHASE 3 - MAXIMUM PEAK
- **AI Tasks**: 12/30 recommended (40% of limit)

## ‚úÖ Completed Tasks (18)
- [x] Initial project setup
- [x] Implement basic authentication
- [x] User CRUD
... (15 more)

## üî® Pending Original Tasks (22)
- [ ] Payment API integration
- [ ] Notification system
... (20 more)

## ü§ñ AI-Recommended Tasks (12/30 used)

### üî¥ MUST HAVE
- [ ] **[AI-001]** Rate limiting on API endpoints
  - **Reason**: Prevent abuse and ensure stability
  - **Impact**: High (security and performance)
  - **Effort**: 3-4 hours
  
- [ ] **[AI-002]** Structured logging for debugging
  - **Reason**: Facilitate troubleshooting in production
  - **Impact**: High (operational)
  - **Effort**: 2-3 hours

### üü° SHOULD HAVE
- [ ] **[AI-003]** Export data to CSV format
  - **Reason**: Stakeholder request for analysis
  - **Impact**: Medium (convenience)
  - **Effort**: 2 hours

... (9 more tasks)

---
**üéØ Next Recommendation Review**: After Sprint 8 (when reaching 60% progress)
```

#### **Disabling Recommendations**

If user wants to **stop** recommendations:

```markdown
## ü§ñ AI Recommendations: DISABLED

_User chose to manage tasks manually._

**To reactivate**: Request AI "Reactivate task recommendations"
```

---

**Why this functionality is valuable?**:
- ‚úÖ **AI Creativity**: Identifies opportunities developers might not see
- ‚úÖ **Professionalism**: Suggests best practices and quality patterns
- ‚úÖ **Refinement**: Collaborates with requirements to meet client expectations
- ‚úÖ **Control**: User has full control (limit, approval, disable)
- ‚úÖ **Focus**: Growth/decay curve prevents feature creep
- ‚úÖ **Scope**: Recommendations based on project context and feedback

**üìÅ TASKS.md File Location**:
- **Default preference**: The `TASKS.md` file, when created, should be placed in `docs/TASKS.md`
- **Create docs/ folder**: If the `docs/` folder does not exist in the project, it should be created automatically
- **Flexibility**: The user or programmer can choose to place it in another location if preferred
- **Creation example**:
  ```bash
  # Create docs folder if it doesn't exist
  mkdir -p docs
  
  # Create or update TASKS.md
  echo "# Tasks" > docs/TASKS.md
  ```

**Example of Marking (REQUIREMENTS.md)**:
```markdown
## üü¢ COULD HAVE (Low Priority)

### ‚úÖ Completed Tasks

#### Task Example - Integrated File Editor (vX.Y.Z)
**Status**: ‚úÖ Complete - 30/11/2025

**Objective**: Implement integrated text editor with color-coded scope differentiation.

**Implementation**:
1. ‚úÖ ComponentE with QTextEdit and syntax highlighting
2. ‚úÖ Color-coded scope differentiation (HTML tags, DATA keys, etc.)
3. ‚úÖ Open/save files (.txt, .data, .html, .tsx, .py)
4. ‚úÖ Integration with File menu ‚Üí Open Editor

**Files Created**:
- `src/gui/editor_dock.py` (500+ lines)
- `tests/test_editor_dock.py` (15 tests)

### üî® Pending Tasks
- **[]** Next unimplementated task...
```

**Recommended Minimum Structure**:
```markdown
# Project - Tasks

## Categories
- MUST HAVE: [X/Y complete] (Z%)
- SHOULD HAVE: [X/Y complete] (Z%)
- COULD HAVE: [X/Y complete] (Z%)
- WOULD HAVE: [X/Y complete] (Z%)

## Statistics
- **TOTAL**: [X/Y complete] (Z%)
```

**Version Documentation Structure**:
```markdown
# Clarify v2.9.X - [Descriptive Name]

**Date**: DD/MM/YYYY
**Sprint**: X tasks in Y hours
**Methodology**: Simplicity Protocol 1

## üìã Sprint Objectives
- Task #X: [description]
- Task #Y: [description]

## üéØ Implemented Tasks
### Task #X: [Name]
- **Problem**: [description of original problem]
- **Solution**: [how it was solved]
- **Modified Files**: [list]
- **Tests**: [quantity and status]

## ‚úÖ Quality (Simplicity Protocol 1)
- ‚úÖ Modular Architecture
- ‚úÖ Type Hints (100%)
- ‚úÖ Complete Docstrings
- ‚úÖ Error Handling
- ‚úÖ Tests (X passing)
- ‚úÖ Semantic Commits
- ‚úÖ Complete Documentation
- ‚úÖ Clean Code (PEP8)

## üìä Statistics
- TOTAL: X% complete (Y/Z tasks)
- Commits: N pushed
```

---

### 1Ô∏è‚É£3Ô∏è‚É£ **Commit and Push**
- **Format**: Conventional Commits (feat/fix/docs/refactor/test)
- **Message**: Descriptive, complete, with context
- **Frequency**: 1 commit per task or logical group of changes

**Commit Message Structure**:
```
<type>: <short description> (<version>)

<ORIGINAL PROBLEM>:
- [Context of the problem]
- [Why it was necessary to solve]

<IMPLEMENTED SOLUTION>:
‚úÖ [Feature/function 1]
   - [Technical detail]
‚úÖ [Feature/function 2]
   - [Technical detail]

‚úÖ [TESTS]:
   - [Quantity] unit tests ([status])
   - [Tested categories]

<MODIFIED FILES>:
- [file1.py] (+X lines)
- [file2.py] (~Y lines)
- [tests/test_X.py] (NEW - Z lines)
- [docs/REQUIREMENTS.md] (updated statistics)

<UPDATED STATISTICS>:
- [CATEGORY]: X ‚Üí Y complete (A% ‚Üí B%)
- TOTAL: X ‚Üí Y complete (A% ‚Üí B%)

<USAGE EXAMPLE>: (if applicable)
  [Practical demonstration]

Refs: [related documentation]
Closes: Task #X (vX.X.X)
```

**Real Example** (Task Example):
```bash
git add src/ tests/ docs/REQUIREMENTS.md
git commit -m "feat: complete Task Example - Feature Update System (vX.Y.Z)

ORIGINAL PROBLEM:
- Implementation vX.Y.Z used string_similarity() (WRONG)
- Did not detect duplicate values, only name similarity
...

‚úÖ IMPLEMENTED SOLUTION:
‚úÖ extract_all_keys_from_obj()
   - Supports Obj AND dict type
   - Returns Dict[str, str] (path ‚Üí value)
...

Closes: Task Example (vX.Y.Z)"

git push
```

---

## üèÜ Professional Quality Criteria

Every implementation must meet **100% of these criteria**:

| # | Criterion | Description | Validation |
|---|----------|-----------|-----------|
| 1 | **Modular Architecture** | Each feature in a separate module | Own file in `src/` |
| 2 | **Type Hints** | 100% of parameters typed | `def func(x: int) -> str:` |
| 3 | **Docstrings** | All public functions documented | Args, Returns, Examples |
| 4 | **Error Handling** | Try/except with clear messages | `except Exception as e:` |
| 5 | **Tests** | Unit + integration (100% coverage) | `tests/test_*.py` passing |
| 6 | **Semantic Commits** | Conventional Commits | `feat:`, `fix:`, `docs:` |
| 7 | **Documentation** | REQUIREMENTS.md + SPECIFICATIONS.md | Updated and complete |
| 8 | **Clean Code** | PEP8, semantic names, DRY | Functions < 50 lines |

---

## üìä Practical Application: Task Example (Complete Example)

### Initial Situation
```markdown
Pending tasks in the SHOULD HAVE category:
[ ] Complex Feature Example (VERY COMPLEX)
[ ] Semantic AI Search (VERY COMPLEX)
[‚ö†Ô∏è] Feature Update (PARTIAL - simpler!) ‚úÖ CHOSEN
[ ] Google Translate API integration (COMPLEX)
```

### Planned Sprint
```
vX.Y.Z: Complete Task Example
Estimate: 3-4 hours
Complexity: MEDIUM (simpler than the others)
```

### Execution (Simplicity Protocol 1)

**1. Read Documentation** ‚úÖ
- Read: `docs/FEATURE_SPEC.md` (662 lines)
- Understood: problem of string similarity vs. value equality

**2. Choose Simple Task** ‚úÖ
- Task Example is **simpler** than text editor or AI
- Clear scope: 2 main functions + integration

**3. Ask Questions** ‚úÖ
- Asked: "How many words to pick? 3-5?"
- Answer: "Default 30 characters"
- Asked: "Convert to camelCase?"
- Answer: "Yes, remove accents"
- Asked: "Name conflicts?"
- Answer: "Shorter line wins, don't change if values are different"

**4. Sprint** ‚úÖ
- 6 subtasks planned (including questions)
- Estimated time: 3h45min

**5. Implement with Architecture** ‚úÖ
```
Order executed:
1. extract_all_keys_from_obj() (helper function - High Cohesion)
2. build_substitution_map_by_value() (main function - Low Coupling)
3. Update cli_dedupe() (integration - Dependency Injection)
4. Create tests (validation)
5. Documentation (finalization)

Applied Patterns:
- ‚úÖ Separate modules (Reuse)
- ‚úÖ Type hints in all functions
- ‚úÖ Information Expert (GRASP): each function has the info it needs
- ‚úÖ Low coupling: independent functions
- ‚úÖ High cohesion: each function does ONE thing
```

**6. Run Tests** ‚úÖ
```
12 unit tests created:
- 4 tests for extract_all_keys_from_obj()
- 5 tests for build_substitution_map_by_value()
- 2 tests for apply_substitutions_to_file()
- 1 test for update_references_in_project()
Result: 12/12 passing (100%)
```

**7. Documentation** ‚úÖ
```
Files created/updated:
- docs/REQUIREMENTS.md (Task Example marked [X])
- docs/FEATURE_SPEC.md (already existed)
- tests/test_reference_updater.py (NEW - 350 lines)
Statistics: 59.6% ‚Üí 60.6% (63 tasks complete)
```

**8. Commit and Push** ‚úÖ
```bash
Commit: 903bca4
Message: 60 lines (complete and detailed)
Status: pushed to GitHub ‚úÖ
```

### Final Result
‚úÖ **Task Example 100% complete**
‚úÖ **Simplicity Protocol 1: 10/10 steps met** (v1.1 - 10 steps)
‚úÖ **Actual time: ~3h (within estimate)**
‚úÖ **Zero bugs detected**
‚úÖ **Professional documentation**

**Note**: This example uses v1.1 of the protocol (10 steps). v1.2 adds 2 more steps (GUI and CLI integration).

---

## üéì Lessons Learned

### ‚úÖ What Works
1. **Choose the simplest**: Task Example was easier than text editor
2. **Incrementality**: Helper function ‚Üí main ‚Üí integration
3. **Tests first**: Detected 2 necessary adjustments before committing
4. **Complete documentation**: Facilitates future maintenance

### ‚ùå Anti-patterns to Avoid
1. **Don't start with the hardest task**
   - ‚ùå "I'll do the text editor first (50h)"
   - ‚úÖ "I'll do the tooltip preview first (30min)"

2. **Don't do everything at once**
   - ‚ùå "I'll implement everything in one giant function"
   - ‚úÖ "I'll split into 3 testable functions"

3. **Don't skip tests**
   - ‚ùå "I'll test manually later"
   - ‚úÖ "I'll create 12 unit tests now"

4. **Don't make generic commits**
   - ‚ùå `git commit -m "updates"`
   - ‚úÖ `git commit -m "feat: Task Example with VALUE EQUALITY (60 lines)"`

---

## üìö References

- **REQUIREMENTS.md**: Complete list of project tasks
- **vX.Y.Z-COMPARISON.md**: First example of the protocol
- **vX.Y.Z-SPECIFICATIONS.md**: Sprint with 3 simple tasks
- **vX.Y.Z-SPECIFICATIONS.md**: Rapid iterations
- **vX.Y.Z-SPECIFICATIONS.md**: 4 UX improvements
- **FEATURE_SPEC.md**: Example of detailed documentation

---

## üîÑ Continuous Cycle

Simplicity Protocol 1 is an **iterative cycle**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. Read Documentation                       ‚îÇ
‚îÇ  2. Choose the Simplest Tasks                ‚îÇ
‚îÇ  3. Ask Questions to the Programmer          ‚îÇ
‚îÇ  4. Analyze and Study the Project            ‚îÇ
‚îÇ  5. Plan Sprint (2-4 tasks, 3-4h)            ‚îÇ
‚îÇ  6. Implement (GoF + GRASP architecture)     ‚îÇ
‚îÇ  7. Verify GUI Integration                   ‚îÇ
‚îÇ  8. Verify CLI Implementation                ‚îÇ
‚îÇ  9. Test (100% coverage)                     ‚îÇ
‚îÇ  10. Organize Root Folder                    ‚îÇ
‚îÇ  11. Document (TASKS + vX.X.X-SPECS)         ‚îÇ
‚îÇ  12. Commit + Push (conventional)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ    REPEAT    ‚îÇ ‚Üê There are always simpler tasks!
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Result**: Constant progress, professional code, zero technical debt.

---

## üéØ Final Message

> "I want complete and professional work!"

**This protocol ensures**:
- ‚úÖ Professional quality (12 mandatory steps)
- ‚úÖ Incremental progress (from simple to complex)
- ‚úÖ Complete documentation (never forget what was done)
- ‚úÖ Tested code (100% reliable)
- ‚úÖ Verified integration (functional GUI + CLI)
- ‚úÖ Organized commits (clean history)

**Reread this document before each sprint!**

---

## üí° Programming Best Practices for AI

> **This section contains specific recommendations to improve the quality of code generated by artificial intelligences.**

### 1. üìñ **Readable and Self-Documenting Code**

**Why it matters**: AIs should produce code that humans can easily understand and maintain.

**Practices**:
- ‚úÖ **Descriptive names**: Use names that explain the purpose
  ```python
  # ‚ùå BAD
  def proc(d, x):
      return d[x] if x in d else None
  
  # ‚úÖ GOOD
  def get_user_preference(preferences_dict, preference_key):
      """Returns user preference or None if it doesn't exist."""
      return preferences_dict.get(preference_key)
  ```

- ‚úÖ **Small and focused functions**: One function = one responsibility
  ```python
  # ‚ùå BAD - Function does multiple things
  def process_user_data(user):
      # validates
      # transforms
      # saves to database
      # sends email
      # logs
      pass  # 150 lines
  
  # ‚úÖ GOOD - Specialized functions
  def validate_user_data(user): pass
  def transform_user_data(user): pass
  def save_user_to_database(user): pass
  def send_welcome_email(user): pass
  def log_user_registration(user): pass
  ```

- ‚úÖ **Avoid "magic numbers"**: Use named constants
  ```python
  # ‚ùå BAD
  if user.age > 18 and balance < 1000:
      apply_fee(balance * 0.05)
  
  # ‚úÖ GOOD
  MINIMUM_ADULT_AGE = 18
  BALANCE_THRESHOLD = 1000
  SERVICE_FEE_RATE = 0.05
  
  if user.age > MINIMUM_ADULT_AGE and balance < BALANCE_THRESHOLD:
      apply_fee(balance * SERVICE_FEE_RATE)
  ```

### 2. üéØ **Consistent Naming Conventions**

**Why it matters**: Consistency facilitates navigation and code comprehension.

**Practices by language**:

**Python**:
- ‚úÖ `snake_case` for functions and variables
- ‚úÖ `PascalCase` for classes
- ‚úÖ `SCREAMING_SNAKE_CASE` for constants
- ‚úÖ `_private_method` for private methods

**JavaScript/TypeScript**:
- ‚úÖ `camelCase` for functions and variables
- ‚úÖ `PascalCase` for classes and components
- ‚úÖ `SCREAMING_SNAKE_CASE` for constants
- ‚úÖ `_privateMethod` or `#privateField` for private

**General conventions**:
- ‚úÖ Verbs for functions: `get_user()`, `calculate_total()`, `validate_input()`
- ‚úÖ Nouns for classes: `UserManager`, `PaymentProcessor`
- ‚úÖ Booleans with prefixes: `is_valid`, `has_permission`, `can_edit`

### 3. üõ°Ô∏è **Robust Error Handling**

**Why it matters**: Production code must gracefully handle failures.

**Practices**:
- ‚úÖ **Always validate input**:
  ```python
  def divide(a, b):
      if not isinstance(a, (int, float)) or not isinstance(b, (int, float)):
          raise TypeError("Arguments must be numbers")
      if b == 0:
          raise ValueError("Divisor cannot be zero")
      return a / b
  ```

- ‚úÖ **Use specific exceptions**:
  ```python
  # ‚ùå BAD - Generic exception
  try:
      process_payment(amount)
  except Exception as e:
      print("Error")
  
  # ‚úÖ GOOD - Specific exceptions
  try:
      process_payment(amount)
  except PaymentDeclinedError as e:
      notify_user("Payment declined")
  except InsufficientFundsError as e:
      notify_user("Insufficient funds")
  except NetworkError as e:
      retry_payment(amount)
  ```

- ‚úÖ **Adequate logging**:
  ```python
  import logging
  
  try:
      result = risky_operation()
  except Exception as e:
      logging.error(f"Failed in risky_operation: {e}", exc_info=True)
      raise  # Re-raise to allow handling at higher level
  ```

### 4. üß™ **Effective Testing Strategies**

**Why it matters**: Tests ensure code works and continues working.

**Practices**:
- ‚úÖ **Unit tests for business logic**:
  ```python
  def test_calculate_discount():
      # Arrange
      original_price = 100
      discount_rate = 0.2
      
      # Act
      final_price = calculate_discount(original_price, discount_rate)
      
      # Assert
      assert final_price == 80
  ```

- ‚úÖ **Test edge cases**:
  ```python
  def test_edge_cases():
      assert calculate_discount(0, 0.5) == 0  # Zero price
      assert calculate_discount(100, 0) == 100  # Zero discount
      assert calculate_discount(100, 1.0) == 0  # 100% discount
      
      with pytest.raises(ValueError):
          calculate_discount(100, -0.1)  # Negative discount
      
      with pytest.raises(ValueError):
          calculate_discount(-100, 0.1)  # Negative price
  ```

- ‚úÖ **Mocks for external dependencies**:
  ```python
  from unittest.mock import Mock, patch
  
  def test_send_notification():
      with patch('email_service.send') as mock_send:
          notify_user("user@example.com", "Test message")
          mock_send.assert_called_once()
  ```

### 5. üîí **Security First**

**Why it matters**: Vulnerabilities can have serious consequences.

**Practices**:
- ‚úÖ **Never trust user input**:
  ```python
  # ‚ùå BAD - SQL Injection
  query = f"SELECT * FROM users WHERE id = {user_id}"
  
  # ‚úÖ GOOD - Parameterization
  query = "SELECT * FROM users WHERE id = ?"
  cursor.execute(query, (user_id,))
  ```

- ‚úÖ **Secrets in environment variables**:
  ```python
  # ‚ùå BAD
  API_KEY = "sk-1234567890abcdef"  # Hardcoded
  
  # ‚úÖ GOOD
  import os
  API_KEY = os.getenv('API_KEY')
  if not API_KEY:
      raise ValueError("API_KEY not configured")
  ```

- ‚úÖ **Sanitize output to prevent XSS**:
  ```python
  from html import escape
  
  # ‚ùå BAD
  html = f"<div>Hello {user_name}</div>"
  
  # ‚úÖ GOOD
  html = f"<div>Hello {escape(user_name)}</div>"
  ```

### 6. ‚ö° **Performance Optimization**

**Why it matters**: Slow code = unhappy users.

**Practices**:
- ‚úÖ **Choose correct data structure**:
  ```python
  # ‚ùå BAD - List search O(n)
  if user_id in user_list:  # 1000 comparisons
      # ...
  
  # ‚úÖ GOOD - Set search O(1)
  if user_id in user_set:  # 1 comparison
      # ...
  ```

- ‚úÖ **Avoid unnecessary loops**:
  ```python
  # ‚ùå BAD - Double loop O(n¬≤)
  for item in list1:
      for item2 in list2:
          if item == item2:
              # ...
  
  # ‚úÖ GOOD - Set intersection O(n)
  common_items = set(list1) & set(list2)
  for item in common_items:
      # ...
  ```

- ‚úÖ **Lazy loading when appropriate**:
  ```python
  # ‚ùå BAD - Load everything into memory
  all_users = User.objects.all()  # 1 million records
  for user in all_users:
      process(user)
  
  # ‚úÖ GOOD - Iterator that loads on demand
  for user in User.objects.iterator():
      process(user)
  ```

### 7. üìù **Clear and Useful Documentation**

**Why it matters**: Code is read much more often than it is written.

**Practices**:
- ‚úÖ **Complete docstrings**:
  ```python
  def calculate_shipping(weight, distance, express=False):
      """
      Calculate shipping cost based on weight and distance.
      
      Args:
          weight (float): Package weight in kg
          distance (float): Distance in km
          express (bool): If True, uses express shipping (default: False)
      
      Returns:
          float: Shipping cost in dollars
      
      Raises:
          ValueError: If weight or distance is negative
      
      Examples:
          >>> calculate_shipping(2.5, 100)
          25.0
          >>> calculate_shipping(2.5, 100, express=True)
          37.5
      """
      if weight < 0 or distance < 0:
          raise ValueError("Weight and distance must be positive")
      
      base_cost = weight * distance * 0.1
      return base_cost * 1.5 if express else base_cost
  ```

- ‚úÖ **Comments explain "why", not "what"**:
  ```python
  # ‚ùå BAD - Comments the obvious
  x = x + 1  # Increment x
  
  # ‚úÖ GOOD - Explains the reason
  # Increment counter to include current element in count
  # since range() excludes the last element
  x = x + 1
  ```

- ‚úÖ **README with practical examples**:
  ```markdown
  # How to use
  
  ## Installation
  ```bash
  pip install mypackage
  ```
  
  ## Basic example
  ```python
  from mypackage import Calculator
  
  calc = Calculator()
  result = calc.add(2, 3)
  print(result)  # Output: 5
  ```
  ```

### 8. üèóÔ∏è **Organization and Modularity**

**Why it matters**: Organized code is easier to maintain and scale.

**Practices**:
- ‚úÖ **Separation of concerns**:
  ```
  project/
  ‚îú‚îÄ‚îÄ models/       # Data structures
  ‚îú‚îÄ‚îÄ services/     # Business logic
  ‚îú‚îÄ‚îÄ controllers/  # Flow coordination
  ‚îú‚îÄ‚îÄ views/        # User interface
  ‚îú‚îÄ‚îÄ utils/        # Helper functions
  ‚îî‚îÄ‚îÄ tests/        # Automated tests
  ```

- ‚úÖ **DRY (Don't Repeat Yourself)**:
  ```python
  # ‚ùå BAD - Duplicated code
  def process_order_a():
      validate()
      calculate()
      save()
  
  def process_order_b():
      validate()
      calculate()
      save()
  
  # ‚úÖ GOOD - Reused code
  def process_order_common():
      validate()
      calculate()
      save()
  
  def process_order_a():
      process_order_common()
      # specific logic A
  
  def process_order_b():
      process_order_common()
      # specific logic B
  ```

- ‚úÖ **Single responsibility principle**:
  ```python
  # ‚ùå BAD - Class does many things
  class User:
      def __init__(self): pass
      def save_to_database(self): pass
      def send_email(self): pass
      def generate_pdf_report(self): pass
  
  # ‚úÖ GOOD - Specialized classes
  class User:
      def __init__(self): pass
  
  class UserRepository:
      def save(self, user): pass
  
  class EmailService:
      def send(self, to, message): pass
  
  class ReportGenerator:
      def generate_pdf(self, user): pass
  ```

### 9. üîÑ **Effective Version Control**

**Why it matters**: Clean history facilitates debugging and collaboration.

**Practices**:
- ‚úÖ **Atomic and descriptive commits**:
  ```bash
  # ‚ùå BAD
  git commit -m "fixes"
  git commit -m "updates"
  
  # ‚úÖ GOOD
  git commit -m "feat: add email validation in registration form"
  git commit -m "fix: correct discount calculation for amounts over $1000"
  ```

- ‚úÖ **Branches for features**:
  ```bash
  # Create branch for new feature
  git checkout -b feature/user-authentication
  
  # Develop and commit
  git commit -m "feat: implement JWT login"
  
  # Merge after review
  git checkout main
  git merge feature/user-authentication
  ```

- ‚úÖ **Appropriate .gitignore**:
  ```gitignore
  # Python
  __pycache__/
  *.pyc
  .env
  venv/
  
  # JavaScript
  node_modules/
  dist/
  .env.local
  
  # IDEs
  .vscode/
  .idea/
  *.swp
  
  # OS
  .DS_Store
  Thumbs.db
  ```

### 10. üì¶ **Dependency Management**

**Why it matters**: Poorly managed dependencies cause compatibility problems.

**Practices**:
- ‚úÖ **Pin versions**:
  ```
  # ‚ùå BAD - requirements.txt
  flask
  requests
  
  # ‚úÖ GOOD - requirements.txt
  flask==2.3.2
  requests==2.31.0
  ```

- ‚úÖ **Use virtual environments**:
  ```bash
  # Python
  python -m venv venv
  source venv/bin/activate
  pip install -r requirements.txt
  
  # Node.js
  npm install  # Uses package-lock.json
  ```

- ‚úÖ **Check for vulnerabilities**:
  ```bash
  # Python
  pip install pip-audit
  pip-audit
  
  # Node.js
  npm audit
  npm audit fix
  ```

### üéØ **Quick Checklist for AI**

Before generating/committing code, verify:

- [ ] Names are descriptive and follow language conventions?
- [ ] Functions have single responsibility and are small?
- [ ] Is there error handling for exceptional cases?
- [ ] Code is tested (unit tests + edge cases)?
- [ ] No obvious security vulnerabilities?
- [ ] Performance is acceptable (no unnecessary O(n¬≤) algorithms)?
- [ ] Is there documentation (docstrings, useful comments)?
- [ ] Code is organized in logical modules?
- [ ] Commits are descriptive (conventional commits)?
- [ ] Dependencies have pinned versions?

### üìö **Additional Resources**

- **Clean Code** (Robert C. Martin) - Clean code principles
- **SOLID Principles** - Well-done object orientation
- **Design Patterns** (GoF) - Common solutions to common problems
- **OWASP Top 10** - Main security vulnerabilities
- **PEP 8** (Python) - Python style guide
- **Google Style Guides** - Style guides by language

---

**Reread this document before each sprint!**

---

**Version**: 2.0
**Last update**: December 16, 2025
**Maintained by**: Josu√© Amaral
**Status**: ACTIVE - Official project protocol
```
